{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "starter_notebook.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sakhile-mabunda/Natural-Language-Processing/blob/master/starter_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Igc5itf-xMGj"
      },
      "source": [
        "# Umsuka English - Isizulu Parallel Corpus\n",
        "\n",
        "## Research question: How do tokenization strategies affect machine translation performance?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "oGRmDELn7Az0",
        "outputId": "5ee284db-3787-4b64-de1c-0c400b8b225f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.flush_and_unmount()\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xpnpzQUk69J9",
        "outputId": "e13a2e04-d3df-482f-bf05-50a4ccb2f548"
      },
      "source": [
        "! rm -r joeynmt\n",
        "! rm -r sample_data\n",
        "! rm -r dev.bpe.en\n",
        "! rm -r dev.bpe.zu\n",
        "! rm -r dev.en\n",
        "! rm -r dev.zu\n",
        "! rm -r test.bpe.en\n",
        "! rm -r test.bpe.zu\n",
        "! rm -r test.en\n",
        "! rm -r test.zu\n",
        "! rm -r train.bpe.en\n",
        "! rm -r train.bpe.zu\n",
        "! rm -r train.en\n",
        "! rm -r train.zu\n",
        "! rm -r en-zu.eval.csv?download=1\n",
        "! rm -r en-zu.training.csv?download=1\n",
        "! rm -r tokenizer-trained.json\n",
        "! rm -r train.uni.en\n",
        "! rm -r train.uni.zu\n",
        "! rm -r dev.uni.en\n",
        "! rm -r dev.uni.zu\n",
        "! rm -r test.uni.en\n",
        "! rm -r test.uni.zu\n",
        "! rm -r train.char.en\n",
        "! rm -r train.char.zu\n",
        "! rm -r test.char.zu\n",
        "! rm -r test.char.en\n",
        "! rm -r m.vocab\n",
        "! rm -r m.model\n",
        "! rm -r dev.char.en\n",
        "! rm -r dev.char.zu\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove 'joeynmt': No such file or directory\n",
            "rm: cannot remove 'sample_data': No such file or directory\n",
            "rm: cannot remove 'dev.bpe.en': No such file or directory\n",
            "rm: cannot remove 'dev.bpe.zu': No such file or directory\n",
            "rm: cannot remove 'dev.en': No such file or directory\n",
            "rm: cannot remove 'dev.zu': No such file or directory\n",
            "rm: cannot remove 'test.bpe.en': No such file or directory\n",
            "rm: cannot remove 'test.bpe.zu': No such file or directory\n",
            "rm: cannot remove 'test.en': No such file or directory\n",
            "rm: cannot remove 'test.zu': No such file or directory\n",
            "rm: cannot remove 'train.bpe.en': No such file or directory\n",
            "rm: cannot remove 'train.bpe.zu': No such file or directory\n",
            "rm: cannot remove 'train.en': No such file or directory\n",
            "rm: cannot remove 'train.zu': No such file or directory\n",
            "rm: cannot remove 'en-zu.training.csv?download=1': No such file or directory\n",
            "rm: cannot remove 'tokenizer-trained.json': No such file or directory\n",
            "rm: cannot remove 'train.uni.en': No such file or directory\n",
            "rm: cannot remove 'train.uni.zu': No such file or directory\n",
            "rm: cannot remove 'dev.uni.en': No such file or directory\n",
            "rm: cannot remove 'dev.uni.zu': No such file or directory\n",
            "rm: cannot remove 'test.uni.en': No such file or directory\n",
            "rm: cannot remove 'test.uni.zu': No such file or directory\n",
            "rm: cannot remove 'train.char.en': No such file or directory\n",
            "rm: cannot remove 'train.char.zu': No such file or directory\n",
            "rm: cannot remove 'test.char.zu': No such file or directory\n",
            "rm: cannot remove 'test.char.en': No such file or directory\n",
            "rm: cannot remove 'm.vocab': No such file or directory\n",
            "rm: cannot remove 'm.model': No such file or directory\n",
            "rm: cannot remove 'dev.char.en': No such file or directory\n",
            "rm: cannot remove 'dev.char.zu': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "Cn3tgQLzUxwn"
      },
      "source": [
        "import os\n",
        "\n",
        "source_language = 'en'\n",
        "target_language = 'zu' \n",
        "lc = False  # If True, lowercase the data.\n",
        "seed = 42  # Random seed for shuffling.\n",
        "tag = 'baseline' # Give a unique name to your folder - this is to ensure you don't rewrite any models you've already submitted\n",
        "\n",
        "os.environ[\"src\"] = source_language # Sets them in bash as well, since we often use bash scripts\n",
        "os.environ[\"tgt\"] = target_language\n",
        "os.environ[\"tag\"] = tag\n",
        "\n",
        "# This will save it to a folder in our gdrive instead!\n",
        "!mkdir -p \"/content/drive/My Drive/nlp-project/$src-$tgt-$tag\"\n",
        "os.environ[\"gdrive_path\"] = \"/content/drive/My Drive/nlp-project/%s-%s-%s\" % (source_language, target_language, tag)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "kBSgJHEw7Nvx",
        "outputId": "80a33bca-c8a7-45a6-c2ab-6321f24c896c"
      },
      "source": [
        "!echo $gdrive_path"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/nlp-project/en-zu-baseline\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jaQqpKUbR_cX",
        "outputId": "8462efa1-b1ab-43b5-feae-2328f3e760a5"
      },
      "source": [
        "! wget https://zenodo.org/record/5035171/files/en-zu.training.csv?download=1\n",
        "\n",
        "! wget https://zenodo.org/record/5035171/files/en-zu.eval.csv?download=1"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-11-23 23:10:11--  https://zenodo.org/record/5035171/files/en-zu.training.csv?download=1\n",
            "Resolving zenodo.org (zenodo.org)... 137.138.76.77\n",
            "Connecting to zenodo.org (zenodo.org)|137.138.76.77|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1529050 (1.5M) [text/plain]\n",
            "Saving to: ‘en-zu.training.csv?download=1’\n",
            "\n",
            "en-zu.training.csv? 100%[===================>]   1.46M  2.35MB/s    in 0.6s    \n",
            "\n",
            "2021-11-23 23:10:13 (2.35 MB/s) - ‘en-zu.training.csv?download=1’ saved [1529050/1529050]\n",
            "\n",
            "--2021-11-23 23:10:14--  https://zenodo.org/record/5035171/files/en-zu.eval.csv?download=1\n",
            "Resolving zenodo.org (zenodo.org)... 137.138.76.77\n",
            "Connecting to zenodo.org (zenodo.org)|137.138.76.77|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 466677 (456K) [text/plain]\n",
            "Saving to: ‘en-zu.eval.csv?download=1’\n",
            "\n",
            "en-zu.eval.csv?down 100%[===================>] 455.74K   934KB/s    in 0.5s    \n",
            "\n",
            "2021-11-23 23:10:15 (934 KB/s) - ‘en-zu.eval.csv?download=1’ saved [466677/466677]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84YLcjWsi6pu"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "train = pd.read_csv('en-zu.training.csv?download=1')\n",
        "\n",
        "eval_df = pd.read_csv('en-zu.eval.csv?download=1')"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YkuK3B4p2AkN"
      },
      "source": [
        "## Pre-processing and export\n",
        "\n",
        "It is generally a good idea to remove duplicate translations and conflicting translations from the corpus. In practice, these public corpora include some number of these that need to be cleaned.\n",
        "\n",
        "In addition we will split our data into dev/test/train and export to the filesystem."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "M_2ouEOH1_1q"
      },
      "source": [
        "# drop duplicate translations\n",
        "train = train.drop_duplicates()\n",
        "\n",
        "# Shuffle the data to remove bias in dev set selection.\n",
        "train = train.sample(frac=1, random_state=seed).reset_index(drop=True)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Z_1BwAApEtMk",
        "outputId": "c34705c6-f70e-4916-d6bd-844b9019f0db"
      },
      "source": [
        "# Install fuzzy wuzzy to remove \"almost duplicate\" sentences in the\n",
        "# test and training sets.\n",
        "! pip install fuzzywuzzy\n",
        "! pip install python-Levenshtein\n",
        "\n",
        "import time\n",
        "from fuzzywuzzy import process\n",
        "import numpy as np\n",
        "from os import cpu_count\n",
        "from functools import partial\n",
        "from multiprocessing import Pool\n",
        "\n",
        "# reset the index of the training set after previous filtering\n",
        "train.reset_index(drop=False, inplace=True)\n",
        "\n",
        "# Remove samples from the training data set if they \"almost overlap\" with the\n",
        "# samples in the test set.\n",
        "\n",
        "# Filtering function. Adjust pad to narrow down the candidate matches to\n",
        "# within a certain length of characters of the given sample.\n",
        "def fuzzfilter(sample, candidates, pad):\n",
        "  candidates = [x for x in candidates if len(x) <= len(sample)+pad and len(x) >= len(sample)-pad] \n",
        "  if len(candidates) > 0:\n",
        "    return process.extractOne(sample, candidates)[1]\n",
        "  else:\n",
        "    return np.nan"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fuzzywuzzy in /usr/local/lib/python3.7/dist-packages (0.18.0)\n",
            "Requirement already satisfied: python-Levenshtein in /usr/local/lib/python3.7/dist-packages (0.12.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from python-Levenshtein) (57.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "hxxBOCA-xXhy",
        "outputId": "66d7e984-1ba5-44c8-8737-aeb284e0a4bd"
      },
      "source": [
        "# This section does the split between train/dev for the parallel corpora then saves them as separate files\n",
        "# We use 1000 dev set and the given test set.\n",
        "# Do the split between dev/train and create parallel corpora\n",
        "\n",
        "num_dev_patterns = 600\n",
        "\n",
        "# lower case the corpora - this will make it easier to generalize, but without proper casing.\n",
        "if lc:\n",
        "    train[\"en\"] = train[\"en\"].str.lower()\n",
        "    train[\"zu\"] = train[\"zu\"].str.lower()\n",
        "\n",
        "# dev and test sets\n",
        "dev = eval_df.tail(num_dev_patterns) \n",
        "test = eval_df.drop(eval_df.tail(num_dev_patterns).index)\n",
        "\n",
        "with open(\"train.\"+source_language, \"w\") as src_file, open(\"train.\"+target_language, \"w\") as trg_file:\n",
        "  for index, row in train.iterrows():\n",
        "    src_file.write(row[\"en\"]+\"\\n\")\n",
        "    trg_file.write(row[\"zu\"]+\"\\n\")\n",
        "    \n",
        "with open(\"dev.\"+source_language, \"w\") as src_file, open(\"dev.\"+target_language, \"w\") as trg_file:\n",
        "  for index, row in dev.iterrows():\n",
        "    src_file.write(row[\"en\"]+\"\\n\")\n",
        "    trg_file.write(row[\"zu\"]+\"\\n\")\n",
        "\n",
        "with open(\"test.\"+source_language, \"w\") as src_file, open(\"test.\"+target_language, \"w\") as trg_file:\n",
        "  for index, row in test.iterrows():\n",
        "    src_file.write(row[\"en\"]+\"\\n\")\n",
        "    trg_file.write(row[\"zu\"]+\"\\n\")\n",
        "\n",
        "# Doublecheck the format below. There should be no extra quotation marks or weird characters.\n",
        "! head train.*\n",
        "! head dev.*\n",
        "! head test.*"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==> train.dropout.bpe.en <==\n",
            "T@@ od@@ ay, those using Pl@@ anc@@ k and c@@ os@@ m@@ ic back@@ ground data to ob@@ ta@@ in a val@@ ue fo@@ r the H@@ ub@@ b@@ le con@@ st@@ ant g@@ et a f@@ ig@@ ur@@ e of 6@@ 7@@ .@@ 4 pl@@ us or min@@ us 0@@ .@@ 5. By contr@@ ast the local appro@@ ach giv@@ es a f@@ ig@@ ure of 7@@ 3.@@ 5 pl@@ us or min@@ us 1.@@ 4. Th@@ ese val@@ u@@ es re@@ pr@@ es@@ ent the two different val@@ u@@ es we have for the exp@@ ans@@ ion of the un@@ ivers@@ e. (@@ Se@@ e \"@@ A mat@@ ter of met@@ r@@ ic@@ s,\" bel@@ ow@@ .@@ )\n",
            "S@@ l@@ ack@@ 's st@@ ock has now f@@ all@@ en nearly 20@@ % fr@@ om i@@ ts ref@@ er@@ ence pr@@ ice of $@@ 2@@ 4 on the day of its W@@ all Street deb@@ ut@@ .\n",
            "A@@ ye@@ sh@@ a Sh@@ ro@@ ff@@ 's latest In@@ stag@@ ra@@ m ent@@ ry des@@ er@@ ves every@@ on@@ e's att@@ ent@@ ion.\n",
            "w@@ int@@ er we@@ ather al@@ er@@ ts from w@@ est V@@ ir@@ g@@ in@@ ia all th@@ e way up to M@@ ai@@ n@@ e.\n",
            "Mr M@@ o@@ is@@ e - who has be@@ en in power since 201@@ 7 - has cal@@ led for tal@@ ks with the op@@ posit@@ ion, with n@@ o succ@@ ess so f@@ ar@@ .\n",
            "5. Cl@@ ue (@@ 198@@ 5@@ )\n",
            "\"@@ My fri@@ end was st@@ ab@@ b@@ ed in the sh@@ ould@@ er and my other fri@@ end was sh@@ ot@@ ,\" she sa@@ id@@ . \"@@ I es@@ cap@@ ed with on@@ e fri@@ end and went home and the@@ n c@@ ame back to l@@ oo@@ k for another fri@@ end@@ .\"\n",
            "R@@ a@@ is@@ ing aw@@ ar@@ en@@ ess or m@@ aking l@@ ight@@ ?\n",
            "There wa@@ s also a re@@ p@@ ort wh@@ er@@ e, be@@ c@@ ause we h@@ ad thre@@ e polic@@ em@@ en in our si@@ de [@@ D@@ e@@ a@@ n R@@ ich@@ ard@@ s, W@@ ade D@@ ool@@ ey and Pa@@ ul Ac@@ k@@ for@@ d@@ ]@@ , it was said that we@@ '@@ d d@@ one t@@ o the W@@ all@@ ab@@ i@@ es w@@ hat happ@@ ened t@@ o Pakist@@ an@@ is and p@@ un@@ ks back in Brit@@ ain@@ .\n",
            "P@@ av@@ ar@@ ott@@ i s@@ ang T@@ on@@ io in D@@ on@@ iz@@ ett@@ i@@ 's L@@ a F@@ il@@ le d@@ u R@@ é@@ g@@ im@@ ent at the cit@@ y's M@@ et@@ ro@@ polit@@ an O@@ per@@ a, ach@@ iev@@ ing the ast@@ on@@ ish@@ ing fe@@ at of h@@ itt@@ ing n@@ ine high C@@ s in a row@@ .\n",
            "\n",
            "==> train.dropout.bpe.zu <==\n",
            "N@@ am@@ uhl@@ a, labo ab@@ asebenzisa id@@ at@@ h@@ a yang@@ emuva ye@@ Pl@@ anc@@ k a@@ n@@ d c@@ os@@ m@@ ic ukuthola int@@ eng@@ o ye@@ H@@ ub@@ b@@ le njalo bath@@ ola is@@ ibalo esing@@ u-@@ 6@@ 7.@@ 4 kweng@@ ezwa n@@ oma kus@@ us@@ we u-@@ 0@@ .@@ 5. O@@ kuph@@ amb@@ ana nen@@ dlela yas@@ endaweni en@@ ikeza is@@ ibalo esin@@ g@@ u-@@ 7@@ 3.@@ 5 kw@@ engez@@ wa nom@@ a ku@@ sus@@ wa u-@@ 1.@@ 4. L@@ a man@@ ani am@@ el@@ elela aman@@ ani am@@ ab@@ i@@ l@@ i a@@ hl@@ uk@@ ene esin@@ awo eku@@ khul@@ is@@ weni kw@@ end@@ awo y@@ onke. (@@ Bh@@ eka \"@@ In@@ d@@ aba yam@@ am@@ e@@ tr@@ ik@@ si@@ ,\" ngez@@ ans@@ i@@ .@@ )\n",
            "Is@@ it@@ oko sa@@ kw@@ a-@@ S@@ l@@ ack se@@ hl@@ e ngo@@ -@@ 20@@ % kun@@ t@@ eng@@ o y@@ aso ey@@ ay@@ iw@@ u $@@ 2@@ 4 ngesikhathi be@@ qala uku@@ ngena ku-@@ W@@ all Stre@@ et@@ .\n",
            "O@@ kub@@ hal@@ we kung@@ eku@@ d@@ ala ku-@@ In@@ st@@ ag@@ r@@ am ka-@@ A@@ ye@@ sha Sh@@ ro@@ ff@@ ' kud@@ inga uku@@ nak@@ ekelwa y@@ iw@@ onke um@@ unt@@ u.\n",
            "ukuq@@ wash@@ iswa ng@@ esi@@ m@@ o siz@@ ulu sas@@ ebus@@ ika sas@@ entsh@@ onal@@ anga ne@@ V@@ ir@@ g@@ in@@ ia kuy@@ o@@ sho eM@@ ain@@ e.\n",
            "UM@@ nu M@@ o@@ i@@ se - ob@@ es@@ eph@@ ethe kusukela ngo-201@@ 7 - uc@@ el@@ e izing@@ xoxo nam@@ aq@@ embu aph@@ ikis@@ ayo, ku@@ ngab@@ i n@@ amp@@ umelelo k@@ u@@ z@@ e kube man@@ j@@ e.\n",
            "5. Cl@@ ue (@@ 198@@ 5@@ )\n",
            "\"@@ Um@@ ng@@ ani wami ug@@ wa@@ z@@ we ehl@@ ombe kw@@ athi omunye wad@@ utsh@@ ul@@ wa@@ ,\" e@@ sho eph@@ awul@@ a. \"Ng@@ iph@@ uny@@ uke no@@ m@@ ng@@ ani oy@@ ed@@ wa ng@@ aya ekhaya ngab@@ uya ng@@ az@@ obh@@ eka omunye um@@ ng@@ an@@ i.\"\n",
            "Uku@@ khul@@ isa ukuq@@ wash@@ isa noma uku@@ ku@@ gq@@ am@@ isa ?\n",
            "K@@ wa@@ kukhona nom@@ b@@ iko futh@@ i, ngoba sas@@ in@@ amaphoyisa amathathu ng@@ ak@@ ithi [@@ u@@ De@@ an R@@ ich@@ ard@@ s, u@@ W@@ ade D@@ ool@@ ey no@@ Pa@@ ul Ac@@ k@@ fo@@ r@@ d@@ ]@@ , o@@ waw@@ uthi s@@ enze kum@@ a@@ W@@ all@@ ab@@ ies okw@@ akw@@ enz@@ eke kum@@ a@@ Pakist@@ ani nama-@@ p@@ un@@ ks e@@ Brith@@ an@@ i.\n",
            "UP@@ av@@ ar@@ ott@@ i w@@ ac@@ ula i-@@ T@@ on@@ io e-@@ D@@ on@@ i@@ ett@@ i@@ 's L@@ a F@@ il@@ le d@@ u R@@ é@@ g@@ im@@ ent ku-@@ M@@ et@@ ro@@ polit@@ an O@@ per@@ a yed@@ ol@@ ob@@ ha@@ , okw@@ aba nom@@ ph@@ um@@ ela oy@@ isim@@ anga woku@@ sh@@ ay@@ a o-@@ C abaph@@ ezulu ab@@ ayis@@ ish@@ ay@@ agal@@ olunye kul@@ and@@ el@@ an@@ a.\n",
            "\n",
            "==> train.en <==\n",
            "Today, those using Planck and cosmic background data to obtain a value for the Hubble constant get a figure of 67.4 plus or minus 0.5. By contrast the local approach gives a figure of 73.5 plus or minus 1.4. These values represent the two different values we have for the expansion of the universe. (See \"A matter of metrics,\" below.)\n",
            "Slack's stock has now fallen nearly 20% from its reference price of $24 on the day of its Wall Street debut.\n",
            "Ayesha Shroff's latest Instagram entry deserves everyone's attention.\n",
            "winter weather alerts from west Virginia all the way up to Maine.\n",
            "Mr Moise - who has been in power since 2017 - has called for talks with the opposition, with no success so far.\n",
            "5. Clue (1985)\n",
            "\"My friend was stabbed in the shoulder and my other friend was shot,\" she said. \"I escaped with one friend and went home and then came back to look for another friend.\"\n",
            "Raising awareness or making light?\n",
            "There was also a report where, because we had three policemen in our side [Dean Richards, Wade Dooley and Paul Ackford], it was said that we'd done to the Wallabies what happened to Pakistanis and punks back in Britain.\n",
            "Pavarotti sang Tonio in Donizetti's La Fille du Régiment at the city's Metropolitan Opera, achieving the astonishing feat of hitting nine high Cs in a row.\n",
            "\n",
            "==> train.zu <==\n",
            "Namuhla, labo abasebenzisa idatha yangemuva yePlanck and cosmic ukuthola intengo yeHubble njalo bathola isibalo esingu-67.4 kwengezwa noma kususwe u-0.5. Okuphambana nendlela yasendaweni enikeza isibalo esingu-73.5 kwengezwa noma kususwa u-1.4. La manani amelelela amanani amabili ahlukene esinawo ekukhulisweni kwendawo yonke. (Bheka \"Indaba yamametriksi,\" ngezansi.)\n",
            "Isitoko sakwa-Slack sehle ngo-20% kuntengo yaso eyayiwu $24 ngesikhathi beqala ukungena ku-Wall Street.\n",
            "Okubhalwe kungekudala ku-Instagram ka-Ayesha Shroff' kudinga ukunakekelwa yiwonke umuntu.\n",
            "ukuqwashiswa ngesimo sizulu sasebusika sasentshonalanga neVirginia kuyosho eMaine.\n",
            "UMnu Moise - obesephethe kusukela ngo-2017 - ucele izingxoxo namaqembu aphikisayo, kungabi nampumelelo kuze kube manje.\n",
            "5. Clue (1985)\n",
            "\"Umngani wami ugwazwe ehlombe kwathi omunye wadutshulwa,\" esho ephawula. \"Ngiphunyuke nomngani oyedwa ngaya ekhaya ngabuya ngazobheka omunye umngani.\"\n",
            "Ukukhulisa ukuqwashisa noma ukukugqamisa ?\n",
            "Kwakukhona nombiko futhi, ngoba sasinamaphoyisa amathathu ngakithi [uDean Richards, uWade Dooley noPaul Ackford], owawuthi senze kumaWallabies okwakwenzeke kumaPakistani nama-punks eBrithani.\n",
            "UPavarotti wacula i-Tonio e-Donietti's La Fille du Régiment ku-Metropolitan Opera yedolobha, okwaba nomphumela oyisimanga wokushaya o-C abaphezulu abayisishayagalolunye kulandelana.\n",
            "==> dev.dropout.bpe.en <==\n",
            "The resear@@ ch by the Un@@ iversity of Ed@@ in@@ burg@@ h over a 30 year look@@ ed at the sp@@ re@@ ad of B@@ ov@@ ine T@@ ub@@ er@@ cul@@ os@@ is (@@ T@@ B@@ ) within c@@ ow@@ s and b@@ ad@@ g@@ er popul@@ ations in Gl@@ ou@@ c@@ es@@ ter@@ sh@@ ire@@ .\n",
            "When I bo@@ ok@@ ed a thre@@ e-@@ day sing@@ ing ret@@ re@@ at at Pet@@ er Ev@@ ans@@ 's f@@ arm@@ h@@ ouse in r@@ ur@@ al Franc@@ e, I had been ap@@ pre@@ h@@ ens@@ ive about dis@@ playing my p@@ it@@ if@@ ul ab@@ il@@ ities to some int@@ im@@ id@@ ating O@@ x@@ ford ch@@ or@@ al sch@@ ol@@ ar typ@@ e. But in@@ ste@@ ad I found m@@ y@@ self war@@ bl@@ ing to a sk@@ inn@@ y former p@@ un@@ k gu@@ it@@ ar@@ ist with a str@@ ong \"@@ s@@ ar@@ f@@ \" London acc@@ ent and an arr@@ ay of tr@@ ul@@ y terr@@ ible j@@ ok@@ es.\n",
            "In 201@@ 2, for exam@@ pl@@ e, m@@ ult@@ i@@ ple gu@@ ests were inj@@ ured when their t@@ our bu@@ s cr@@ ash@@ ed in St@@ . Mart@@ in.\n",
            "\"We were really good that tour@@ nam@@ ent right the way through to the fin@@ al. I don't think it mot@@ iv@@ ates or car@@ ri@@ es me to do any bet@@ ter@@ ,\" G@@ en@@ ia said. \"@@ It's just emb@@ rac@@ ing it and enj@@ oy@@ ing it. It's once every four years. It's a different kind of b@@ uz@@ z and a different kind of ener@@ gy when you are h@@ ere when you are playing at a World C@@ up because all the att@@ ention is on r@@ ug@@ b@@ y@@ .\"\n",
            "An@@ ir@@ b@@ an L@@ ah@@ ir@@ i of Ind@@ ia did not have to qu@@ alif@@ y for his pre@@ vi@@ ous two U.S. O@@ p@@ en@@ s, and he doesn't want to have to go through it again@@ . L@@ ah@@ ir@@ i had no tr@@ ou@@ b@@ le with a 6@@ 5 at Sc@@ i@@ oto and a 6@@ 7 at Bro@@ ok@@ sid@@ e, but he could do without the st@@ res@@ s. He had never seen e@@ ither cour@@ s@@ e, op@@ ting for what he called \"@@ point and shoot@@ .\"\n",
            "M@@ e@@ an@@ wh@@ ile, the oil market rece@@ ived a more direc@@ t bo@@ ost this week on news that S@@ aud@@ i Ar@@ ab@@ ia sl@@ ash@@ ed ship@@ ments to the United St@@ at@@ es. The US has the most trans@@ par@@ ent and up@@ -@@ t@@ o-@@ d@@ ate data on the oil mark@@ et, which include week@@ ly releas@@ es on produc@@ tion l@@ evel@@ s, im@@ ports and ex@@ port@@ s, and inv@@ ent@@ or@@ ies. That kind of vis@@ ib@@ ility is not read@@ ily av@@ ail@@ able in most plac@@ es around the world@@ .\n",
            "Ex@@ ist@@ ing sol@@ ut@@ ions for contr@@ oll@@ ing re@@ fl@@ ection of wa@@ ves have low eff@@ ici@@ ency or difficul@@ t im@@ pl@@ ement@@ ation@@ ,@@ ' says An@@ a D@@ í@@ az@@ -@@ R@@ ub@@ i@@ o, post@@ doc@@ tor@@ al resear@@ ch@@ er at A@@ al@@ to Un@@ ivers@@ ity.\n",
            "When you start using h@@ ash@@ t@@ ag@@ s with more than 3@@ 00@@ ,000 post@@ s, it g@@ ets extr@@ em@@ ely difficul@@ t to be dis@@ cover@@ ed from that h@@ ash@@ t@@ ag@@ .\n",
            "Faceb@@ ook r@@ out@@ in@@ ely compl@@ ies with government re@@ quest@@ s to b@@ lock cont@@ ent that viol@@ ate local law@@ s.\n",
            "H@@ ous@@ ing compl@@ et@@ ions f@@ ell 4.@@ 8@@ % to 1.@@ 16@@ 1 million un@@ its last month@@ , the low@@ est l@@ ev@@ el since Dec@@ emb@@ er.\n",
            "\n",
            "==> dev.dropout.bpe.zu <==\n",
            "U@@ c@@ waningo lwe@@ N@@ y@@ uvesi yase-@@ Ed@@ in@@ burg@@ h ol@@ uth@@ athe iminyaka engama-@@ 30 l@@ ubh@@ eke ukus@@ abal@@ ala kwe-@@ B@@ ov@@ ine T@@ ub@@ er@@ cul@@ os@@ is (@@ i-@@ T@@ B@@ ) ezin@@ kom@@ eni naku@@ m@@ a-@@ b@@ ad@@ g@@ er e@@ Gl@@ ou@@ es@@ ter@@ sh@@ ire@@ .\n",
            "Ngenkathi ngib@@ ek@@ isela uhlelo lw@@ ezin@@ suku ezintathu loku@@ v@@ us@@ elela ikh@@ ono lom@@ culo endl@@ ini yase@@ pl@@ az@@ ini ka@@ Pet@@ er Ev@@ an endaweni es@@ em@@ akh@@ aya e@@ Franc@@ e, ngang@@ in@@ okuz@@ ib@@ amba maqondana noku@@ v@@ eza amakh@@ ono ami al@@ us@@ izi kuhl@@ obo lw@@ ab@@ ac@@ uli be@@ kw@@ aya base-@@ O@@ x@@ for@@ d@@ . Kodwa esikhundleni sal@@ okho ngiz@@ ithole ng@@ ic@@ ul@@ ela phansi kukh@@ ala isig@@ ingc@@ i s@@ isho ic@@ ulo ngoku@@ gc@@ iz@@ elela indlela yoku@@ kh@@ uluma yase@@ London ku@@ hamb@@ isana nam@@ ahl@@ aya amab@@ i kakhulu.\n",
            "Ng@@ owezi-201@@ 2, is@@ ibon@@ elo, kwal@@ im@@ ala inq@@ waba yez@@ ivakash@@ i lapho ibh@@ asi labo loku@@ v@@ akash@@ a l@@ ish@@ ay@@ isa e-@@ St@@ . Mart@@ in.\n",
            "\"S@@ idl@@ ale kahle kakhulu kul@@ owo m@@ qh@@ ud@@ elwano zib@@ ekwa nje kw@@ aze kw@@ af@@ ikw@@ a eku@@ gcin@@ eni. Ang@@ ib@@ oni ukuthi kuy@@ akh@@ uth@@ aza noma kung@@ iq@@ hub@@ a kang@@ con@@ o,\" kusho uG@@ en@@ ia. \"@@ W@@ uku@@ kwam@@ ukela nje noku@@ ku@@ jabul@@ ela. K@@ uba njalo eminyakeni em@@ in@@ e. W@@ uhlobo ol@@ uhl@@ uk@@ ile lom@@ s@@ indo futhi uhlobo ol@@ uhl@@ uk@@ ile lw@@ amandla uma ul@@ aph@@ a uma u@@ dlala kw@@ iN@@ deb@@ e Y@@ omhlaba ngoba kun@@ akw@@ a kakhulu umdlalo w@@ ebhola l@@ omb@@ ho@@ x@@ o.\"\n",
            "U-@@ An@@ ir@@ b@@ an L@@ ah@@ ir@@ i wase@@ N@@ d@@ iya akaz@@ ange af@@ an@@ eleke uku@@ ngena kuma-@@ U.S. O@@ p@@ en amabili a@@ dlul@@ e, futhi aku@@ d@@ ing@@ eki aph@@ inde a@@ dlul@@ e kuwo futh@@ i. U@@ L@@ ah@@ ir@@ i ak@@ abanga n@@ enk@@ inga ne-@@ 6@@ 5 at Sc@@ i@@ oto kanye ne-@@ 6@@ 7 at Bro@@ ok@@ sid@@ e, kodwa ub@@ eng@@ akw@@ enza ngaphandle kw@@ engc@@ ind@@ ez@@ i. U@@ beng@@ akazi az@@ ib@@ one zonke izin@@ kundla zoku@@ dlal@@ a, wakh@@ etha lokhu akub@@ ize ngokuthi \"@@ point and shoot@@ .\"\n",
            "Kus@@ enj@@ alo, im@@ akethe k@@ aw@@ oy@@ ela ithole u@@ xh@@ aso ol@@ uq@@ ond@@ e ngqo kuleli son@@ to ng@@ ezindaba zoku@@ thi iS@@ aud@@ i Ar@@ ab@@ ia iy@@ eke ukuth@@ um@@ ele imp@@ ahla e-@@ United St@@ at@@ es. I-@@ US in@@ em@@ in@@ in@@ ingo es@@ obala kakhulu n@@ eb@@ uy@@ ek@@ ez@@ iwe maqondana nez@@ im@@ akethe z@@ aw@@ oy@@ ela, okub@@ andakanya ukukh@@ ishwa kw@@ amaz@@ inga om@@ kh@@ iqizo es@@ ont@@ o, okuth@@ eng@@ iselwa amanye amazwe noku@@ th@@ eng@@ wa kuv@@ ela kw@@ amanye amaz@@ we, kanye no@@ hl@@ u lw@@ emp@@ ahl@@ a. L@@ olo hlobo loku@@ b@@ onakala al@@ uth@@ ol@@ ak@@ ali ngaleso sikhathi ezindaweni eziningi emhlab@@ eni.\n",
            "Iz@@ inhl@@ elo ezikh@@ ona zoku@@ law@@ ula amag@@ ag@@ asi av@@ elayo az@@ igc@@ ul@@ isi ngokw@@ anele noma ku@@ ba n@@ zima ukuz@@ iq@@ alis@@ a@@ ,@@ ' kusho u-@@ An@@ a D@@ í@@ az@@ -@@ R@@ ub@@ i@@ o, um@@ c@@ wan@@ ingi wez@@ iq@@ u ezil@@ andela ez@@ ob@@ ud@@ okotela eN@@ y@@ uvesi yase-@@ A@@ al@@ t@@ o.\n",
            "Uma u@@ qala uku@@ sebenzisa ama-@@ h@@ ash@@ ta@@ g an@@ amaph@@ ost@@ i angaph@@ ezu kwez@@ i-@@ 3@@ 00@@ ,00@@ 0, ku@@ ba n@@ zima kakhulu ukuth@@ olakala ku@@ leyo h@@ ash@@ t@@ ag@@ .\n",
            "I@@ Faceb@@ ook iv@@ ame uku@@ hamb@@ isana nez@@ icelo zik@@ ahulumeni ukuv@@ imba okuq@@ uk@@ eth@@ we okuph@@ ula imith@@ etho yas@@ end@@ aw@@ eni.\n",
            "Uku@@ q@@ ed@@ wa kwezin@@ dlu kuw@@ ela kuma-@@ 4.@@ 8@@ % kw@@ aya kum@@ ay@@ un@@ ithi ayis@@ igidi esing@@ u-@@ 1.@@ 16@@ 1 ngen@@ y@@ anga edlule, okuy@@ iz@@ inga eliph@@ ansi kakhulu kusukela ngo@@ Dis@@ emb@@ a.\n",
            "\n",
            "==> dev.en <==\n",
            "The research by the University of Edinburgh over a 30 year looked at the spread of Bovine Tuberculosis (TB) within cows and badger populations in Gloucestershire.\n",
            "When I booked a three-day singing retreat at Peter Evans's farmhouse in rural France, I had been apprehensive about displaying my pitiful abilities to some intimidating Oxford choral scholar type. But instead I found myself warbling to a skinny former punk guitarist with a strong \"sarf\" London accent and an array of truly terrible jokes.\n",
            "In 2012, for example, multiple guests were injured when their tour bus crashed in St. Martin.\n",
            "\"We were really good that tournament right the way through to the final. I don't think it motivates or carries me to do any better,\" Genia said. \"It's just embracing it and enjoying it. It's once every four years. It's a different kind of buzz and a different kind of energy when you are here when you are playing at a World Cup because all the attention is on rugby.\"\n",
            "Anirban Lahiri of India did not have to qualify for his previous two U.S. Opens, and he doesn't want to have to go through it again. Lahiri had no trouble with a 65 at Scioto and a 67 at Brookside, but he could do without the stress. He had never seen either course, opting for what he called \"point and shoot.\"\n",
            "Meanwhile, the oil market received a more direct boost this week on news that Saudi Arabia slashed shipments to the United States. The US has the most transparent and up-to-date data on the oil market, which include weekly releases on production levels, imports and exports, and inventories. That kind of visibility is not readily available in most places around the world.\n",
            "Existing solutions for controlling reflection of waves have low efficiency or difficult implementation,' says Ana Díaz-Rubio, postdoctoral researcher at Aalto University.\n",
            "When you start using hashtags with more than 300,000 posts, it gets extremely difficult to be discovered from that hashtag.\n",
            "Facebook routinely complies with government requests to block content that violate local laws.\n",
            "Housing completions fell 4.8% to 1.161 million units last month, the lowest level since December.\n",
            "\n",
            "==> dev.zu <==\n",
            "Ucwaningo lweNyuvesi yase-Edinburgh oluthathe iminyaka engama-30 lubheke ukusabalala kwe-Bovine Tuberculosis (i-TB) ezinkomeni nakuma-badger eGlouestershire.\n",
            "Ngenkathi ngibekisela uhlelo lwezinsuku ezintathu lokuvuselela ikhono lomculo endlini yaseplazini kaPeter Evan endaweni esemakhaya eFrance, nganginokuzibamba maqondana nokuveza amakhono ami alusizi kuhlobo lwabaculi bekwaya base-Oxford. Kodwa esikhundleni salokho ngizithole ngiculela phansi kukhala isigingci sisho iculo ngokugcizelela indlela yokukhuluma yaseLondon kuhambisana namahlaya amabi kakhulu.\n",
            "Ngowezi-2012, isibonelo, kwalimala inqwaba yezivakashi lapho ibhasi labo lokuvakasha lishayisa e-St. Martin.\n",
            "\"Sidlale kahle kakhulu kulowo mqhudelwano zibekwa nje kwaze kwafikwa ekugcineni. Angiboni ukuthi kuyakhuthaza noma kungiqhuba kangcono,\" kusho uGenia. \"Wukukwamukela nje nokukujabulela. Kuba njalo eminyakeni emine. Wuhlobo oluhlukile lomsindo futhi uhlobo oluhlukile lwamandla uma ulapha uma udlala kwiNdebe Yomhlaba ngoba kunakwa kakhulu umdlalo webhola lombhoxo.\"\n",
            "U-Anirban Lahiri waseNdiya akazange afaneleke ukungena kuma-U.S. Open amabili adlule, futhi akudingeki aphinde adlule kuwo futhi. ULahiri akabanga nenkinga ne-65 at Scioto kanye ne-67 at Brookside, kodwa ubengakwenza ngaphandle kwengcindezi. Ubengakazi azibone zonke izinkundla zokudlala, wakhetha lokhu akubize ngokuthi \"point and shoot.\"\n",
            "Kusenjalo, imakethe kawoyela ithole uxhaso oluqonde ngqo kuleli sonto ngezindaba zokuthi iSaudi Arabia iyeke ukuthumele impahla e-United States. I-US inemininingo esobala kakhulu nebuyekeziwe maqondana nezimakethe zawoyela, okubandakanya ukukhishwa kwamazinga omkhiqizo esonto, okuthengiselwa amanye amazwe nokuthengwa kuvela kwamanye amazwe, kanye nohlu lwempahla. Lolo hlobo lokubonakala alutholakali ngaleso sikhathi ezindaweni eziningi emhlabeni.\n",
            "Izinhlelo ezikhona zokulawula amagagasi avelayo azigculisi ngokwanele noma kuba nzima ukuziqalisa,' kusho u-Ana Díaz-Rubio, umcwaningi weziqu ezilandela ezobudokotela eNyuvesi yase-Aalto.\n",
            "Uma uqala ukusebenzisa ama-hashtag anamaphosti angaphezu kwezi-300,000, kuba nzima kakhulu ukutholakala kuleyo hashtag.\n",
            "IFacebook ivame ukuhambisana nezicelo zikahulumeni ukuvimba okuqukethwe okuphula imithetho yasendaweni.\n",
            "Ukuqedwa kwezindlu kuwela kuma-4.8% kwaya kumayunithi ayisigidi esingu-1.161 ngenyanga edlule, okuyizinga eliphansi kakhulu kusukela ngoDisemba.\n",
            "==> test.dropout.bpe.en <==\n",
            "Pet@@ er V@@ an S@@ ant@@ : And it me@@ ans wh@@ at@@ ?\n",
            "The c@@ ost to soci@@ ety will be sub@@ st@@ ant@@ i@@ al, the report say@@ s. In 2019 al@@ on@@ e, it est@@ im@@ ates a $@@ 2@@ 9@@ 0 bill@@ ion bur@@ d@@ en from health car@@ e, long@@ -@@ term c@@ ase and hosp@@ ice comb@@ in@@ ed. Med@@ ic@@ are and Med@@ ic@@ a@@ id will c@@ over $@@ 19@@ 5 bill@@ ion of that, with out@@ -@@ of@@ -@@ p@@ ock@@ et cost@@ s to car@@ eg@@ iv@@ ers re@@ ach@@ ing $@@ 6@@ 3 bill@@ ion.\n",
            "It is now up to them to make the most of it.\n",
            "\"The C@@ P@@ S is car@@ ef@@ ul@@ ly consid@@ ering all the av@@ ail@@ able inform@@ ation, including the imp@@ act on Harr@@ y's famil@@ y, in order to make an ind@@ epend@@ ent and ob@@ j@@ ective charg@@ ing decis@@ ion.\n",
            "TV aud@@ i@@ ences were left ou@@ tr@@ ag@@ ed after a te@@ en@@ age g@@ ir@@ l appear@@ ed on Ch@@ an@@ ne@@ l 4@@ 's 2@@ 4 H@@ ours in A@@ &@@ E compl@@ aining of a bro@@ k@@ en f@@ ing@@ er n@@ ail@@ .\n",
            "The dis@@ g@@ us@@ ting not@@ e read@@ : '@@ P@@ ut your do@@ g on a lead@@ , sl@@ ag@@ !\n",
            "S@@ en S@@ en C@@ E@@ O S@@ ub@@ h@@ ash Ch@@ all@@ a\n",
            "S@@ om@@ e@@ one behind the c@@ amer@@ a say@@ s: '@@ H@@ i g@@ uy@@ s, h@@ i@@ !\n",
            "\"@@ She was d@@ ev@@ ot@@ ed to children and es@@ p@@ ec@@ ially an@@ imal@@ s, including a w@@ ild fo@@ x who we are continu@@ ing to fe@@ ed now that she has g@@ on@@ e.\"\n",
            "A@@ re there l@@ it@@ er@@ ally ... no other act@@ ors al@@ iv@@ e?\n",
            "\n",
            "==> test.dropout.bpe.zu <==\n",
            "Pet@@ er V@@ an S@@ ant@@ : B@@ ese kusho ukuth@@ in@@ i?\n",
            "Iz@@ in@@ dleko zom@@ phakathi ziz@@ oba zin@@ kul@@ u, ngokusho k@@ omb@@ ik@@ o. Ng@@ onyaka wez@@ i-@@ 2019 uw@@ od@@ wa, umth@@ w@@ alo w@@ ezin@@ dleko zez@@ empilo ul@@ ing@@ anis@@ elwa kuz@@ igid@@ igidi ezingama-@@ $@@ 2@@ 9@@ 0, ku@@ hlangan@@ iswa ukug@@ ula kw@@ esikhathi eside kanye ne-@@ h@@ osp@@ ic@@ e. I-@@ Med@@ ic@@ are ne-@@ Med@@ ic@@ a@@ id iz@@ okh@@ okh@@ ela izin@@ dleko eziy@@ iz@@ igid@@ igidi ezingama-@@ $@@ 19@@ 5 wal@@ okho, ngezin@@ dleko ezikh@@ okh@@ wa ngqo z@@ aban@@ ak@@ ek@@ eli ezif@@ inyelela kuz@@ igid@@ igidi ezingama-@@ $@@ 6@@ 3.\n",
            "Se@@ kul@@ ele kub@@ o ukuthi bay@@ isebenz@@ ise ngendlela ez@@ oba n@@ enz@@ uzo kakhulu.\n",
            "\"@@ I-@@ C@@ P@@ S ic@@ ub@@ ung@@ ul@@ is@@ isa lonke ul@@ wazi ol@@ ukh@@ on@@ a, okub@@ andakanya umth@@ elela em@@ nd@@ en@@ ini ka@@ Harr@@ y, ukuze kuth@@ ath@@ we isinqumo soku@@ kh@@ okh@@ isa es@@ izim@@ ele nes@@ ing@@ ach@@ em@@ ile.\n",
            "Iz@@ ib@@ uk@@ eli z@@ aku@@ -@@ TV z@@ ish@@ iy@@ we zim@@ ang@@ ele emuva kokuba kuv@@ ele int@@ omb@@ azane es@@ akh@@ ula ohl@@ elweni i-@@ 2@@ 4 H@@ ours ku-@@ A@@ &@@ E ku@@ Sh@@ an@@ eli 4 ikh@@ al@@ aza ngokuph@@ uk@@ elwa w@@ uz@@ iph@@ o.\n",
            "Um@@ bh@@ alo ony@@ any@@ isayo ub@@ uf@@ und@@ eka kanj@@ e: '@@ B@@ eka inj@@ a y@@ akho ph@@ amb@@ ili, sl@@ ag@@ !\n",
            "US@@ en S@@ en u-@@ C@@ E@@ O we@@ S@@ ubh@@ ash Ch@@ all@@ a\n",
            "Kun@@ om@@ untu ong@@ emuva kw@@ ekh@@ amer@@ a oth@@ i: S@@ an@@ ibon@@ ani baf@@ ow@@ eth@@ u, s@@ an@@ ibon@@ an@@ i@@ !\n",
            "\"@@ Way@@ ezin@@ ik@@ ele ezing@@ an@@ eni ikh@@ al@@ ukazi nas@@ ezil@@ wan@@ eni, okub@@ andakanya imp@@ ung@@ ush@@ e yas@@ endl@@ e es@@ iq@@ hub@@ ekayo noku@@ y@@ iph@@ ak@@ ela n@@ amanje eng@@ ase@@ kh@@ o.\"\n",
            "I@@ ngabe ngemp@@ ela ... ab@@ ase@@ kho n@@ hlobo abanye abal@@ ing@@ isi abas@@ aph@@ il@@ a@@ ?\n",
            "\n",
            "==> test.en <==\n",
            "Peter Van Sant: And it means what?\n",
            "The cost to society will be substantial, the report says. In 2019 alone, it estimates a $290 billion burden from health care, long-term case and hospice combined. Medicare and Medicaid will cover $195 billion of that, with out-of-pocket costs to caregivers reaching $63 billion.\n",
            "It is now up to them to make the most of it.\n",
            "\"The CPS is carefully considering all the available information, including the impact on Harry's family, in order to make an independent and objective charging decision.\n",
            "TV audiences were left outraged after a teenage girl appeared on Channel 4's 24 Hours in A&E complaining of a broken finger nail.\n",
            "The disgusting note read: 'Put your dog on a lead, slag!\n",
            "Sen Sen CEO Subhash Challa\n",
            "Someone behind the camera says: 'Hi guys, hi!\n",
            "\"She was devoted to children and especially animals, including a wild fox who we are continuing to feed now that she has gone.\"\n",
            "Are there literally ... no other actors alive?\n",
            "\n",
            "==> test.zu <==\n",
            "Peter Van Sant: Bese kusho ukuthini?\n",
            "Izindleko zomphakathi zizoba zinkulu, ngokusho kombiko. Ngonyaka wezi-2019 uwodwa, umthwalo wezindleko zezempilo ulinganiselwa kuzigidigidi ezingama-$290, kuhlanganiswa ukugula kwesikhathi eside kanye ne-hospice.  I-Medicare ne-Medicaid izokhokhela izindleko eziyizigidigidi ezingama-$195 walokho, ngezindleko ezikhokhwa ngqo zabanakekeli ezifinyelela kuzigidigidi ezingama-$63.\n",
            "Sekulele kubo ukuthi bayisebenzise ngendlela ezoba nenzuzo kakhulu.\n",
            "\"I-CPS icubungulisisa lonke ulwazi olukhona, okubandakanya umthelela emndenini kaHarry, ukuze kuthathwe isinqumo sokukhokhisa esizimele nesingachemile.\n",
            "Izibukeli zaku-TV zishiywe zimangele emuva kokuba kuvele intombazane esakhula ohlelweni i-24 Hours ku-A&E kuShaneli 4 ikhalaza ngokuphukelwa wuzipho.\n",
            "Umbhalo onyanyisayo ubufundeka kanje: 'Beka inja yakho phambili, slag!\n",
            "USen Sen u-CEO weSubhash Challa\n",
            "Kunomuntu ongemuva kwekhamera othi: Sanibonani bafowethu, sanibonani!\n",
            "\"Wayezinikele ezinganeni ikhalukazi nasezilwaneni, okubandakanya impungushe yasendle esiqhubekayo nokuyiphakela namanje engasekho.\"\n",
            "Ingabe ngempela ... abasekho nhlobo abanye abalingisi abasaphila?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epeCydmCyS8X"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## Installation of JoeyNMT\n",
        "\n",
        "[JoeyNMT](https://joeynmt.readthedocs.io) is a simple, minimalist NMT package.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iBRMm4kMxZ8L",
        "outputId": "33945f85-281a-42db-b28c-1a7c06da21f8"
      },
      "source": [
        "# Install JoeyNMT\n",
        "! git clone https://github.com/joeynmt/joeynmt.git\n",
        "! cd joeynmt;pip3 install .\n",
        "\n",
        "# Install Pytorch with GPU support v1.7.1.\n",
        "# ! pip uninstall torch\n",
        "! pip install torch==1.10.0+cu102 -f https://download.pytorch.org/whl/torch_stable.html"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'joeynmt'...\n",
            "remote: Enumerating objects: 3224, done.\u001b[K\n",
            "remote: Counting objects: 100% (273/273), done.\u001b[K\n",
            "remote: Compressing objects: 100% (143/143), done.\u001b[K\n",
            "remote: Total 3224 (delta 155), reused 209 (delta 130), pack-reused 2951\u001b[K\n",
            "Receiving objects: 100% (3224/3224), 8.18 MiB | 15.78 MiB/s, done.\n",
            "Resolving deltas: 100% (2184/2184), done.\n",
            "Processing /content/joeynmt\n",
            "\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
            "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (0.16.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (7.1.2)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (1.19.5)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (57.4.0)\n",
            "Requirement already satisfied: torch>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (1.10.0+cu102)\n",
            "Requirement already satisfied: tensorboard>=1.15 in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (2.7.0)\n",
            "Requirement already satisfied: torchtext>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (0.11.0)\n",
            "Requirement already satisfied: sacrebleu>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (2.0.0)\n",
            "Requirement already satisfied: subword-nmt in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (0.3.7)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (3.2.2)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (0.11.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (6.0)\n",
            "Requirement already satisfied: pylint>=2.9.6 in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (2.11.1)\n",
            "Requirement already satisfied: six==1.12 in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (1.12.0)\n",
            "Requirement already satisfied: wrapt==1.11.1 in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (1.11.1)\n",
            "Requirement already satisfied: toml>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from pylint>=2.9.6->joeynmt==1.3) (0.10.2)\n",
            "Requirement already satisfied: platformdirs>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pylint>=2.9.6->joeynmt==1.3) (2.4.0)\n",
            "Requirement already satisfied: mccabe<0.7,>=0.6 in /usr/local/lib/python3.7/dist-packages (from pylint>=2.9.6->joeynmt==1.3) (0.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.10.0 in /usr/local/lib/python3.7/dist-packages (from pylint>=2.9.6->joeynmt==1.3) (3.10.0.2)\n",
            "Requirement already satisfied: isort<6,>=4.2.5 in /usr/local/lib/python3.7/dist-packages (from pylint>=2.9.6->joeynmt==1.3) (5.10.1)\n",
            "Requirement already satisfied: astroid<2.9,>=2.8.0 in /usr/local/lib/python3.7/dist-packages (from pylint>=2.9.6->joeynmt==1.3) (2.8.6)\n",
            "Requirement already satisfied: typed-ast<2.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from astroid<2.9,>=2.8.0->pylint>=2.9.6->joeynmt==1.3) (1.5.0)\n",
            "Requirement already satisfied: lazy-object-proxy>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from astroid<2.9,>=2.8.0->pylint>=2.9.6->joeynmt==1.3) (1.6.0)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.7/dist-packages (from sacrebleu>=2.0.0->joeynmt==1.3) (0.8.9)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.7/dist-packages (from sacrebleu>=2.0.0->joeynmt==1.3) (0.4.4)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from sacrebleu>=2.0.0->joeynmt==1.3) (2019.12.20)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.7/dist-packages (from sacrebleu>=2.0.0->joeynmt==1.3) (2.3.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (2.23.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (1.8.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (0.37.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (0.12.0)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (3.17.3)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (1.42.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (0.6.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (3.3.6)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (1.0.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (1.35.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=1.15->joeynmt==1.3) (4.7.2)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=1.15->joeynmt==1.3) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=1.15->joeynmt==1.3) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.15->joeynmt==1.3) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=1.15->joeynmt==1.3) (4.8.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=1.15->joeynmt==1.3) (3.6.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=1.15->joeynmt==1.3) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.15->joeynmt==1.3) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.15->joeynmt==1.3) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.15->joeynmt==1.3) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.15->joeynmt==1.3) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.15->joeynmt==1.3) (3.1.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext>=0.10.0->joeynmt==1.3) (4.62.3)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->joeynmt==1.3) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->joeynmt==1.3) (3.0.6)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->joeynmt==1.3) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->joeynmt==1.3) (1.3.2)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.7/dist-packages (from seaborn->joeynmt==1.3) (1.4.1)\n",
            "Requirement already satisfied: pandas>=0.23 in /usr/local/lib/python3.7/dist-packages (from seaborn->joeynmt==1.3) (1.1.5)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23->seaborn->joeynmt==1.3) (2018.9)\n",
            "Building wheels for collected packages: joeynmt\n",
            "  Building wheel for joeynmt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for joeynmt: filename=joeynmt-1.3-py3-none-any.whl size=86029 sha256=45e3e6618b8f557e643db52285a0f3edaab085a99ee7025c6c9bac1d1541ce47\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-5_qi3ir6/wheels/0a/f4/bf/6c9d3b8efbfece6cd209f865be37382b02e7c3584df2e28ca4\n",
            "Successfully built joeynmt\n",
            "Installing collected packages: joeynmt\n",
            "  Attempting uninstall: joeynmt\n",
            "    Found existing installation: joeynmt 1.3\n",
            "    Uninstalling joeynmt-1.3:\n",
            "      Successfully uninstalled joeynmt-1.3\n",
            "Successfully installed joeynmt-1.3\n",
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Requirement already satisfied: torch==1.10.0+cu102 in /usr/local/lib/python3.7/dist-packages (1.10.0+cu102)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.10.0+cu102) (3.10.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUxy4M4wIaek"
      },
      "source": [
        "from os import path\n",
        "os.environ[\"src\"] = source_language # Sets them in bash as well, since we often use bash scripts\n",
        "os.environ[\"tgt\"] = target_language\n",
        "\n",
        "# Learn BPEs on the training data.\n",
        "os.environ[\"data_path\"] = path.join(\"joeynmt\", \"data\", source_language + target_language) # Herman! "
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TqMMWLLQjnEE"
      },
      "source": [
        "# 1. Preprocessing the data into sentencepiece char tokens"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cCJb8Dr8ji2K",
        "outputId": "f7867665-d551-43b3-9e9c-214ca6cfc6c0"
      },
      "source": [
        "! pip install sentencepiece"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (0.1.96)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BQGyV3aCjzgK"
      },
      "source": [
        "import sentencepiece as spm\n",
        "\n",
        "# train sentencepiece model\n",
        "# makes segmenter instance\n",
        "# encode: file\n",
        "spm.SentencePieceTrainer.train('--input=train.en,train.zu --model_prefix=m_char --vocab_size=2000 --model_type=char')\n",
        "\n",
        "sp = spm.SentencePieceProcessor(model_file=\"m_char.model\")\n",
        "\n",
        "with open(\"train.en\",\"r\") as r_train_en, open(\"train.char.\"+source_language,\"w\") as w_train_char_en, \\\n",
        "open(\"train.zu\",\"r\") as r_train_zu, open(\"train.char.\"+target_language,\"w\") as w_train_char_zu:\n",
        "  for line in r_train_en:\n",
        "    w_train_char_en.write(\" \".join(sp.encode(line, out_type=str))+\"\\n\")\n",
        "  for line in r_train_zu:\n",
        "    w_train_char_zu.write(\" \".join(sp.encode(line, out_type=str))+\"\\n\")\n",
        "\n",
        "with open(\"dev.en\",\"r\") as r_dev_en, open(\"dev.char.\"+source_language,\"w\") as w_dev_char_en, \\\n",
        "open(\"dev.zu\",\"r\") as r_dev_zu, open(\"dev.char.\"+target_language,\"w\") as w_dev_char_zu:\n",
        "  for line in r_dev_en:\n",
        "    w_dev_char_en.write(\" \".join(sp.encode(line, out_type=str))+\"\\n\")\n",
        "  for line in r_dev_zu:\n",
        "    w_dev_char_zu.write(\" \".join(sp.encode(line, out_type=str))+\"\\n\")\n",
        "\n",
        "with open(\"test.en\",\"r\") as r_test_en, open(\"test.char.\"+source_language,\"w\") as w_test_char_en, \\\n",
        "open(\"test.zu\",\"r\") as r_test_zu, open(\"test.char.\"+target_language,\"w\") as w_test_char_zu:\n",
        "  for line in r_test_en:\n",
        "    w_test_char_en.write(\" \".join(sp.encode(line, out_type=str))+\"\\n\")\n",
        "  for line in r_test_zu:\n",
        "    w_test_char_zu.write(\" \".join(sp.encode(line, out_type=str))+\"\\n\")\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGF3XAJzuhiU"
      },
      "source": [
        "# 2. Preprocessing the Data into Subword BPE Tokens without dropout"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VdGaY_-oug18"
      },
      "source": [
        "# Do subword NMT\n",
        "\n",
        "# Learn BPEs on the training data.\n",
        "! subword-nmt learn-joint-bpe-and-vocab --input train.$src train.$tgt -s 4000 -o bpe.codes.4000 --write-vocabulary vocab.$src vocab.$tgt\n",
        "\n",
        "# Apply BPE splits to the development and test data.\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$src < train.$src > train.bpe.$src\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$tgt < train.$tgt > train.bpe.$tgt\n",
        "\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$src < dev.$src > dev.bpe.$src\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$tgt < dev.$tgt > dev.bpe.$tgt\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$src < test.$src > test.bpe.$src\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$tgt < test.$tgt > test.bpe.$tgt\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AaE77Tcppex9"
      },
      "source": [
        "# 3. Preprocessing the Data into Subword BPE Tokens with dropout\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-TyjtmXB1mL"
      },
      "source": [
        "# Do subword NMT\n",
        "from os import path\n",
        "os.environ['src'] = source_language # Sets them in bash as well, since we often use bash scripts\n",
        "os.environ['tgt'] = target_language\n",
        "\n",
        "# Learn BPEs on the training data.\n",
        "os.environ['data_path'] = path.join('joeynmt', 'data', source_language + target_language)\n",
        "! subword-nmt learn-joint-bpe-and-vocab --input train.$src train.$tgt -s 4000 -o bpe.codes.4000 --write-vocabulary vocab.$src vocab.$tgt\n",
        "\n",
        "# Apply BPE splits to the development and test data.\n",
        "! subword-nmt apply-bpe --dropout 0.1 -c bpe.codes.4000 --vocabulary vocab.$src < train.$src > train.dropout.bpe.$src\n",
        "! subword-nmt apply-bpe --dropout 0.1 -c bpe.codes.4000 --vocabulary vocab.$tgt < train.$tgt > train.dropout.bpe.$tgt\n",
        "\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$src < dev.$src > dev.dropout.bpe.$src\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$tgt < dev.$tgt > dev.dropout.bpe.$tgt\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$src < test.$src > test.dropout.bpe.$src\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$tgt < test.$tgt > test.dropout.bpe.$tgt"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPJB5oXaHeX3"
      },
      "source": [
        "# 4. Preprocessing the data into sentencepiece char tokens"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMgu3woIHoEB"
      },
      "source": [
        "# train sentencepiece model\n",
        "# makes segmenter instance\n",
        "# encode: file\n",
        "spm.SentencePieceTrainer.train('--input=train.en,train.zu --model_prefix=m_word --vocab_size=2000 --model_type=word')\n",
        "\n",
        "sp = spm.SentencePieceProcessor(model_file=\"m_word.model\")\n",
        "\n",
        "with open(\"train.en\",\"r\") as r_train_en, open(\"train.word.\"+source_language,\"w\") as w_train_word_en, \\\n",
        "open(\"train.zu\",\"r\") as r_train_zu, open(\"train.word.\"+target_language,\"w\") as w_train_word_zu:\n",
        "  for line in r_train_en:\n",
        "    w_train_word_en.write(\" \".join(sp.encode(line, out_type=str))+\"\\n\")\n",
        "  for line in r_train_zu:\n",
        "    w_train_word_zu.write(\" \".join(sp.encode(line, out_type=str))+\"\\n\")\n",
        "\n",
        "with open(\"dev.en\",\"r\") as r_dev_en, open(\"dev.word.\"+source_language,\"w\") as w_dev_word_en, \\\n",
        "open(\"dev.zu\",\"r\") as r_dev_zu, open(\"dev.word.\"+target_language,\"w\") as w_dev_word_zu:\n",
        "  for line in r_dev_en:\n",
        "    w_dev_word_en.write(\" \".join(sp.encode(line, out_type=str))+\"\\n\")\n",
        "  for line in r_dev_zu:\n",
        "    w_dev_word_zu.write(\" \".join(sp.encode(line, out_type=str))+\"\\n\")\n",
        "\n",
        "with open(\"test.en\",\"r\") as r_test_en, open(\"test.word.\"+source_language,\"w\") as w_test_word_en, \\\n",
        "open(\"test.zu\",\"r\") as r_test_zu, open(\"test.word.\"+target_language,\"w\") as w_test_word_zu:\n",
        "  for line in r_test_en:\n",
        "    w_test_word_en.write(\" \".join(sp.encode(line, out_type=str))+\"\\n\")\n",
        "  for line in r_test_zu:\n",
        "    w_test_word_zu.write(\" \".join(sp.encode(line, out_type=str))+\"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wp09MIztOJi0",
        "outputId": "d09a38fd-8c75-444e-bbd3-e1b3576936a6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Create directory, move everyone we care about to the correct location\n",
        "! mkdir -p $data_path\n",
        "! cp *.en $data_path\n",
        "! cp *.zu $data_path\n",
        "! cp bpe.codes.4000 $data_path\n",
        "! cp m.* $data_path\n",
        "! ls $data_path\n",
        "\n",
        "# Also move everything we care about to a mounted location in google drive (relevant if running in colab) at gdrive_path\n",
        "! cp *.en \"$gdrive_path\"\n",
        "! cp *.zu \"$gdrive_path\"\n",
        "! cp bpe.codes.4000 \"$gdrive_path\"\n",
        "! cp m.* \"$gdrive_path\"\n",
        "! ls \"$gdrive_path\"\n",
        "\n",
        "# Create that vocab using build_vocab\n",
        "! sudo chmod 777 joeynmt/scripts/build_vocab.py\n",
        "! joeynmt/scripts/build_vocab.py joeynmt/data/$src$tgt/train.bpe.$src joeynmt/data/$src$tgt/train.bpe.$tgt --output_path joeynmt/data/$src$tgt/vocab.txt\n",
        "! joeynmt/scripts/build_vocab.py joeynmt/data/$src$tgt/train.char.$src joeynmt/data/$src$tgt/train.char.$tgt --output_path joeynmt/data/$src$tgt/m.vocab\n",
        "\n",
        "# Some output\n",
        "! echo \"BPE Zulu Sentences\"\n",
        "! tail -n 5 test.bpe.$tgt\n",
        "! echo \"Combined BPE Vocab\"\n",
        "! tail -n 10 joeynmt/data/$src$tgt/m.vocab  # Herman"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cp: cannot stat 'm.*': No such file or directory\n",
            "bpe.codes.4000\t    test.bpe.en\t\t train.bpe.zu\n",
            "dev.bpe.en\t    test.bpe.zu\t\t train.char.en\n",
            "dev.bpe.zu\t    test.char.en\t train.char.zu\n",
            "dev.char.en\t    test.char.zu\t train.dropout.bpe.en\n",
            "dev.char.zu\t    test.dropout.bpe.en  train.dropout.bpe.zu\n",
            "dev.dropout.bpe.en  test.dropout.bpe.zu  train.en\n",
            "dev.dropout.bpe.zu  test.en\t\t train.zu\n",
            "dev.en\t\t    test.zu\t\t vocab.en\n",
            "dev.zu\t\t    train.bpe.en\t vocab.zu\n",
            "cp: cannot stat 'm.*': No such file or directory\n",
            "bpe.codes.4000\t    test.bpe.en\t\t train.bpe.zu\n",
            "dev.bpe.en\t    test.bpe.zu\t\t train.char.en\n",
            "dev.bpe.zu\t    test.char.en\t train.char.zu\n",
            "dev.char.en\t    test.char.zu\t train.dropout.bpe.en\n",
            "dev.char.zu\t    test.dropout.bpe.en  train.dropout.bpe.zu\n",
            "dev.dropout.bpe.en  test.dropout.bpe.zu  train.en\n",
            "dev.dropout.bpe.zu  test.en\t\t train.zu\n",
            "dev.en\t\t    test.zu\t\t vocab.en\n",
            "dev.zu\t\t    train.bpe.en\t vocab.zu\n",
            "BPE Zulu Sentences\n",
            "In@@ dlela ay@@ ekh@@ uluma ngayo emuva kwal@@ okho, kwakung@@ athi akw@@ enzek@@ anga l@@ utho is@@ onto l@@ onke.\n",
            "I-@@ T@@ ott@@ en@@ ham iz@@ on@@ q@@ ob@@ a: idl@@ ala ngoku@@ ling@@ ana\n",
            "B@@ ese (@@ i-@@ New York J@@ et@@ s@@ '@@ ) i-@@ Le@@ V@@ e@@ on B@@ ell iy@@ ez@@ a, ukuze ukw@@ azi ukug@@ ij@@ ima oku@@ phambili se@@ ku@@ bu@@ ya ihl@@ andla l@@ esib@@ ili.\n",
            "\"@@ Ang@@ ik@@ amb@@ uzi ng@@ akh@@ o,\" kusho u@@ Pa@@ ul@@ son wase@@ B@@ ul@@ loc@@ k, o@@ dlala ek@@ a@@ D@@ eb@@ b@@ ie Oc@@ e@@ an. \"Ng@@ iz@@ okwenza lokho ngoba ngoba ng@@ ifuna ukubona ukuthi ing@@ abe siz@@ ok@@ e si@@ kw@@ enze yini lokh@@ o. Kung@@ aba m@@ nandi kakhulu@@ .\"\n",
            "Um@@ z@@ amo ub@@ andakanya imiphumela y@@ os@@ om@@ abh@@ izinisi aban@@ amandla kakhulu, aban@@ x@@ en@@ x@@ i kanye n@@ abab@@ eng@@ aman@@ x@@ usa ab@@ aq@@ oq@@ ela u@@ George W@@ . B@@ ush@@ , u@@ John Mc@@ C@@ ain noM@@ it@@ t R@@ om@@ ney imali enkulu - futhi manje abal@@ ung@@ is@@ elela uku@@ ngena ku@@ man@@ eth@@ iwe@@ kh@@ i amakhulu b@@ enz@@ ela uTrump emuva koku@@ ch@@ ith@@ wa kom@@ kh@@ ankaso wakhe w@@ ob@@ om@@ ongameli woku@@ qal@@ a.\n",
            "Combined BPE Vocab\n",
            "●\n",
            "ç\n",
            "\n",
            "′′\n",
            "â\n",
            "******\n",
            "â\n",
            "*****\n",
            "***\n",
            "*******\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ixmzi60WsUZ8"
      },
      "source": [
        "# Creating the JoeyNMT Config\n",
        "\n",
        "JoeyNMT requires a yaml config. We provide a template below. We've also set a number of defaults with it, that you may play with!\n",
        "\n",
        "- We used Transformer architecture \n",
        "- We set our dropout to reasonably high: 0.3 (recommended in  [(Sennrich, 2019)](https://www.aclweb.org/anthology/P19-1021))\n",
        "\n",
        "Things worth playing with:\n",
        "- The batch size (also recommended to change for low-resourced languages)\n",
        "- The number of epochs (we've set it at 30 just so it runs in about an hour, for testing purposes)\n",
        "- The decoder options (beam_size, alpha)\n",
        "- Evaluation metrics (BLEU versus Crhf4)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "PIs1lY2hxMsl"
      },
      "source": [
        "name = '%s%s' % (source_language, target_language)\n",
        "gdrive_path = os.environ[\"gdrive_path\"]\n",
        "\n",
        "# Create the config\n",
        "config = \"\"\"\n",
        "name: \"{name}_transformer\"\n",
        "\n",
        "data:\n",
        "    src: \"{source_language}\"\n",
        "    trg: \"{target_language}\"\n",
        "    train: \"data/{name}/train.char\"\n",
        "    dev:   \"data/{name}/dev.char\"\n",
        "    test:  \"data/{name}/test.char\"\n",
        "    level: \"char\"\n",
        "    lowercase: False\n",
        "    max_sent_length: 100\n",
        "    src_vocab: \"data/{name}/m.vocab\"\n",
        "    trg_vocab: \"data/{name}/m.vocab\"\n",
        "\n",
        "testing:\n",
        "    beam_size: 5\n",
        "    alpha: 1.0\n",
        "\n",
        "training:\n",
        "    #load_model: \"{gdrive_path}/models/{name}_transformer/1.ckpt\" # if uncommented, load a pre-trained model from this checkpoint\n",
        "    random_seed: 42\n",
        "    optimizer: \"adam\"\n",
        "    normalization: \"tokens\"\n",
        "    adam_betas: [0.9, 0.999] \n",
        "    scheduling: \"plateau\"           # TODO: try switching from plateau to Noam scheduling\n",
        "    patience: 5                     # For plateau: decrease learning rate by decrease_factor if validation score has not improved for this many validation rounds.\n",
        "    learning_rate_factor: 0.5       # factor for Noam scheduler (used with Transformer)\n",
        "    learning_rate_warmup: 1000      # warmup steps for Noam scheduler (used with Transformer)\n",
        "    decrease_factor: 0.7\n",
        "    loss: \"crossentropy\"\n",
        "    learning_rate: 0.0003\n",
        "    learning_rate_min: 0.00000001\n",
        "    weight_decay: 0.0\n",
        "    label_smoothing: 0.1\n",
        "    batch_size: 4096\n",
        "    batch_type: \"token\"\n",
        "    eval_batch_size: 3600\n",
        "    eval_batch_type: \"token\"\n",
        "    batch_multiplier: 1\n",
        "    early_stopping_metric: \"ppl\"\n",
        "    epochs: 1000                     # TODO: Decrease for when playing around and checking of working. Around 30 is sufficient to check if its working at all\n",
        "    validation_freq: 1000          # TODO: Set to at least once per epoch.\n",
        "    logging_freq: 100\n",
        "    eval_metric: \"bleu\"\n",
        "    model_dir: \"models/{name}_transformer\"\n",
        "    overwrite: False               # TODO: Set to True if you want to overwrite possibly existing models. \n",
        "    shuffle: True\n",
        "    use_cuda: True\n",
        "    max_output_length: 100\n",
        "    print_valid_sents: [0, 1, 2, 3]\n",
        "    keep_last_ckpts: 3\n",
        "\n",
        "model:\n",
        "    initializer: \"xavier\"\n",
        "    bias_initializer: \"zeros\"\n",
        "    init_gain: 1.0\n",
        "    embed_initializer: \"xavier\"\n",
        "    embed_init_gain: 1.0\n",
        "    tied_embeddings: True\n",
        "    tied_softmax: True\n",
        "    encoder:\n",
        "        type: \"transformer\"\n",
        "        num_layers: 6\n",
        "        num_heads: 4             # TODO: Increase to 8 for larger data.\n",
        "        embeddings:\n",
        "            embedding_dim: 256   # TODO: Increase to 512 for larger data.\n",
        "            scale: True\n",
        "            dropout: 0.2\n",
        "        # typically ff_size = 4 x hidden_size\n",
        "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
        "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
        "        dropout: 0.3\n",
        "    decoder:\n",
        "        type: \"transformer\"\n",
        "        num_layers: 6\n",
        "        num_heads: 4              # TODO: Increase to 8 for larger data.\n",
        "        embeddings:\n",
        "            embedding_dim: 256    # TODO: Increase to 512 for larger data.\n",
        "            scale: True\n",
        "            dropout: 0.2\n",
        "        # typically ff_size = 4 x hidden_size\n",
        "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
        "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
        "        dropout: 0.3\n",
        "\"\"\".format(name=name, gdrive_path=os.environ[\"gdrive_path\"], source_language=source_language, target_language=target_language)\n",
        "with open(\"joeynmt/configs/transformer_{name}.yaml\".format(name=name),'w') as f:\n",
        "    f.write(config)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIifxE3Qzuvs"
      },
      "source": [
        "# Train the Model\n",
        "\n",
        "This single line of joeynmt runs the training using the config we made above"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "6ZBPFwT94WpI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55445f8c-10ee-4c89-ccb6-7fe65f4d4842"
      },
      "source": [
        "# Train the model\n",
        "\n",
        "!cd joeynmt; python3 -m joeynmt train configs/transformer_$src$tgt.yaml"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2021-11-23 23:11:25,712 - INFO - root - Hello! This is Joey-NMT (version 1.3).\n",
            "2021-11-23 23:11:25,737 - INFO - joeynmt.data - Loading training data...\n",
            "2021-11-23 23:11:25,956 - INFO - joeynmt.data - Building vocabulary...\n",
            "2021-11-23 23:11:25,957 - INFO - joeynmt.data - Loading dev data...\n",
            "2021-11-23 23:11:25,967 - INFO - joeynmt.data - Loading test data...\n",
            "2021-11-23 23:11:25,974 - INFO - joeynmt.data - Data loaded.\n",
            "2021-11-23 23:11:25,974 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
            "2021-11-23 23:11:26,219 - INFO - joeynmt.model - Enc-dec model built.\n",
            "2021-11-23 23:11:28,676 - INFO - joeynmt.training - Total params: 11095296\n",
            "2021-11-23 23:11:28,679 - WARNING - joeynmt.training - `keep_last_ckpts` option is outdated. Please use `keep_best_ckpts`, instead.\n",
            "2021-11-23 23:11:30,889 - INFO - joeynmt.helpers - cfg.name                           : enzu_transformer\n",
            "2021-11-23 23:11:30,889 - INFO - joeynmt.helpers - cfg.data.src                       : en\n",
            "2021-11-23 23:11:30,889 - INFO - joeynmt.helpers - cfg.data.trg                       : zu\n",
            "2021-11-23 23:11:30,889 - INFO - joeynmt.helpers - cfg.data.train                     : data/enzu/train.char\n",
            "2021-11-23 23:11:30,889 - INFO - joeynmt.helpers - cfg.data.dev                       : data/enzu/dev.char\n",
            "2021-11-23 23:11:30,890 - INFO - joeynmt.helpers - cfg.data.test                      : data/enzu/test.char\n",
            "2021-11-23 23:11:30,890 - INFO - joeynmt.helpers - cfg.data.level                     : char\n",
            "2021-11-23 23:11:30,890 - INFO - joeynmt.helpers - cfg.data.lowercase                 : False\n",
            "2021-11-23 23:11:30,890 - INFO - joeynmt.helpers - cfg.data.max_sent_length           : 100\n",
            "2021-11-23 23:11:30,890 - INFO - joeynmt.helpers - cfg.data.src_vocab                 : data/enzu/m.vocab\n",
            "2021-11-23 23:11:30,890 - INFO - joeynmt.helpers - cfg.data.trg_vocab                 : data/enzu/m.vocab\n",
            "2021-11-23 23:11:30,890 - INFO - joeynmt.helpers - cfg.testing.beam_size              : 5\n",
            "2021-11-23 23:11:30,890 - INFO - joeynmt.helpers - cfg.testing.alpha                  : 1.0\n",
            "2021-11-23 23:11:30,891 - INFO - joeynmt.helpers - cfg.training.random_seed           : 42\n",
            "2021-11-23 23:11:30,891 - INFO - joeynmt.helpers - cfg.training.optimizer             : adam\n",
            "2021-11-23 23:11:30,891 - INFO - joeynmt.helpers - cfg.training.normalization         : tokens\n",
            "2021-11-23 23:11:30,891 - INFO - joeynmt.helpers - cfg.training.adam_betas            : [0.9, 0.999]\n",
            "2021-11-23 23:11:30,891 - INFO - joeynmt.helpers - cfg.training.scheduling            : plateau\n",
            "2021-11-23 23:11:30,891 - INFO - joeynmt.helpers - cfg.training.patience              : 5\n",
            "2021-11-23 23:11:30,891 - INFO - joeynmt.helpers - cfg.training.learning_rate_factor  : 0.5\n",
            "2021-11-23 23:11:30,891 - INFO - joeynmt.helpers - cfg.training.learning_rate_warmup  : 1000\n",
            "2021-11-23 23:11:30,892 - INFO - joeynmt.helpers - cfg.training.decrease_factor       : 0.7\n",
            "2021-11-23 23:11:30,892 - INFO - joeynmt.helpers - cfg.training.loss                  : crossentropy\n",
            "2021-11-23 23:11:30,892 - INFO - joeynmt.helpers - cfg.training.learning_rate         : 0.0003\n",
            "2021-11-23 23:11:30,892 - INFO - joeynmt.helpers - cfg.training.learning_rate_min     : 1e-08\n",
            "2021-11-23 23:11:30,892 - INFO - joeynmt.helpers - cfg.training.weight_decay          : 0.0\n",
            "2021-11-23 23:11:30,892 - INFO - joeynmt.helpers - cfg.training.label_smoothing       : 0.1\n",
            "2021-11-23 23:11:30,892 - INFO - joeynmt.helpers - cfg.training.batch_size            : 4096\n",
            "2021-11-23 23:11:30,892 - INFO - joeynmt.helpers - cfg.training.batch_type            : token\n",
            "2021-11-23 23:11:30,892 - INFO - joeynmt.helpers - cfg.training.eval_batch_size       : 3600\n",
            "2021-11-23 23:11:30,893 - INFO - joeynmt.helpers - cfg.training.eval_batch_type       : token\n",
            "2021-11-23 23:11:30,893 - INFO - joeynmt.helpers - cfg.training.batch_multiplier      : 1\n",
            "2021-11-23 23:11:30,893 - INFO - joeynmt.helpers - cfg.training.early_stopping_metric : ppl\n",
            "2021-11-23 23:11:30,893 - INFO - joeynmt.helpers - cfg.training.epochs                : 1000\n",
            "2021-11-23 23:11:30,893 - INFO - joeynmt.helpers - cfg.training.validation_freq       : 1000\n",
            "2021-11-23 23:11:30,893 - INFO - joeynmt.helpers - cfg.training.logging_freq          : 100\n",
            "2021-11-23 23:11:30,893 - INFO - joeynmt.helpers - cfg.training.eval_metric           : bleu\n",
            "2021-11-23 23:11:30,893 - INFO - joeynmt.helpers - cfg.training.model_dir             : models/enzu_transformer\n",
            "2021-11-23 23:11:30,893 - INFO - joeynmt.helpers - cfg.training.overwrite             : False\n",
            "2021-11-23 23:11:30,894 - INFO - joeynmt.helpers - cfg.training.shuffle               : True\n",
            "2021-11-23 23:11:30,894 - INFO - joeynmt.helpers - cfg.training.use_cuda              : True\n",
            "2021-11-23 23:11:30,894 - INFO - joeynmt.helpers - cfg.training.max_output_length     : 100\n",
            "2021-11-23 23:11:30,894 - INFO - joeynmt.helpers - cfg.training.print_valid_sents     : [0, 1, 2, 3]\n",
            "2021-11-23 23:11:30,894 - INFO - joeynmt.helpers - cfg.training.keep_last_ckpts       : 3\n",
            "2021-11-23 23:11:30,894 - INFO - joeynmt.helpers - cfg.model.initializer              : xavier\n",
            "2021-11-23 23:11:30,894 - INFO - joeynmt.helpers - cfg.model.bias_initializer         : zeros\n",
            "2021-11-23 23:11:30,894 - INFO - joeynmt.helpers - cfg.model.init_gain                : 1.0\n",
            "2021-11-23 23:11:30,895 - INFO - joeynmt.helpers - cfg.model.embed_initializer        : xavier\n",
            "2021-11-23 23:11:30,895 - INFO - joeynmt.helpers - cfg.model.embed_init_gain          : 1.0\n",
            "2021-11-23 23:11:30,895 - INFO - joeynmt.helpers - cfg.model.tied_embeddings          : True\n",
            "2021-11-23 23:11:30,895 - INFO - joeynmt.helpers - cfg.model.tied_softmax             : True\n",
            "2021-11-23 23:11:30,895 - INFO - joeynmt.helpers - cfg.model.encoder.type             : transformer\n",
            "2021-11-23 23:11:30,895 - INFO - joeynmt.helpers - cfg.model.encoder.num_layers       : 6\n",
            "2021-11-23 23:11:30,895 - INFO - joeynmt.helpers - cfg.model.encoder.num_heads        : 4\n",
            "2021-11-23 23:11:30,895 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.embedding_dim : 256\n",
            "2021-11-23 23:11:30,896 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.scale : True\n",
            "2021-11-23 23:11:30,896 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.dropout : 0.2\n",
            "2021-11-23 23:11:30,896 - INFO - joeynmt.helpers - cfg.model.encoder.hidden_size      : 256\n",
            "2021-11-23 23:11:30,896 - INFO - joeynmt.helpers - cfg.model.encoder.ff_size          : 1024\n",
            "2021-11-23 23:11:30,896 - INFO - joeynmt.helpers - cfg.model.encoder.dropout          : 0.3\n",
            "2021-11-23 23:11:30,896 - INFO - joeynmt.helpers - cfg.model.decoder.type             : transformer\n",
            "2021-11-23 23:11:30,896 - INFO - joeynmt.helpers - cfg.model.decoder.num_layers       : 6\n",
            "2021-11-23 23:11:30,896 - INFO - joeynmt.helpers - cfg.model.decoder.num_heads        : 4\n",
            "2021-11-23 23:11:30,896 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.embedding_dim : 256\n",
            "2021-11-23 23:11:30,897 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.scale : True\n",
            "2021-11-23 23:11:30,897 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.dropout : 0.2\n",
            "2021-11-23 23:11:30,897 - INFO - joeynmt.helpers - cfg.model.decoder.hidden_size      : 256\n",
            "2021-11-23 23:11:30,897 - INFO - joeynmt.helpers - cfg.model.decoder.ff_size          : 1024\n",
            "2021-11-23 23:11:30,897 - INFO - joeynmt.helpers - cfg.model.decoder.dropout          : 0.3\n",
            "2021-11-23 23:11:30,897 - INFO - joeynmt.helpers - Data set sizes: \n",
            "\ttrain 58,\n",
            "\tvalid 598,\n",
            "\ttest 398\n",
            "2021-11-23 23:11:30,897 - INFO - joeynmt.helpers - First training example:\n",
            "\t[SRC] ▁   5   .   ▁   C   l   u   e   ▁   (   1   9   8   5   )\n",
            "\t[TRG] ▁   5   .   ▁   C   l   u   e   ▁   (   1   9   8   5   )\n",
            "2021-11-23 23:11:30,897 - INFO - joeynmt.helpers - First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) ▁ (5) e (6) a (7) i (8) n (9) o\n",
            "2021-11-23 23:11:30,898 - INFO - joeynmt.helpers - First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) ▁ (5) e (6) a (7) i (8) n (9) o\n",
            "2021-11-23 23:11:30,898 - INFO - joeynmt.helpers - Number of Src words (types): 137\n",
            "2021-11-23 23:11:30,898 - INFO - joeynmt.helpers - Number of Trg words (types): 137\n",
            "2021-11-23 23:11:30,898 - INFO - joeynmt.training - Model(\n",
            "\tencoder=TransformerEncoder(num_layers=6, num_heads=4),\n",
            "\tdecoder=TransformerDecoder(num_layers=6, num_heads=4),\n",
            "\tsrc_embed=Embeddings(embedding_dim=256, vocab_size=137),\n",
            "\ttrg_embed=Embeddings(embedding_dim=256, vocab_size=137))\n",
            "2021-11-23 23:11:30,899 - INFO - joeynmt.training - Train stats:\n",
            "\tdevice: cuda\n",
            "\tn_gpu: 1\n",
            "\t16-bits training: False\n",
            "\tgradient accumulation: 1\n",
            "\tbatch size per device: 4096\n",
            "\ttotal batch size (w. parallel & accumulation): 4096\n",
            "2021-11-23 23:11:30,899 - INFO - joeynmt.training - EPOCH 1\n",
            "2021-11-23 23:11:31,617 - INFO - joeynmt.training - Epoch   1: total training loss 9.65\n",
            "2021-11-23 23:11:31,618 - INFO - joeynmt.training - EPOCH 2\n",
            "2021-11-23 23:11:32,178 - INFO - joeynmt.training - Epoch   2: total training loss 6.32\n",
            "2021-11-23 23:11:32,179 - INFO - joeynmt.training - EPOCH 3\n",
            "2021-11-23 23:11:32,747 - INFO - joeynmt.training - Epoch   3: total training loss 5.51\n",
            "2021-11-23 23:11:32,747 - INFO - joeynmt.training - EPOCH 4\n",
            "2021-11-23 23:11:33,321 - INFO - joeynmt.training - Epoch   4: total training loss 4.92\n",
            "2021-11-23 23:11:33,321 - INFO - joeynmt.training - EPOCH 5\n",
            "2021-11-23 23:11:33,879 - INFO - joeynmt.training - Epoch   5: total training loss 4.73\n",
            "2021-11-23 23:11:33,879 - INFO - joeynmt.training - EPOCH 6\n",
            "2021-11-23 23:11:34,443 - INFO - joeynmt.training - Epoch   6: total training loss 4.49\n",
            "2021-11-23 23:11:34,444 - INFO - joeynmt.training - EPOCH 7\n",
            "2021-11-23 23:11:35,006 - INFO - joeynmt.training - Epoch   7: total training loss 4.43\n",
            "2021-11-23 23:11:35,006 - INFO - joeynmt.training - EPOCH 8\n",
            "2021-11-23 23:11:35,567 - INFO - joeynmt.training - Epoch   8: total training loss 4.31\n",
            "2021-11-23 23:11:35,568 - INFO - joeynmt.training - EPOCH 9\n",
            "2021-11-23 23:11:36,141 - INFO - joeynmt.training - Epoch   9: total training loss 4.21\n",
            "2021-11-23 23:11:36,141 - INFO - joeynmt.training - EPOCH 10\n",
            "2021-11-23 23:11:36,702 - INFO - joeynmt.training - Epoch  10: total training loss 4.14\n",
            "2021-11-23 23:11:36,703 - INFO - joeynmt.training - EPOCH 11\n",
            "2021-11-23 23:11:37,267 - INFO - joeynmt.training - Epoch  11: total training loss 4.07\n",
            "2021-11-23 23:11:37,268 - INFO - joeynmt.training - EPOCH 12\n",
            "2021-11-23 23:11:37,840 - INFO - joeynmt.training - Epoch  12: total training loss 4.00\n",
            "2021-11-23 23:11:37,840 - INFO - joeynmt.training - EPOCH 13\n",
            "2021-11-23 23:11:38,408 - INFO - joeynmt.training - Epoch  13: total training loss 3.91\n",
            "2021-11-23 23:11:38,408 - INFO - joeynmt.training - EPOCH 14\n",
            "2021-11-23 23:11:38,966 - INFO - joeynmt.training - Epoch  14: total training loss 3.84\n",
            "2021-11-23 23:11:38,967 - INFO - joeynmt.training - EPOCH 15\n",
            "2021-11-23 23:11:39,525 - INFO - joeynmt.training - Epoch  15: total training loss 3.73\n",
            "2021-11-23 23:11:39,525 - INFO - joeynmt.training - EPOCH 16\n",
            "2021-11-23 23:11:40,093 - INFO - joeynmt.training - Epoch  16: total training loss 3.61\n",
            "2021-11-23 23:11:40,094 - INFO - joeynmt.training - EPOCH 17\n",
            "2021-11-23 23:11:40,661 - INFO - joeynmt.training - Epoch  17: total training loss 3.48\n",
            "2021-11-23 23:11:40,662 - INFO - joeynmt.training - EPOCH 18\n",
            "2021-11-23 23:11:41,233 - INFO - joeynmt.training - Epoch  18: total training loss 3.29\n",
            "2021-11-23 23:11:41,234 - INFO - joeynmt.training - EPOCH 19\n",
            "2021-11-23 23:11:41,797 - INFO - joeynmt.training - Epoch  19: total training loss 3.18\n",
            "2021-11-23 23:11:41,797 - INFO - joeynmt.training - EPOCH 20\n",
            "2021-11-23 23:11:42,374 - INFO - joeynmt.training - Epoch  20: total training loss 3.14\n",
            "2021-11-23 23:11:42,374 - INFO - joeynmt.training - EPOCH 21\n",
            "2021-11-23 23:11:42,949 - INFO - joeynmt.training - Epoch  21: total training loss 3.12\n",
            "2021-11-23 23:11:42,949 - INFO - joeynmt.training - EPOCH 22\n",
            "2021-11-23 23:11:43,508 - INFO - joeynmt.training - Epoch  22: total training loss 3.11\n",
            "2021-11-23 23:11:43,509 - INFO - joeynmt.training - EPOCH 23\n",
            "2021-11-23 23:11:44,071 - INFO - joeynmt.training - Epoch  23: total training loss 3.07\n",
            "2021-11-23 23:11:44,071 - INFO - joeynmt.training - EPOCH 24\n",
            "2021-11-23 23:11:44,640 - INFO - joeynmt.training - Epoch  24: total training loss 3.08\n",
            "2021-11-23 23:11:44,640 - INFO - joeynmt.training - EPOCH 25\n",
            "2021-11-23 23:11:45,198 - INFO - joeynmt.training - Epoch  25: total training loss 3.01\n",
            "2021-11-23 23:11:45,199 - INFO - joeynmt.training - EPOCH 26\n",
            "2021-11-23 23:11:45,772 - INFO - joeynmt.training - Epoch  26: total training loss 3.01\n",
            "2021-11-23 23:11:45,773 - INFO - joeynmt.training - EPOCH 27\n",
            "2021-11-23 23:11:46,344 - INFO - joeynmt.training - Epoch  27: total training loss 3.02\n",
            "2021-11-23 23:11:46,345 - INFO - joeynmt.training - EPOCH 28\n",
            "2021-11-23 23:11:46,916 - INFO - joeynmt.training - Epoch  28: total training loss 3.00\n",
            "2021-11-23 23:11:46,916 - INFO - joeynmt.training - EPOCH 29\n",
            "2021-11-23 23:11:47,488 - INFO - joeynmt.training - Epoch  29: total training loss 3.03\n",
            "2021-11-23 23:11:47,489 - INFO - joeynmt.training - EPOCH 30\n",
            "2021-11-23 23:11:48,046 - INFO - joeynmt.training - Epoch  30: total training loss 2.98\n",
            "2021-11-23 23:11:48,046 - INFO - joeynmt.training - EPOCH 31\n",
            "2021-11-23 23:11:48,602 - INFO - joeynmt.training - Epoch  31: total training loss 2.99\n",
            "2021-11-23 23:11:48,603 - INFO - joeynmt.training - EPOCH 32\n",
            "2021-11-23 23:11:49,178 - INFO - joeynmt.training - Epoch  32: total training loss 2.93\n",
            "2021-11-23 23:11:49,178 - INFO - joeynmt.training - EPOCH 33\n",
            "2021-11-23 23:11:49,742 - INFO - joeynmt.training - Epoch  33: total training loss 2.95\n",
            "2021-11-23 23:11:49,743 - INFO - joeynmt.training - EPOCH 34\n",
            "2021-11-23 23:11:50,320 - INFO - joeynmt.training - Epoch  34: total training loss 2.94\n",
            "2021-11-23 23:11:50,321 - INFO - joeynmt.training - EPOCH 35\n",
            "2021-11-23 23:11:50,887 - INFO - joeynmt.training - Epoch  35: total training loss 2.97\n",
            "2021-11-23 23:11:50,887 - INFO - joeynmt.training - EPOCH 36\n",
            "2021-11-23 23:11:51,452 - INFO - joeynmt.training - Epoch  36: total training loss 2.91\n",
            "2021-11-23 23:11:51,452 - INFO - joeynmt.training - EPOCH 37\n",
            "2021-11-23 23:11:52,026 - INFO - joeynmt.training - Epoch  37: total training loss 2.94\n",
            "2021-11-23 23:11:52,026 - INFO - joeynmt.training - EPOCH 38\n",
            "2021-11-23 23:11:52,605 - INFO - joeynmt.training - Epoch  38: total training loss 2.92\n",
            "2021-11-23 23:11:52,605 - INFO - joeynmt.training - EPOCH 39\n",
            "2021-11-23 23:11:53,173 - INFO - joeynmt.training - Epoch  39: total training loss 2.92\n",
            "2021-11-23 23:11:53,174 - INFO - joeynmt.training - EPOCH 40\n",
            "2021-11-23 23:11:53,756 - INFO - joeynmt.training - Epoch  40: total training loss 2.89\n",
            "2021-11-23 23:11:53,757 - INFO - joeynmt.training - EPOCH 41\n",
            "2021-11-23 23:11:54,322 - INFO - joeynmt.training - Epoch  41: total training loss 2.89\n",
            "2021-11-23 23:11:54,322 - INFO - joeynmt.training - EPOCH 42\n",
            "2021-11-23 23:11:54,895 - INFO - joeynmt.training - Epoch  42: total training loss 2.90\n",
            "2021-11-23 23:11:54,896 - INFO - joeynmt.training - EPOCH 43\n",
            "2021-11-23 23:11:55,461 - INFO - joeynmt.training - Epoch  43: total training loss 2.89\n",
            "2021-11-23 23:11:55,462 - INFO - joeynmt.training - EPOCH 44\n",
            "2021-11-23 23:11:56,032 - INFO - joeynmt.training - Epoch  44: total training loss 2.86\n",
            "2021-11-23 23:11:56,032 - INFO - joeynmt.training - EPOCH 45\n",
            "2021-11-23 23:11:56,600 - INFO - joeynmt.training - Epoch  45: total training loss 2.86\n",
            "2021-11-23 23:11:56,600 - INFO - joeynmt.training - EPOCH 46\n",
            "2021-11-23 23:11:57,165 - INFO - joeynmt.training - Epoch  46: total training loss 2.87\n",
            "2021-11-23 23:11:57,166 - INFO - joeynmt.training - EPOCH 47\n",
            "2021-11-23 23:11:57,743 - INFO - joeynmt.training - Epoch  47: total training loss 2.85\n",
            "2021-11-23 23:11:57,744 - INFO - joeynmt.training - EPOCH 48\n",
            "2021-11-23 23:11:58,311 - INFO - joeynmt.training - Epoch  48: total training loss 2.88\n",
            "2021-11-23 23:11:58,312 - INFO - joeynmt.training - EPOCH 49\n",
            "2021-11-23 23:11:58,875 - INFO - joeynmt.training - Epoch  49: total training loss 2.85\n",
            "2021-11-23 23:11:58,875 - INFO - joeynmt.training - EPOCH 50\n",
            "2021-11-23 23:11:59,444 - INFO - joeynmt.training - Epoch  50, Step:      100, Batch Loss:     1.411526, Tokens per Sec:     6640, Lr: 0.000300\n",
            "2021-11-23 23:11:59,445 - INFO - joeynmt.training - Epoch  50: total training loss 2.85\n",
            "2021-11-23 23:11:59,445 - INFO - joeynmt.training - EPOCH 51\n",
            "2021-11-23 23:12:00,011 - INFO - joeynmt.training - Epoch  51: total training loss 2.85\n",
            "2021-11-23 23:12:00,011 - INFO - joeynmt.training - EPOCH 52\n",
            "2021-11-23 23:12:00,572 - INFO - joeynmt.training - Epoch  52: total training loss 2.83\n",
            "2021-11-23 23:12:00,572 - INFO - joeynmt.training - EPOCH 53\n",
            "2021-11-23 23:12:01,140 - INFO - joeynmt.training - Epoch  53: total training loss 2.82\n",
            "2021-11-23 23:12:01,140 - INFO - joeynmt.training - EPOCH 54\n",
            "2021-11-23 23:12:01,719 - INFO - joeynmt.training - Epoch  54: total training loss 2.83\n",
            "2021-11-23 23:12:01,720 - INFO - joeynmt.training - EPOCH 55\n",
            "2021-11-23 23:12:02,291 - INFO - joeynmt.training - Epoch  55: total training loss 2.82\n",
            "2021-11-23 23:12:02,291 - INFO - joeynmt.training - EPOCH 56\n",
            "2021-11-23 23:12:02,865 - INFO - joeynmt.training - Epoch  56: total training loss 2.84\n",
            "2021-11-23 23:12:02,866 - INFO - joeynmt.training - EPOCH 57\n",
            "2021-11-23 23:12:03,426 - INFO - joeynmt.training - Epoch  57: total training loss 2.85\n",
            "2021-11-23 23:12:03,426 - INFO - joeynmt.training - EPOCH 58\n",
            "2021-11-23 23:12:03,995 - INFO - joeynmt.training - Epoch  58: total training loss 2.83\n",
            "2021-11-23 23:12:03,995 - INFO - joeynmt.training - EPOCH 59\n",
            "2021-11-23 23:12:04,572 - INFO - joeynmt.training - Epoch  59: total training loss 2.82\n",
            "2021-11-23 23:12:04,573 - INFO - joeynmt.training - EPOCH 60\n",
            "2021-11-23 23:12:05,132 - INFO - joeynmt.training - Epoch  60: total training loss 2.82\n",
            "2021-11-23 23:12:05,133 - INFO - joeynmt.training - EPOCH 61\n",
            "2021-11-23 23:12:05,691 - INFO - joeynmt.training - Epoch  61: total training loss 2.82\n",
            "2021-11-23 23:12:05,692 - INFO - joeynmt.training - EPOCH 62\n",
            "2021-11-23 23:12:06,267 - INFO - joeynmt.training - Epoch  62: total training loss 2.82\n",
            "2021-11-23 23:12:06,268 - INFO - joeynmt.training - EPOCH 63\n",
            "2021-11-23 23:12:06,842 - INFO - joeynmt.training - Epoch  63: total training loss 2.78\n",
            "2021-11-23 23:12:06,842 - INFO - joeynmt.training - EPOCH 64\n",
            "2021-11-23 23:12:07,414 - INFO - joeynmt.training - Epoch  64: total training loss 2.80\n",
            "2021-11-23 23:12:07,415 - INFO - joeynmt.training - EPOCH 65\n",
            "2021-11-23 23:12:07,986 - INFO - joeynmt.training - Epoch  65: total training loss 2.80\n",
            "2021-11-23 23:12:07,986 - INFO - joeynmt.training - EPOCH 66\n",
            "2021-11-23 23:12:08,554 - INFO - joeynmt.training - Epoch  66: total training loss 2.80\n",
            "2021-11-23 23:12:08,554 - INFO - joeynmt.training - EPOCH 67\n",
            "2021-11-23 23:12:09,113 - INFO - joeynmt.training - Epoch  67: total training loss 2.78\n",
            "2021-11-23 23:12:09,113 - INFO - joeynmt.training - EPOCH 68\n",
            "2021-11-23 23:12:09,678 - INFO - joeynmt.training - Epoch  68: total training loss 2.79\n",
            "2021-11-23 23:12:09,679 - INFO - joeynmt.training - EPOCH 69\n",
            "2021-11-23 23:12:10,246 - INFO - joeynmt.training - Epoch  69: total training loss 2.76\n",
            "2021-11-23 23:12:10,246 - INFO - joeynmt.training - EPOCH 70\n",
            "2021-11-23 23:12:10,809 - INFO - joeynmt.training - Epoch  70: total training loss 2.77\n",
            "2021-11-23 23:12:10,810 - INFO - joeynmt.training - EPOCH 71\n",
            "2021-11-23 23:12:11,384 - INFO - joeynmt.training - Epoch  71: total training loss 2.77\n",
            "2021-11-23 23:12:11,384 - INFO - joeynmt.training - EPOCH 72\n",
            "2021-11-23 23:12:11,958 - INFO - joeynmt.training - Epoch  72: total training loss 2.76\n",
            "2021-11-23 23:12:11,959 - INFO - joeynmt.training - EPOCH 73\n",
            "2021-11-23 23:12:12,532 - INFO - joeynmt.training - Epoch  73: total training loss 2.75\n",
            "2021-11-23 23:12:12,533 - INFO - joeynmt.training - EPOCH 74\n",
            "2021-11-23 23:12:13,107 - INFO - joeynmt.training - Epoch  74: total training loss 2.75\n",
            "2021-11-23 23:12:13,108 - INFO - joeynmt.training - EPOCH 75\n",
            "2021-11-23 23:12:13,672 - INFO - joeynmt.training - Epoch  75: total training loss 2.79\n",
            "2021-11-23 23:12:13,673 - INFO - joeynmt.training - EPOCH 76\n",
            "2021-11-23 23:12:14,234 - INFO - joeynmt.training - Epoch  76: total training loss 2.75\n",
            "2021-11-23 23:12:14,234 - INFO - joeynmt.training - EPOCH 77\n",
            "2021-11-23 23:12:14,795 - INFO - joeynmt.training - Epoch  77: total training loss 2.74\n",
            "2021-11-23 23:12:14,795 - INFO - joeynmt.training - EPOCH 78\n",
            "2021-11-23 23:12:15,351 - INFO - joeynmt.training - Epoch  78: total training loss 2.75\n",
            "2021-11-23 23:12:15,351 - INFO - joeynmt.training - EPOCH 79\n",
            "2021-11-23 23:12:15,920 - INFO - joeynmt.training - Epoch  79: total training loss 2.75\n",
            "2021-11-23 23:12:15,920 - INFO - joeynmt.training - EPOCH 80\n",
            "2021-11-23 23:12:16,496 - INFO - joeynmt.training - Epoch  80: total training loss 2.74\n",
            "2021-11-23 23:12:16,497 - INFO - joeynmt.training - EPOCH 81\n",
            "2021-11-23 23:12:17,059 - INFO - joeynmt.training - Epoch  81: total training loss 2.72\n",
            "2021-11-23 23:12:17,060 - INFO - joeynmt.training - EPOCH 82\n",
            "2021-11-23 23:12:17,630 - INFO - joeynmt.training - Epoch  82: total training loss 2.73\n",
            "2021-11-23 23:12:17,630 - INFO - joeynmt.training - EPOCH 83\n",
            "2021-11-23 23:12:18,196 - INFO - joeynmt.training - Epoch  83: total training loss 2.72\n",
            "2021-11-23 23:12:18,197 - INFO - joeynmt.training - EPOCH 84\n",
            "2021-11-23 23:12:18,779 - INFO - joeynmt.training - Epoch  84: total training loss 2.73\n",
            "2021-11-23 23:12:18,779 - INFO - joeynmt.training - EPOCH 85\n",
            "2021-11-23 23:12:19,347 - INFO - joeynmt.training - Epoch  85: total training loss 2.72\n",
            "2021-11-23 23:12:19,347 - INFO - joeynmt.training - EPOCH 86\n",
            "2021-11-23 23:12:19,913 - INFO - joeynmt.training - Epoch  86: total training loss 2.74\n",
            "2021-11-23 23:12:19,913 - INFO - joeynmt.training - EPOCH 87\n",
            "2021-11-23 23:12:20,466 - INFO - joeynmt.training - Epoch  87: total training loss 2.74\n",
            "2021-11-23 23:12:20,467 - INFO - joeynmt.training - EPOCH 88\n",
            "2021-11-23 23:12:21,028 - INFO - joeynmt.training - Epoch  88: total training loss 2.69\n",
            "2021-11-23 23:12:21,028 - INFO - joeynmt.training - EPOCH 89\n",
            "2021-11-23 23:12:21,597 - INFO - joeynmt.training - Epoch  89: total training loss 2.71\n",
            "2021-11-23 23:12:21,598 - INFO - joeynmt.training - EPOCH 90\n",
            "2021-11-23 23:12:22,162 - INFO - joeynmt.training - Epoch  90: total training loss 2.71\n",
            "2021-11-23 23:12:22,162 - INFO - joeynmt.training - EPOCH 91\n",
            "2021-11-23 23:12:22,723 - INFO - joeynmt.training - Epoch  91: total training loss 2.68\n",
            "2021-11-23 23:12:22,724 - INFO - joeynmt.training - EPOCH 92\n",
            "2021-11-23 23:12:23,281 - INFO - joeynmt.training - Epoch  92: total training loss 2.67\n",
            "2021-11-23 23:12:23,281 - INFO - joeynmt.training - EPOCH 93\n",
            "2021-11-23 23:12:23,856 - INFO - joeynmt.training - Epoch  93: total training loss 2.67\n",
            "2021-11-23 23:12:23,856 - INFO - joeynmt.training - EPOCH 94\n",
            "2021-11-23 23:12:24,426 - INFO - joeynmt.training - Epoch  94: total training loss 2.66\n",
            "2021-11-23 23:12:24,427 - INFO - joeynmt.training - EPOCH 95\n",
            "2021-11-23 23:12:24,986 - INFO - joeynmt.training - Epoch  95: total training loss 2.69\n",
            "2021-11-23 23:12:24,986 - INFO - joeynmt.training - EPOCH 96\n",
            "2021-11-23 23:12:25,545 - INFO - joeynmt.training - Epoch  96: total training loss 2.68\n",
            "2021-11-23 23:12:25,546 - INFO - joeynmt.training - EPOCH 97\n",
            "2021-11-23 23:12:26,117 - INFO - joeynmt.training - Epoch  97: total training loss 2.67\n",
            "2021-11-23 23:12:26,118 - INFO - joeynmt.training - EPOCH 98\n",
            "2021-11-23 23:12:26,686 - INFO - joeynmt.training - Epoch  98: total training loss 2.64\n",
            "2021-11-23 23:12:26,686 - INFO - joeynmt.training - EPOCH 99\n",
            "2021-11-23 23:12:27,268 - INFO - joeynmt.training - Epoch  99: total training loss 2.67\n",
            "2021-11-23 23:12:27,269 - INFO - joeynmt.training - EPOCH 100\n",
            "2021-11-23 23:12:27,832 - INFO - joeynmt.training - Epoch 100, Step:      200, Batch Loss:     1.372848, Tokens per Sec:     6707, Lr: 0.000300\n",
            "2021-11-23 23:12:27,832 - INFO - joeynmt.training - Epoch 100: total training loss 2.68\n",
            "2021-11-23 23:12:27,832 - INFO - joeynmt.training - EPOCH 101\n",
            "2021-11-23 23:12:28,400 - INFO - joeynmt.training - Epoch 101: total training loss 2.63\n",
            "2021-11-23 23:12:28,401 - INFO - joeynmt.training - EPOCH 102\n",
            "2021-11-23 23:12:28,966 - INFO - joeynmt.training - Epoch 102: total training loss 2.64\n",
            "2021-11-23 23:12:28,966 - INFO - joeynmt.training - EPOCH 103\n",
            "2021-11-23 23:12:29,529 - INFO - joeynmt.training - Epoch 103: total training loss 2.66\n",
            "2021-11-23 23:12:29,529 - INFO - joeynmt.training - EPOCH 104\n",
            "2021-11-23 23:12:30,095 - INFO - joeynmt.training - Epoch 104: total training loss 2.60\n",
            "2021-11-23 23:12:30,096 - INFO - joeynmt.training - EPOCH 105\n",
            "2021-11-23 23:12:30,664 - INFO - joeynmt.training - Epoch 105: total training loss 2.61\n",
            "2021-11-23 23:12:30,664 - INFO - joeynmt.training - EPOCH 106\n",
            "2021-11-23 23:12:31,232 - INFO - joeynmt.training - Epoch 106: total training loss 2.61\n",
            "2021-11-23 23:12:31,232 - INFO - joeynmt.training - EPOCH 107\n",
            "2021-11-23 23:12:31,801 - INFO - joeynmt.training - Epoch 107: total training loss 2.60\n",
            "2021-11-23 23:12:31,802 - INFO - joeynmt.training - EPOCH 108\n",
            "2021-11-23 23:12:32,376 - INFO - joeynmt.training - Epoch 108: total training loss 2.61\n",
            "2021-11-23 23:12:32,377 - INFO - joeynmt.training - EPOCH 109\n",
            "2021-11-23 23:12:32,946 - INFO - joeynmt.training - Epoch 109: total training loss 2.61\n",
            "2021-11-23 23:12:32,946 - INFO - joeynmt.training - EPOCH 110\n",
            "2021-11-23 23:12:33,509 - INFO - joeynmt.training - Epoch 110: total training loss 2.59\n",
            "2021-11-23 23:12:33,509 - INFO - joeynmt.training - EPOCH 111\n",
            "2021-11-23 23:12:34,082 - INFO - joeynmt.training - Epoch 111: total training loss 2.62\n",
            "2021-11-23 23:12:34,083 - INFO - joeynmt.training - EPOCH 112\n",
            "2021-11-23 23:12:34,646 - INFO - joeynmt.training - Epoch 112: total training loss 2.59\n",
            "2021-11-23 23:12:34,646 - INFO - joeynmt.training - EPOCH 113\n",
            "2021-11-23 23:12:35,222 - INFO - joeynmt.training - Epoch 113: total training loss 2.56\n",
            "2021-11-23 23:12:35,222 - INFO - joeynmt.training - EPOCH 114\n",
            "2021-11-23 23:12:35,785 - INFO - joeynmt.training - Epoch 114: total training loss 2.59\n",
            "2021-11-23 23:12:35,785 - INFO - joeynmt.training - EPOCH 115\n",
            "2021-11-23 23:12:36,356 - INFO - joeynmt.training - Epoch 115: total training loss 2.56\n",
            "2021-11-23 23:12:36,357 - INFO - joeynmt.training - EPOCH 116\n",
            "2021-11-23 23:12:36,915 - INFO - joeynmt.training - Epoch 116: total training loss 2.55\n",
            "2021-11-23 23:12:36,916 - INFO - joeynmt.training - EPOCH 117\n",
            "2021-11-23 23:12:37,486 - INFO - joeynmt.training - Epoch 117: total training loss 2.54\n",
            "2021-11-23 23:12:37,487 - INFO - joeynmt.training - EPOCH 118\n",
            "2021-11-23 23:12:38,057 - INFO - joeynmt.training - Epoch 118: total training loss 2.54\n",
            "2021-11-23 23:12:38,057 - INFO - joeynmt.training - EPOCH 119\n",
            "2021-11-23 23:12:38,629 - INFO - joeynmt.training - Epoch 119: total training loss 2.54\n",
            "2021-11-23 23:12:38,629 - INFO - joeynmt.training - EPOCH 120\n",
            "2021-11-23 23:12:39,187 - INFO - joeynmt.training - Epoch 120: total training loss 2.53\n",
            "2021-11-23 23:12:39,187 - INFO - joeynmt.training - EPOCH 121\n",
            "2021-11-23 23:12:39,750 - INFO - joeynmt.training - Epoch 121: total training loss 2.54\n",
            "2021-11-23 23:12:39,750 - INFO - joeynmt.training - EPOCH 122\n",
            "2021-11-23 23:12:40,315 - INFO - joeynmt.training - Epoch 122: total training loss 2.53\n",
            "2021-11-23 23:12:40,315 - INFO - joeynmt.training - EPOCH 123\n",
            "2021-11-23 23:12:40,882 - INFO - joeynmt.training - Epoch 123: total training loss 2.51\n",
            "2021-11-23 23:12:40,883 - INFO - joeynmt.training - EPOCH 124\n",
            "2021-11-23 23:12:41,447 - INFO - joeynmt.training - Epoch 124: total training loss 2.53\n",
            "2021-11-23 23:12:41,448 - INFO - joeynmt.training - EPOCH 125\n",
            "2021-11-23 23:12:42,019 - INFO - joeynmt.training - Epoch 125: total training loss 2.51\n",
            "2021-11-23 23:12:42,020 - INFO - joeynmt.training - EPOCH 126\n",
            "2021-11-23 23:12:42,578 - INFO - joeynmt.training - Epoch 126: total training loss 2.49\n",
            "2021-11-23 23:12:42,578 - INFO - joeynmt.training - EPOCH 127\n",
            "2021-11-23 23:12:43,147 - INFO - joeynmt.training - Epoch 127: total training loss 2.48\n",
            "2021-11-23 23:12:43,148 - INFO - joeynmt.training - EPOCH 128\n",
            "2021-11-23 23:12:43,713 - INFO - joeynmt.training - Epoch 128: total training loss 2.47\n",
            "2021-11-23 23:12:43,713 - INFO - joeynmt.training - EPOCH 129\n",
            "2021-11-23 23:12:44,281 - INFO - joeynmt.training - Epoch 129: total training loss 2.45\n",
            "2021-11-23 23:12:44,281 - INFO - joeynmt.training - EPOCH 130\n",
            "2021-11-23 23:12:44,848 - INFO - joeynmt.training - Epoch 130: total training loss 2.50\n",
            "2021-11-23 23:12:44,849 - INFO - joeynmt.training - EPOCH 131\n",
            "2021-11-23 23:12:45,418 - INFO - joeynmt.training - Epoch 131: total training loss 2.44\n",
            "2021-11-23 23:12:45,418 - INFO - joeynmt.training - EPOCH 132\n",
            "2021-11-23 23:12:45,988 - INFO - joeynmt.training - Epoch 132: total training loss 2.46\n",
            "2021-11-23 23:12:45,989 - INFO - joeynmt.training - EPOCH 133\n",
            "2021-11-23 23:12:46,549 - INFO - joeynmt.training - Epoch 133: total training loss 2.38\n",
            "2021-11-23 23:12:46,550 - INFO - joeynmt.training - EPOCH 134\n",
            "2021-11-23 23:12:47,120 - INFO - joeynmt.training - Epoch 134: total training loss 2.44\n",
            "2021-11-23 23:12:47,120 - INFO - joeynmt.training - EPOCH 135\n",
            "2021-11-23 23:12:47,689 - INFO - joeynmt.training - Epoch 135: total training loss 2.42\n",
            "2021-11-23 23:12:47,689 - INFO - joeynmt.training - EPOCH 136\n",
            "2021-11-23 23:12:48,256 - INFO - joeynmt.training - Epoch 136: total training loss 2.42\n",
            "2021-11-23 23:12:48,256 - INFO - joeynmt.training - EPOCH 137\n",
            "2021-11-23 23:12:48,831 - INFO - joeynmt.training - Epoch 137: total training loss 2.41\n",
            "2021-11-23 23:12:48,832 - INFO - joeynmt.training - EPOCH 138\n",
            "2021-11-23 23:12:49,389 - INFO - joeynmt.training - Epoch 138: total training loss 2.40\n",
            "2021-11-23 23:12:49,390 - INFO - joeynmt.training - EPOCH 139\n",
            "2021-11-23 23:12:49,951 - INFO - joeynmt.training - Epoch 139: total training loss 2.36\n",
            "2021-11-23 23:12:49,952 - INFO - joeynmt.training - EPOCH 140\n",
            "2021-11-23 23:12:50,521 - INFO - joeynmt.training - Epoch 140: total training loss 2.39\n",
            "2021-11-23 23:12:50,521 - INFO - joeynmt.training - EPOCH 141\n",
            "2021-11-23 23:12:51,066 - INFO - joeynmt.training - Epoch 141: total training loss 2.38\n",
            "2021-11-23 23:12:51,066 - INFO - joeynmt.training - EPOCH 142\n",
            "2021-11-23 23:12:51,641 - INFO - joeynmt.training - Epoch 142: total training loss 2.41\n",
            "2021-11-23 23:12:51,641 - INFO - joeynmt.training - EPOCH 143\n",
            "2021-11-23 23:12:52,209 - INFO - joeynmt.training - Epoch 143: total training loss 2.37\n",
            "2021-11-23 23:12:52,210 - INFO - joeynmt.training - EPOCH 144\n",
            "2021-11-23 23:12:52,768 - INFO - joeynmt.training - Epoch 144: total training loss 2.36\n",
            "2021-11-23 23:12:52,768 - INFO - joeynmt.training - EPOCH 145\n",
            "2021-11-23 23:12:53,333 - INFO - joeynmt.training - Epoch 145: total training loss 2.35\n",
            "2021-11-23 23:12:53,334 - INFO - joeynmt.training - EPOCH 146\n",
            "2021-11-23 23:12:53,900 - INFO - joeynmt.training - Epoch 146: total training loss 2.32\n",
            "2021-11-23 23:12:53,900 - INFO - joeynmt.training - EPOCH 147\n",
            "2021-11-23 23:12:54,463 - INFO - joeynmt.training - Epoch 147: total training loss 2.31\n",
            "2021-11-23 23:12:54,464 - INFO - joeynmt.training - EPOCH 148\n",
            "2021-11-23 23:12:55,030 - INFO - joeynmt.training - Epoch 148: total training loss 2.29\n",
            "2021-11-23 23:12:55,030 - INFO - joeynmt.training - EPOCH 149\n",
            "2021-11-23 23:12:55,590 - INFO - joeynmt.training - Epoch 149: total training loss 2.33\n",
            "2021-11-23 23:12:55,590 - INFO - joeynmt.training - EPOCH 150\n",
            "2021-11-23 23:12:56,156 - INFO - joeynmt.training - Epoch 150, Step:      300, Batch Loss:     1.162914, Tokens per Sec:     6669, Lr: 0.000300\n",
            "2021-11-23 23:12:56,157 - INFO - joeynmt.training - Epoch 150: total training loss 2.30\n",
            "2021-11-23 23:12:56,157 - INFO - joeynmt.training - EPOCH 151\n",
            "2021-11-23 23:12:56,707 - INFO - joeynmt.training - Epoch 151: total training loss 2.28\n",
            "2021-11-23 23:12:56,708 - INFO - joeynmt.training - EPOCH 152\n",
            "2021-11-23 23:12:57,277 - INFO - joeynmt.training - Epoch 152: total training loss 2.27\n",
            "2021-11-23 23:12:57,278 - INFO - joeynmt.training - EPOCH 153\n",
            "2021-11-23 23:12:57,849 - INFO - joeynmt.training - Epoch 153: total training loss 2.24\n",
            "2021-11-23 23:12:57,850 - INFO - joeynmt.training - EPOCH 154\n",
            "2021-11-23 23:12:58,412 - INFO - joeynmt.training - Epoch 154: total training loss 2.23\n",
            "2021-11-23 23:12:58,412 - INFO - joeynmt.training - EPOCH 155\n",
            "2021-11-23 23:12:58,979 - INFO - joeynmt.training - Epoch 155: total training loss 2.24\n",
            "2021-11-23 23:12:58,980 - INFO - joeynmt.training - EPOCH 156\n",
            "2021-11-23 23:12:59,538 - INFO - joeynmt.training - Epoch 156: total training loss 2.22\n",
            "2021-11-23 23:12:59,538 - INFO - joeynmt.training - EPOCH 157\n",
            "2021-11-23 23:13:00,106 - INFO - joeynmt.training - Epoch 157: total training loss 2.19\n",
            "2021-11-23 23:13:00,107 - INFO - joeynmt.training - EPOCH 158\n",
            "2021-11-23 23:13:00,681 - INFO - joeynmt.training - Epoch 158: total training loss 2.22\n",
            "2021-11-23 23:13:00,682 - INFO - joeynmt.training - EPOCH 159\n",
            "2021-11-23 23:13:01,261 - INFO - joeynmt.training - Epoch 159: total training loss 2.22\n",
            "2021-11-23 23:13:01,262 - INFO - joeynmt.training - EPOCH 160\n",
            "2021-11-23 23:13:01,830 - INFO - joeynmt.training - Epoch 160: total training loss 2.22\n",
            "2021-11-23 23:13:01,830 - INFO - joeynmt.training - EPOCH 161\n",
            "2021-11-23 23:13:02,405 - INFO - joeynmt.training - Epoch 161: total training loss 2.19\n",
            "2021-11-23 23:13:02,406 - INFO - joeynmt.training - EPOCH 162\n",
            "2021-11-23 23:13:02,968 - INFO - joeynmt.training - Epoch 162: total training loss 2.18\n",
            "2021-11-23 23:13:02,968 - INFO - joeynmt.training - EPOCH 163\n",
            "2021-11-23 23:13:03,530 - INFO - joeynmt.training - Epoch 163: total training loss 2.17\n",
            "2021-11-23 23:13:03,530 - INFO - joeynmt.training - EPOCH 164\n",
            "2021-11-23 23:13:04,102 - INFO - joeynmt.training - Epoch 164: total training loss 2.15\n",
            "2021-11-23 23:13:04,103 - INFO - joeynmt.training - EPOCH 165\n",
            "2021-11-23 23:13:04,667 - INFO - joeynmt.training - Epoch 165: total training loss 2.15\n",
            "2021-11-23 23:13:04,667 - INFO - joeynmt.training - EPOCH 166\n",
            "2021-11-23 23:13:05,218 - INFO - joeynmt.training - Epoch 166: total training loss 2.15\n",
            "2021-11-23 23:13:05,219 - INFO - joeynmt.training - EPOCH 167\n",
            "2021-11-23 23:13:05,792 - INFO - joeynmt.training - Epoch 167: total training loss 2.10\n",
            "2021-11-23 23:13:05,793 - INFO - joeynmt.training - EPOCH 168\n",
            "2021-11-23 23:13:06,355 - INFO - joeynmt.training - Epoch 168: total training loss 2.14\n",
            "2021-11-23 23:13:06,355 - INFO - joeynmt.training - EPOCH 169\n",
            "2021-11-23 23:13:06,928 - INFO - joeynmt.training - Epoch 169: total training loss 2.14\n",
            "2021-11-23 23:13:06,928 - INFO - joeynmt.training - EPOCH 170\n",
            "2021-11-23 23:13:07,497 - INFO - joeynmt.training - Epoch 170: total training loss 2.13\n",
            "2021-11-23 23:13:07,498 - INFO - joeynmt.training - EPOCH 171\n",
            "2021-11-23 23:13:08,068 - INFO - joeynmt.training - Epoch 171: total training loss 2.09\n",
            "2021-11-23 23:13:08,068 - INFO - joeynmt.training - EPOCH 172\n",
            "2021-11-23 23:13:08,640 - INFO - joeynmt.training - Epoch 172: total training loss 2.10\n",
            "2021-11-23 23:13:08,640 - INFO - joeynmt.training - EPOCH 173\n",
            "2021-11-23 23:13:09,204 - INFO - joeynmt.training - Epoch 173: total training loss 2.09\n",
            "2021-11-23 23:13:09,205 - INFO - joeynmt.training - EPOCH 174\n",
            "2021-11-23 23:13:09,776 - INFO - joeynmt.training - Epoch 174: total training loss 2.09\n",
            "2021-11-23 23:13:09,776 - INFO - joeynmt.training - EPOCH 175\n",
            "2021-11-23 23:13:10,340 - INFO - joeynmt.training - Epoch 175: total training loss 2.07\n",
            "2021-11-23 23:13:10,341 - INFO - joeynmt.training - EPOCH 176\n",
            "2021-11-23 23:13:10,895 - INFO - joeynmt.training - Epoch 176: total training loss 2.05\n",
            "2021-11-23 23:13:10,895 - INFO - joeynmt.training - EPOCH 177\n",
            "2021-11-23 23:13:11,454 - INFO - joeynmt.training - Epoch 177: total training loss 2.05\n",
            "2021-11-23 23:13:11,455 - INFO - joeynmt.training - EPOCH 178\n",
            "2021-11-23 23:13:12,011 - INFO - joeynmt.training - Epoch 178: total training loss 2.03\n",
            "2021-11-23 23:13:12,012 - INFO - joeynmt.training - EPOCH 179\n",
            "2021-11-23 23:13:12,574 - INFO - joeynmt.training - Epoch 179: total training loss 2.05\n",
            "2021-11-23 23:13:12,575 - INFO - joeynmt.training - EPOCH 180\n",
            "2021-11-23 23:13:13,154 - INFO - joeynmt.training - Epoch 180: total training loss 2.02\n",
            "2021-11-23 23:13:13,155 - INFO - joeynmt.training - EPOCH 181\n",
            "2021-11-23 23:13:13,717 - INFO - joeynmt.training - Epoch 181: total training loss 2.01\n",
            "2021-11-23 23:13:13,717 - INFO - joeynmt.training - EPOCH 182\n",
            "2021-11-23 23:13:14,283 - INFO - joeynmt.training - Epoch 182: total training loss 1.97\n",
            "2021-11-23 23:13:14,284 - INFO - joeynmt.training - EPOCH 183\n",
            "2021-11-23 23:13:14,860 - INFO - joeynmt.training - Epoch 183: total training loss 2.02\n",
            "2021-11-23 23:13:14,861 - INFO - joeynmt.training - EPOCH 184\n",
            "2021-11-23 23:13:15,432 - INFO - joeynmt.training - Epoch 184: total training loss 1.99\n",
            "2021-11-23 23:13:15,432 - INFO - joeynmt.training - EPOCH 185\n",
            "2021-11-23 23:13:16,012 - INFO - joeynmt.training - Epoch 185: total training loss 1.98\n",
            "2021-11-23 23:13:16,012 - INFO - joeynmt.training - EPOCH 186\n",
            "2021-11-23 23:13:16,576 - INFO - joeynmt.training - Epoch 186: total training loss 1.97\n",
            "2021-11-23 23:13:16,576 - INFO - joeynmt.training - EPOCH 187\n",
            "2021-11-23 23:13:17,149 - INFO - joeynmt.training - Epoch 187: total training loss 1.97\n",
            "2021-11-23 23:13:17,149 - INFO - joeynmt.training - EPOCH 188\n",
            "2021-11-23 23:13:17,716 - INFO - joeynmt.training - Epoch 188: total training loss 1.98\n",
            "2021-11-23 23:13:17,716 - INFO - joeynmt.training - EPOCH 189\n",
            "2021-11-23 23:13:18,275 - INFO - joeynmt.training - Epoch 189: total training loss 1.94\n",
            "2021-11-23 23:13:18,275 - INFO - joeynmt.training - EPOCH 190\n",
            "2021-11-23 23:13:18,848 - INFO - joeynmt.training - Epoch 190: total training loss 1.94\n",
            "2021-11-23 23:13:18,848 - INFO - joeynmt.training - EPOCH 191\n",
            "2021-11-23 23:13:19,404 - INFO - joeynmt.training - Epoch 191: total training loss 1.95\n",
            "2021-11-23 23:13:19,404 - INFO - joeynmt.training - EPOCH 192\n",
            "2021-11-23 23:13:19,979 - INFO - joeynmt.training - Epoch 192: total training loss 1.94\n",
            "2021-11-23 23:13:19,979 - INFO - joeynmt.training - EPOCH 193\n",
            "2021-11-23 23:13:20,545 - INFO - joeynmt.training - Epoch 193: total training loss 1.93\n",
            "2021-11-23 23:13:20,546 - INFO - joeynmt.training - EPOCH 194\n",
            "2021-11-23 23:13:21,114 - INFO - joeynmt.training - Epoch 194: total training loss 1.94\n",
            "2021-11-23 23:13:21,114 - INFO - joeynmt.training - EPOCH 195\n",
            "2021-11-23 23:13:21,688 - INFO - joeynmt.training - Epoch 195: total training loss 1.93\n",
            "2021-11-23 23:13:21,688 - INFO - joeynmt.training - EPOCH 196\n",
            "2021-11-23 23:13:22,246 - INFO - joeynmt.training - Epoch 196: total training loss 1.95\n",
            "2021-11-23 23:13:22,247 - INFO - joeynmt.training - EPOCH 197\n",
            "2021-11-23 23:13:22,806 - INFO - joeynmt.training - Epoch 197: total training loss 1.91\n",
            "2021-11-23 23:13:22,806 - INFO - joeynmt.training - EPOCH 198\n",
            "2021-11-23 23:13:23,374 - INFO - joeynmt.training - Epoch 198: total training loss 1.92\n",
            "2021-11-23 23:13:23,375 - INFO - joeynmt.training - EPOCH 199\n",
            "2021-11-23 23:13:23,942 - INFO - joeynmt.training - Epoch 199: total training loss 1.90\n",
            "2021-11-23 23:13:23,943 - INFO - joeynmt.training - EPOCH 200\n",
            "2021-11-23 23:13:24,504 - INFO - joeynmt.training - Epoch 200, Step:      400, Batch Loss:     0.902409, Tokens per Sec:     6731, Lr: 0.000300\n",
            "2021-11-23 23:13:24,504 - INFO - joeynmt.training - Epoch 200: total training loss 1.87\n",
            "2021-11-23 23:13:24,504 - INFO - joeynmt.training - EPOCH 201\n",
            "2021-11-23 23:13:25,069 - INFO - joeynmt.training - Epoch 201: total training loss 1.87\n",
            "2021-11-23 23:13:25,069 - INFO - joeynmt.training - EPOCH 202\n",
            "2021-11-23 23:13:25,626 - INFO - joeynmt.training - Epoch 202: total training loss 1.85\n",
            "2021-11-23 23:13:25,626 - INFO - joeynmt.training - EPOCH 203\n",
            "2021-11-23 23:13:26,201 - INFO - joeynmt.training - Epoch 203: total training loss 1.83\n",
            "2021-11-23 23:13:26,202 - INFO - joeynmt.training - EPOCH 204\n",
            "2021-11-23 23:13:26,770 - INFO - joeynmt.training - Epoch 204: total training loss 1.85\n",
            "2021-11-23 23:13:26,770 - INFO - joeynmt.training - EPOCH 205\n",
            "2021-11-23 23:13:27,326 - INFO - joeynmt.training - Epoch 205: total training loss 1.87\n",
            "2021-11-23 23:13:27,326 - INFO - joeynmt.training - EPOCH 206\n",
            "2021-11-23 23:13:27,891 - INFO - joeynmt.training - Epoch 206: total training loss 1.85\n",
            "2021-11-23 23:13:27,892 - INFO - joeynmt.training - EPOCH 207\n",
            "2021-11-23 23:13:28,459 - INFO - joeynmt.training - Epoch 207: total training loss 1.83\n",
            "2021-11-23 23:13:28,460 - INFO - joeynmt.training - EPOCH 208\n",
            "2021-11-23 23:13:29,025 - INFO - joeynmt.training - Epoch 208: total training loss 1.81\n",
            "2021-11-23 23:13:29,025 - INFO - joeynmt.training - EPOCH 209\n",
            "2021-11-23 23:13:29,596 - INFO - joeynmt.training - Epoch 209: total training loss 1.76\n",
            "2021-11-23 23:13:29,597 - INFO - joeynmt.training - EPOCH 210\n",
            "2021-11-23 23:13:30,162 - INFO - joeynmt.training - Epoch 210: total training loss 1.79\n",
            "2021-11-23 23:13:30,162 - INFO - joeynmt.training - EPOCH 211\n",
            "2021-11-23 23:13:30,730 - INFO - joeynmt.training - Epoch 211: total training loss 1.78\n",
            "2021-11-23 23:13:30,731 - INFO - joeynmt.training - EPOCH 212\n",
            "2021-11-23 23:13:31,292 - INFO - joeynmt.training - Epoch 212: total training loss 1.79\n",
            "2021-11-23 23:13:31,292 - INFO - joeynmt.training - EPOCH 213\n",
            "2021-11-23 23:13:31,852 - INFO - joeynmt.training - Epoch 213: total training loss 1.76\n",
            "2021-11-23 23:13:31,853 - INFO - joeynmt.training - EPOCH 214\n",
            "2021-11-23 23:13:32,421 - INFO - joeynmt.training - Epoch 214: total training loss 1.79\n",
            "2021-11-23 23:13:32,422 - INFO - joeynmt.training - EPOCH 215\n",
            "2021-11-23 23:13:32,985 - INFO - joeynmt.training - Epoch 215: total training loss 1.77\n",
            "2021-11-23 23:13:32,985 - INFO - joeynmt.training - EPOCH 216\n",
            "2021-11-23 23:13:33,547 - INFO - joeynmt.training - Epoch 216: total training loss 1.76\n",
            "2021-11-23 23:13:33,547 - INFO - joeynmt.training - EPOCH 217\n",
            "2021-11-23 23:13:34,117 - INFO - joeynmt.training - Epoch 217: total training loss 1.77\n",
            "2021-11-23 23:13:34,118 - INFO - joeynmt.training - EPOCH 218\n",
            "2021-11-23 23:13:34,678 - INFO - joeynmt.training - Epoch 218: total training loss 1.74\n",
            "2021-11-23 23:13:34,678 - INFO - joeynmt.training - EPOCH 219\n",
            "2021-11-23 23:13:35,254 - INFO - joeynmt.training - Epoch 219: total training loss 1.71\n",
            "2021-11-23 23:13:35,254 - INFO - joeynmt.training - EPOCH 220\n",
            "2021-11-23 23:13:35,826 - INFO - joeynmt.training - Epoch 220: total training loss 1.73\n",
            "2021-11-23 23:13:35,826 - INFO - joeynmt.training - EPOCH 221\n",
            "2021-11-23 23:13:36,394 - INFO - joeynmt.training - Epoch 221: total training loss 1.71\n",
            "2021-11-23 23:13:36,394 - INFO - joeynmt.training - EPOCH 222\n",
            "2021-11-23 23:13:36,966 - INFO - joeynmt.training - Epoch 222: total training loss 1.72\n",
            "2021-11-23 23:13:36,966 - INFO - joeynmt.training - EPOCH 223\n",
            "2021-11-23 23:13:37,537 - INFO - joeynmt.training - Epoch 223: total training loss 1.70\n",
            "2021-11-23 23:13:37,538 - INFO - joeynmt.training - EPOCH 224\n",
            "2021-11-23 23:13:38,108 - INFO - joeynmt.training - Epoch 224: total training loss 1.71\n",
            "2021-11-23 23:13:38,109 - INFO - joeynmt.training - EPOCH 225\n",
            "2021-11-23 23:13:38,681 - INFO - joeynmt.training - Epoch 225: total training loss 1.69\n",
            "2021-11-23 23:13:38,682 - INFO - joeynmt.training - EPOCH 226\n",
            "2021-11-23 23:13:39,250 - INFO - joeynmt.training - Epoch 226: total training loss 1.69\n",
            "2021-11-23 23:13:39,250 - INFO - joeynmt.training - EPOCH 227\n",
            "2021-11-23 23:13:39,820 - INFO - joeynmt.training - Epoch 227: total training loss 1.69\n",
            "2021-11-23 23:13:39,821 - INFO - joeynmt.training - EPOCH 228\n",
            "2021-11-23 23:13:40,389 - INFO - joeynmt.training - Epoch 228: total training loss 1.67\n",
            "2021-11-23 23:13:40,390 - INFO - joeynmt.training - EPOCH 229\n",
            "2021-11-23 23:13:40,952 - INFO - joeynmt.training - Epoch 229: total training loss 1.63\n",
            "2021-11-23 23:13:40,953 - INFO - joeynmt.training - EPOCH 230\n",
            "2021-11-23 23:13:41,518 - INFO - joeynmt.training - Epoch 230: total training loss 1.62\n",
            "2021-11-23 23:13:41,518 - INFO - joeynmt.training - EPOCH 231\n",
            "2021-11-23 23:13:42,085 - INFO - joeynmt.training - Epoch 231: total training loss 1.63\n",
            "2021-11-23 23:13:42,086 - INFO - joeynmt.training - EPOCH 232\n",
            "2021-11-23 23:13:42,647 - INFO - joeynmt.training - Epoch 232: total training loss 1.61\n",
            "2021-11-23 23:13:42,648 - INFO - joeynmt.training - EPOCH 233\n",
            "2021-11-23 23:13:43,214 - INFO - joeynmt.training - Epoch 233: total training loss 1.62\n",
            "2021-11-23 23:13:43,214 - INFO - joeynmt.training - EPOCH 234\n",
            "2021-11-23 23:13:43,771 - INFO - joeynmt.training - Epoch 234: total training loss 1.65\n",
            "2021-11-23 23:13:43,771 - INFO - joeynmt.training - EPOCH 235\n",
            "2021-11-23 23:13:44,327 - INFO - joeynmt.training - Epoch 235: total training loss 1.59\n",
            "2021-11-23 23:13:44,328 - INFO - joeynmt.training - EPOCH 236\n",
            "2021-11-23 23:13:44,892 - INFO - joeynmt.training - Epoch 236: total training loss 1.63\n",
            "2021-11-23 23:13:44,892 - INFO - joeynmt.training - EPOCH 237\n",
            "2021-11-23 23:13:45,473 - INFO - joeynmt.training - Epoch 237: total training loss 1.63\n",
            "2021-11-23 23:13:45,474 - INFO - joeynmt.training - EPOCH 238\n",
            "2021-11-23 23:13:46,038 - INFO - joeynmt.training - Epoch 238: total training loss 1.56\n",
            "2021-11-23 23:13:46,039 - INFO - joeynmt.training - EPOCH 239\n",
            "2021-11-23 23:13:46,597 - INFO - joeynmt.training - Epoch 239: total training loss 1.59\n",
            "2021-11-23 23:13:46,597 - INFO - joeynmt.training - EPOCH 240\n",
            "2021-11-23 23:13:47,169 - INFO - joeynmt.training - Epoch 240: total training loss 1.61\n",
            "2021-11-23 23:13:47,169 - INFO - joeynmt.training - EPOCH 241\n",
            "2021-11-23 23:13:47,733 - INFO - joeynmt.training - Epoch 241: total training loss 1.55\n",
            "2021-11-23 23:13:47,733 - INFO - joeynmt.training - EPOCH 242\n",
            "2021-11-23 23:13:48,293 - INFO - joeynmt.training - Epoch 242: total training loss 1.54\n",
            "2021-11-23 23:13:48,293 - INFO - joeynmt.training - EPOCH 243\n",
            "2021-11-23 23:13:48,858 - INFO - joeynmt.training - Epoch 243: total training loss 1.55\n",
            "2021-11-23 23:13:48,859 - INFO - joeynmt.training - EPOCH 244\n",
            "2021-11-23 23:13:49,426 - INFO - joeynmt.training - Epoch 244: total training loss 1.53\n",
            "2021-11-23 23:13:49,427 - INFO - joeynmt.training - EPOCH 245\n",
            "2021-11-23 23:13:50,001 - INFO - joeynmt.training - Epoch 245: total training loss 1.56\n",
            "2021-11-23 23:13:50,002 - INFO - joeynmt.training - EPOCH 246\n",
            "2021-11-23 23:13:50,566 - INFO - joeynmt.training - Epoch 246: total training loss 1.52\n",
            "2021-11-23 23:13:50,566 - INFO - joeynmt.training - EPOCH 247\n",
            "2021-11-23 23:13:51,129 - INFO - joeynmt.training - Epoch 247: total training loss 1.50\n",
            "2021-11-23 23:13:51,130 - INFO - joeynmt.training - EPOCH 248\n",
            "2021-11-23 23:13:51,701 - INFO - joeynmt.training - Epoch 248: total training loss 1.50\n",
            "2021-11-23 23:13:51,702 - INFO - joeynmt.training - EPOCH 249\n",
            "2021-11-23 23:13:52,268 - INFO - joeynmt.training - Epoch 249: total training loss 1.50\n",
            "2021-11-23 23:13:52,268 - INFO - joeynmt.training - EPOCH 250\n",
            "2021-11-23 23:13:52,822 - INFO - joeynmt.training - Epoch 250, Step:      500, Batch Loss:     0.727115, Tokens per Sec:     6817, Lr: 0.000300\n",
            "2021-11-23 23:13:52,823 - INFO - joeynmt.training - Epoch 250: total training loss 1.48\n",
            "2021-11-23 23:13:52,823 - INFO - joeynmt.training - EPOCH 251\n",
            "2021-11-23 23:13:53,411 - INFO - joeynmt.training - Epoch 251: total training loss 1.49\n",
            "2021-11-23 23:13:53,411 - INFO - joeynmt.training - EPOCH 252\n",
            "2021-11-23 23:13:53,976 - INFO - joeynmt.training - Epoch 252: total training loss 1.46\n",
            "2021-11-23 23:13:53,977 - INFO - joeynmt.training - EPOCH 253\n",
            "2021-11-23 23:13:54,544 - INFO - joeynmt.training - Epoch 253: total training loss 1.46\n",
            "2021-11-23 23:13:54,544 - INFO - joeynmt.training - EPOCH 254\n",
            "2021-11-23 23:13:55,101 - INFO - joeynmt.training - Epoch 254: total training loss 1.47\n",
            "2021-11-23 23:13:55,101 - INFO - joeynmt.training - EPOCH 255\n",
            "2021-11-23 23:13:55,663 - INFO - joeynmt.training - Epoch 255: total training loss 1.44\n",
            "2021-11-23 23:13:55,663 - INFO - joeynmt.training - EPOCH 256\n",
            "2021-11-23 23:13:56,225 - INFO - joeynmt.training - Epoch 256: total training loss 1.43\n",
            "2021-11-23 23:13:56,225 - INFO - joeynmt.training - EPOCH 257\n",
            "2021-11-23 23:13:56,790 - INFO - joeynmt.training - Epoch 257: total training loss 1.44\n",
            "2021-11-23 23:13:56,791 - INFO - joeynmt.training - EPOCH 258\n",
            "2021-11-23 23:13:57,342 - INFO - joeynmt.training - Epoch 258: total training loss 1.40\n",
            "2021-11-23 23:13:57,342 - INFO - joeynmt.training - EPOCH 259\n",
            "2021-11-23 23:13:57,908 - INFO - joeynmt.training - Epoch 259: total training loss 1.42\n",
            "2021-11-23 23:13:57,909 - INFO - joeynmt.training - EPOCH 260\n",
            "2021-11-23 23:13:58,488 - INFO - joeynmt.training - Epoch 260: total training loss 1.39\n",
            "2021-11-23 23:13:58,488 - INFO - joeynmt.training - EPOCH 261\n",
            "2021-11-23 23:13:59,051 - INFO - joeynmt.training - Epoch 261: total training loss 1.39\n",
            "2021-11-23 23:13:59,051 - INFO - joeynmt.training - EPOCH 262\n",
            "2021-11-23 23:13:59,615 - INFO - joeynmt.training - Epoch 262: total training loss 1.39\n",
            "2021-11-23 23:13:59,615 - INFO - joeynmt.training - EPOCH 263\n",
            "2021-11-23 23:14:00,185 - INFO - joeynmt.training - Epoch 263: total training loss 1.43\n",
            "2021-11-23 23:14:00,185 - INFO - joeynmt.training - EPOCH 264\n",
            "2021-11-23 23:14:00,758 - INFO - joeynmt.training - Epoch 264: total training loss 1.42\n",
            "2021-11-23 23:14:00,758 - INFO - joeynmt.training - EPOCH 265\n",
            "2021-11-23 23:14:01,331 - INFO - joeynmt.training - Epoch 265: total training loss 1.39\n",
            "2021-11-23 23:14:01,331 - INFO - joeynmt.training - EPOCH 266\n",
            "2021-11-23 23:14:01,903 - INFO - joeynmt.training - Epoch 266: total training loss 1.35\n",
            "2021-11-23 23:14:01,903 - INFO - joeynmt.training - EPOCH 267\n",
            "2021-11-23 23:14:02,471 - INFO - joeynmt.training - Epoch 267: total training loss 1.39\n",
            "2021-11-23 23:14:02,472 - INFO - joeynmt.training - EPOCH 268\n",
            "2021-11-23 23:14:03,033 - INFO - joeynmt.training - Epoch 268: total training loss 1.32\n",
            "2021-11-23 23:14:03,033 - INFO - joeynmt.training - EPOCH 269\n",
            "2021-11-23 23:14:03,597 - INFO - joeynmt.training - Epoch 269: total training loss 1.33\n",
            "2021-11-23 23:14:03,598 - INFO - joeynmt.training - EPOCH 270\n",
            "2021-11-23 23:14:04,168 - INFO - joeynmt.training - Epoch 270: total training loss 1.32\n",
            "2021-11-23 23:14:04,169 - INFO - joeynmt.training - EPOCH 271\n",
            "2021-11-23 23:14:04,735 - INFO - joeynmt.training - Epoch 271: total training loss 1.35\n",
            "2021-11-23 23:14:04,736 - INFO - joeynmt.training - EPOCH 272\n",
            "2021-11-23 23:14:05,316 - INFO - joeynmt.training - Epoch 272: total training loss 1.36\n",
            "2021-11-23 23:14:05,317 - INFO - joeynmt.training - EPOCH 273\n",
            "2021-11-23 23:14:05,876 - INFO - joeynmt.training - Epoch 273: total training loss 1.31\n",
            "2021-11-23 23:14:05,877 - INFO - joeynmt.training - EPOCH 274\n",
            "2021-11-23 23:14:06,450 - INFO - joeynmt.training - Epoch 274: total training loss 1.33\n",
            "2021-11-23 23:14:06,451 - INFO - joeynmt.training - EPOCH 275\n",
            "2021-11-23 23:14:07,012 - INFO - joeynmt.training - Epoch 275: total training loss 1.25\n",
            "2021-11-23 23:14:07,012 - INFO - joeynmt.training - EPOCH 276\n",
            "2021-11-23 23:14:07,565 - INFO - joeynmt.training - Epoch 276: total training loss 1.25\n",
            "2021-11-23 23:14:07,566 - INFO - joeynmt.training - EPOCH 277\n",
            "2021-11-23 23:14:08,126 - INFO - joeynmt.training - Epoch 277: total training loss 1.28\n",
            "2021-11-23 23:14:08,126 - INFO - joeynmt.training - EPOCH 278\n",
            "2021-11-23 23:14:08,684 - INFO - joeynmt.training - Epoch 278: total training loss 1.23\n",
            "2021-11-23 23:14:08,685 - INFO - joeynmt.training - EPOCH 279\n",
            "2021-11-23 23:14:09,248 - INFO - joeynmt.training - Epoch 279: total training loss 1.27\n",
            "2021-11-23 23:14:09,248 - INFO - joeynmt.training - EPOCH 280\n",
            "2021-11-23 23:14:09,818 - INFO - joeynmt.training - Epoch 280: total training loss 1.25\n",
            "2021-11-23 23:14:09,819 - INFO - joeynmt.training - EPOCH 281\n",
            "2021-11-23 23:14:10,376 - INFO - joeynmt.training - Epoch 281: total training loss 1.24\n",
            "2021-11-23 23:14:10,377 - INFO - joeynmt.training - EPOCH 282\n",
            "2021-11-23 23:14:10,934 - INFO - joeynmt.training - Epoch 282: total training loss 1.25\n",
            "2021-11-23 23:14:10,934 - INFO - joeynmt.training - EPOCH 283\n",
            "2021-11-23 23:14:11,505 - INFO - joeynmt.training - Epoch 283: total training loss 1.22\n",
            "2021-11-23 23:14:11,505 - INFO - joeynmt.training - EPOCH 284\n",
            "2021-11-23 23:14:12,072 - INFO - joeynmt.training - Epoch 284: total training loss 1.19\n",
            "2021-11-23 23:14:12,073 - INFO - joeynmt.training - EPOCH 285\n",
            "2021-11-23 23:14:12,645 - INFO - joeynmt.training - Epoch 285: total training loss 1.22\n",
            "2021-11-23 23:14:12,646 - INFO - joeynmt.training - EPOCH 286\n",
            "2021-11-23 23:14:13,206 - INFO - joeynmt.training - Epoch 286: total training loss 1.21\n",
            "2021-11-23 23:14:13,206 - INFO - joeynmt.training - EPOCH 287\n",
            "2021-11-23 23:14:13,776 - INFO - joeynmt.training - Epoch 287: total training loss 1.20\n",
            "2021-11-23 23:14:13,777 - INFO - joeynmt.training - EPOCH 288\n",
            "2021-11-23 23:14:14,346 - INFO - joeynmt.training - Epoch 288: total training loss 1.17\n",
            "2021-11-23 23:14:14,347 - INFO - joeynmt.training - EPOCH 289\n",
            "2021-11-23 23:14:14,906 - INFO - joeynmt.training - Epoch 289: total training loss 1.17\n",
            "2021-11-23 23:14:14,906 - INFO - joeynmt.training - EPOCH 290\n",
            "2021-11-23 23:14:15,475 - INFO - joeynmt.training - Epoch 290: total training loss 1.13\n",
            "2021-11-23 23:14:15,476 - INFO - joeynmt.training - EPOCH 291\n",
            "2021-11-23 23:14:16,040 - INFO - joeynmt.training - Epoch 291: total training loss 1.14\n",
            "2021-11-23 23:14:16,040 - INFO - joeynmt.training - EPOCH 292\n",
            "2021-11-23 23:14:16,602 - INFO - joeynmt.training - Epoch 292: total training loss 1.14\n",
            "2021-11-23 23:14:16,603 - INFO - joeynmt.training - EPOCH 293\n",
            "2021-11-23 23:14:17,165 - INFO - joeynmt.training - Epoch 293: total training loss 1.16\n",
            "2021-11-23 23:14:17,166 - INFO - joeynmt.training - EPOCH 294\n",
            "2021-11-23 23:14:17,731 - INFO - joeynmt.training - Epoch 294: total training loss 1.14\n",
            "2021-11-23 23:14:17,731 - INFO - joeynmt.training - EPOCH 295\n",
            "2021-11-23 23:14:18,298 - INFO - joeynmt.training - Epoch 295: total training loss 1.16\n",
            "2021-11-23 23:14:18,299 - INFO - joeynmt.training - EPOCH 296\n",
            "2021-11-23 23:14:18,868 - INFO - joeynmt.training - Epoch 296: total training loss 1.08\n",
            "2021-11-23 23:14:18,868 - INFO - joeynmt.training - EPOCH 297\n",
            "2021-11-23 23:14:19,432 - INFO - joeynmt.training - Epoch 297: total training loss 1.11\n",
            "2021-11-23 23:14:19,432 - INFO - joeynmt.training - EPOCH 298\n",
            "2021-11-23 23:14:19,997 - INFO - joeynmt.training - Epoch 298: total training loss 1.11\n",
            "2021-11-23 23:14:19,997 - INFO - joeynmt.training - EPOCH 299\n",
            "2021-11-23 23:14:20,558 - INFO - joeynmt.training - Epoch 299: total training loss 1.10\n",
            "2021-11-23 23:14:20,558 - INFO - joeynmt.training - EPOCH 300\n",
            "2021-11-23 23:14:21,120 - INFO - joeynmt.training - Epoch 300, Step:      600, Batch Loss:     0.543758, Tokens per Sec:     6731, Lr: 0.000300\n",
            "2021-11-23 23:14:21,120 - INFO - joeynmt.training - Epoch 300: total training loss 1.10\n",
            "2021-11-23 23:14:21,120 - INFO - joeynmt.training - EPOCH 301\n",
            "2021-11-23 23:14:21,694 - INFO - joeynmt.training - Epoch 301: total training loss 1.09\n",
            "2021-11-23 23:14:21,694 - INFO - joeynmt.training - EPOCH 302\n",
            "2021-11-23 23:14:22,258 - INFO - joeynmt.training - Epoch 302: total training loss 1.06\n",
            "2021-11-23 23:14:22,258 - INFO - joeynmt.training - EPOCH 303\n",
            "2021-11-23 23:14:22,827 - INFO - joeynmt.training - Epoch 303: total training loss 1.08\n",
            "2021-11-23 23:14:22,827 - INFO - joeynmt.training - EPOCH 304\n",
            "2021-11-23 23:14:23,390 - INFO - joeynmt.training - Epoch 304: total training loss 1.08\n",
            "2021-11-23 23:14:23,391 - INFO - joeynmt.training - EPOCH 305\n",
            "2021-11-23 23:14:23,943 - INFO - joeynmt.training - Epoch 305: total training loss 1.04\n",
            "2021-11-23 23:14:23,944 - INFO - joeynmt.training - EPOCH 306\n",
            "2021-11-23 23:14:24,509 - INFO - joeynmt.training - Epoch 306: total training loss 1.06\n",
            "2021-11-23 23:14:24,509 - INFO - joeynmt.training - EPOCH 307\n",
            "2021-11-23 23:14:25,068 - INFO - joeynmt.training - Epoch 307: total training loss 1.00\n",
            "2021-11-23 23:14:25,069 - INFO - joeynmt.training - EPOCH 308\n",
            "2021-11-23 23:14:25,634 - INFO - joeynmt.training - Epoch 308: total training loss 1.04\n",
            "2021-11-23 23:14:25,634 - INFO - joeynmt.training - EPOCH 309\n",
            "2021-11-23 23:14:26,183 - INFO - joeynmt.training - Epoch 309: total training loss 0.97\n",
            "2021-11-23 23:14:26,183 - INFO - joeynmt.training - EPOCH 310\n",
            "2021-11-23 23:14:26,743 - INFO - joeynmt.training - Epoch 310: total training loss 1.05\n",
            "2021-11-23 23:14:26,744 - INFO - joeynmt.training - EPOCH 311\n",
            "2021-11-23 23:14:27,303 - INFO - joeynmt.training - Epoch 311: total training loss 1.00\n",
            "2021-11-23 23:14:27,303 - INFO - joeynmt.training - EPOCH 312\n",
            "2021-11-23 23:14:27,874 - INFO - joeynmt.training - Epoch 312: total training loss 1.02\n",
            "2021-11-23 23:14:27,874 - INFO - joeynmt.training - EPOCH 313\n",
            "2021-11-23 23:14:28,452 - INFO - joeynmt.training - Epoch 313: total training loss 1.02\n",
            "2021-11-23 23:14:28,452 - INFO - joeynmt.training - EPOCH 314\n",
            "2021-11-23 23:14:29,021 - INFO - joeynmt.training - Epoch 314: total training loss 1.01\n",
            "2021-11-23 23:14:29,021 - INFO - joeynmt.training - EPOCH 315\n",
            "2021-11-23 23:14:29,580 - INFO - joeynmt.training - Epoch 315: total training loss 0.99\n",
            "2021-11-23 23:14:29,580 - INFO - joeynmt.training - EPOCH 316\n",
            "2021-11-23 23:14:30,137 - INFO - joeynmt.training - Epoch 316: total training loss 0.96\n",
            "2021-11-23 23:14:30,137 - INFO - joeynmt.training - EPOCH 317\n",
            "2021-11-23 23:14:30,699 - INFO - joeynmt.training - Epoch 317: total training loss 0.99\n",
            "2021-11-23 23:14:30,700 - INFO - joeynmt.training - EPOCH 318\n",
            "2021-11-23 23:14:31,267 - INFO - joeynmt.training - Epoch 318: total training loss 0.98\n",
            "2021-11-23 23:14:31,267 - INFO - joeynmt.training - EPOCH 319\n",
            "2021-11-23 23:14:31,833 - INFO - joeynmt.training - Epoch 319: total training loss 0.91\n",
            "2021-11-23 23:14:31,834 - INFO - joeynmt.training - EPOCH 320\n",
            "2021-11-23 23:14:32,398 - INFO - joeynmt.training - Epoch 320: total training loss 0.92\n",
            "2021-11-23 23:14:32,399 - INFO - joeynmt.training - EPOCH 321\n",
            "2021-11-23 23:14:32,964 - INFO - joeynmt.training - Epoch 321: total training loss 0.97\n",
            "2021-11-23 23:14:32,965 - INFO - joeynmt.training - EPOCH 322\n",
            "2021-11-23 23:14:33,531 - INFO - joeynmt.training - Epoch 322: total training loss 0.95\n",
            "2021-11-23 23:14:33,531 - INFO - joeynmt.training - EPOCH 323\n",
            "2021-11-23 23:14:34,090 - INFO - joeynmt.training - Epoch 323: total training loss 0.94\n",
            "2021-11-23 23:14:34,090 - INFO - joeynmt.training - EPOCH 324\n",
            "2021-11-23 23:14:34,648 - INFO - joeynmt.training - Epoch 324: total training loss 0.92\n",
            "2021-11-23 23:14:34,648 - INFO - joeynmt.training - EPOCH 325\n",
            "2021-11-23 23:14:35,201 - INFO - joeynmt.training - Epoch 325: total training loss 0.90\n",
            "2021-11-23 23:14:35,202 - INFO - joeynmt.training - EPOCH 326\n",
            "2021-11-23 23:14:35,774 - INFO - joeynmt.training - Epoch 326: total training loss 0.91\n",
            "2021-11-23 23:14:35,774 - INFO - joeynmt.training - EPOCH 327\n",
            "2021-11-23 23:14:36,345 - INFO - joeynmt.training - Epoch 327: total training loss 0.90\n",
            "2021-11-23 23:14:36,345 - INFO - joeynmt.training - EPOCH 328\n",
            "2021-11-23 23:14:36,900 - INFO - joeynmt.training - Epoch 328: total training loss 0.89\n",
            "2021-11-23 23:14:36,901 - INFO - joeynmt.training - EPOCH 329\n",
            "2021-11-23 23:14:37,462 - INFO - joeynmt.training - Epoch 329: total training loss 0.89\n",
            "2021-11-23 23:14:37,463 - INFO - joeynmt.training - EPOCH 330\n",
            "2021-11-23 23:14:38,031 - INFO - joeynmt.training - Epoch 330: total training loss 0.90\n",
            "2021-11-23 23:14:38,031 - INFO - joeynmt.training - EPOCH 331\n",
            "2021-11-23 23:14:38,605 - INFO - joeynmt.training - Epoch 331: total training loss 0.86\n",
            "2021-11-23 23:14:38,605 - INFO - joeynmt.training - EPOCH 332\n",
            "2021-11-23 23:14:39,175 - INFO - joeynmt.training - Epoch 332: total training loss 0.83\n",
            "2021-11-23 23:14:39,176 - INFO - joeynmt.training - EPOCH 333\n",
            "2021-11-23 23:14:39,729 - INFO - joeynmt.training - Epoch 333: total training loss 0.89\n",
            "2021-11-23 23:14:39,730 - INFO - joeynmt.training - EPOCH 334\n",
            "2021-11-23 23:14:40,291 - INFO - joeynmt.training - Epoch 334: total training loss 0.89\n",
            "2021-11-23 23:14:40,291 - INFO - joeynmt.training - EPOCH 335\n",
            "2021-11-23 23:14:40,861 - INFO - joeynmt.training - Epoch 335: total training loss 0.85\n",
            "2021-11-23 23:14:40,862 - INFO - joeynmt.training - EPOCH 336\n",
            "2021-11-23 23:14:41,424 - INFO - joeynmt.training - Epoch 336: total training loss 0.81\n",
            "2021-11-23 23:14:41,424 - INFO - joeynmt.training - EPOCH 337\n",
            "2021-11-23 23:14:41,989 - INFO - joeynmt.training - Epoch 337: total training loss 0.85\n",
            "2021-11-23 23:14:41,989 - INFO - joeynmt.training - EPOCH 338\n",
            "2021-11-23 23:14:42,560 - INFO - joeynmt.training - Epoch 338: total training loss 0.84\n",
            "2021-11-23 23:14:42,560 - INFO - joeynmt.training - EPOCH 339\n",
            "2021-11-23 23:14:43,143 - INFO - joeynmt.training - Epoch 339: total training loss 0.81\n",
            "2021-11-23 23:14:43,144 - INFO - joeynmt.training - EPOCH 340\n",
            "2021-11-23 23:14:43,706 - INFO - joeynmt.training - Epoch 340: total training loss 0.80\n",
            "2021-11-23 23:14:43,706 - INFO - joeynmt.training - EPOCH 341\n",
            "2021-11-23 23:14:44,277 - INFO - joeynmt.training - Epoch 341: total training loss 0.82\n",
            "2021-11-23 23:14:44,278 - INFO - joeynmt.training - EPOCH 342\n",
            "2021-11-23 23:14:44,846 - INFO - joeynmt.training - Epoch 342: total training loss 0.79\n",
            "2021-11-23 23:14:44,846 - INFO - joeynmt.training - EPOCH 343\n",
            "2021-11-23 23:14:45,428 - INFO - joeynmt.training - Epoch 343: total training loss 0.79\n",
            "2021-11-23 23:14:45,428 - INFO - joeynmt.training - EPOCH 344\n",
            "2021-11-23 23:14:46,000 - INFO - joeynmt.training - Epoch 344: total training loss 0.78\n",
            "2021-11-23 23:14:46,001 - INFO - joeynmt.training - EPOCH 345\n",
            "2021-11-23 23:14:46,563 - INFO - joeynmt.training - Epoch 345: total training loss 0.79\n",
            "2021-11-23 23:14:46,564 - INFO - joeynmt.training - EPOCH 346\n",
            "2021-11-23 23:14:47,125 - INFO - joeynmt.training - Epoch 346: total training loss 0.78\n",
            "2021-11-23 23:14:47,125 - INFO - joeynmt.training - EPOCH 347\n",
            "2021-11-23 23:14:47,697 - INFO - joeynmt.training - Epoch 347: total training loss 0.81\n",
            "2021-11-23 23:14:47,697 - INFO - joeynmt.training - EPOCH 348\n",
            "2021-11-23 23:14:48,270 - INFO - joeynmt.training - Epoch 348: total training loss 0.75\n",
            "2021-11-23 23:14:48,271 - INFO - joeynmt.training - EPOCH 349\n",
            "2021-11-23 23:14:48,836 - INFO - joeynmt.training - Epoch 349: total training loss 0.74\n",
            "2021-11-23 23:14:48,836 - INFO - joeynmt.training - EPOCH 350\n",
            "2021-11-23 23:14:49,408 - INFO - joeynmt.training - Epoch 350, Step:      700, Batch Loss:     0.379796, Tokens per Sec:     6599, Lr: 0.000300\n",
            "2021-11-23 23:14:49,409 - INFO - joeynmt.training - Epoch 350: total training loss 0.76\n",
            "2021-11-23 23:14:49,409 - INFO - joeynmt.training - EPOCH 351\n",
            "2021-11-23 23:14:49,963 - INFO - joeynmt.training - Epoch 351: total training loss 0.72\n",
            "2021-11-23 23:14:49,963 - INFO - joeynmt.training - EPOCH 352\n",
            "2021-11-23 23:14:50,524 - INFO - joeynmt.training - Epoch 352: total training loss 0.73\n",
            "2021-11-23 23:14:50,525 - INFO - joeynmt.training - EPOCH 353\n",
            "2021-11-23 23:14:51,098 - INFO - joeynmt.training - Epoch 353: total training loss 0.75\n",
            "2021-11-23 23:14:51,098 - INFO - joeynmt.training - EPOCH 354\n",
            "2021-11-23 23:14:51,660 - INFO - joeynmt.training - Epoch 354: total training loss 0.76\n",
            "2021-11-23 23:14:51,660 - INFO - joeynmt.training - EPOCH 355\n",
            "2021-11-23 23:14:52,231 - INFO - joeynmt.training - Epoch 355: total training loss 0.75\n",
            "2021-11-23 23:14:52,232 - INFO - joeynmt.training - EPOCH 356\n",
            "2021-11-23 23:14:52,798 - INFO - joeynmt.training - Epoch 356: total training loss 0.75\n",
            "2021-11-23 23:14:52,799 - INFO - joeynmt.training - EPOCH 357\n",
            "2021-11-23 23:14:53,363 - INFO - joeynmt.training - Epoch 357: total training loss 0.72\n",
            "2021-11-23 23:14:53,364 - INFO - joeynmt.training - EPOCH 358\n",
            "2021-11-23 23:14:53,925 - INFO - joeynmt.training - Epoch 358: total training loss 0.74\n",
            "2021-11-23 23:14:53,926 - INFO - joeynmt.training - EPOCH 359\n",
            "2021-11-23 23:14:54,489 - INFO - joeynmt.training - Epoch 359: total training loss 0.71\n",
            "2021-11-23 23:14:54,490 - INFO - joeynmt.training - EPOCH 360\n",
            "2021-11-23 23:14:55,060 - INFO - joeynmt.training - Epoch 360: total training loss 0.71\n",
            "2021-11-23 23:14:55,061 - INFO - joeynmt.training - EPOCH 361\n",
            "2021-11-23 23:14:55,622 - INFO - joeynmt.training - Epoch 361: total training loss 0.69\n",
            "2021-11-23 23:14:55,622 - INFO - joeynmt.training - EPOCH 362\n",
            "2021-11-23 23:14:56,177 - INFO - joeynmt.training - Epoch 362: total training loss 0.69\n",
            "2021-11-23 23:14:56,177 - INFO - joeynmt.training - EPOCH 363\n",
            "2021-11-23 23:14:56,740 - INFO - joeynmt.training - Epoch 363: total training loss 0.71\n",
            "2021-11-23 23:14:56,740 - INFO - joeynmt.training - EPOCH 364\n",
            "2021-11-23 23:14:57,306 - INFO - joeynmt.training - Epoch 364: total training loss 0.69\n",
            "2021-11-23 23:14:57,306 - INFO - joeynmt.training - EPOCH 365\n",
            "2021-11-23 23:14:57,874 - INFO - joeynmt.training - Epoch 365: total training loss 0.69\n",
            "2021-11-23 23:14:57,874 - INFO - joeynmt.training - EPOCH 366\n",
            "2021-11-23 23:14:58,438 - INFO - joeynmt.training - Epoch 366: total training loss 0.66\n",
            "2021-11-23 23:14:58,438 - INFO - joeynmt.training - EPOCH 367\n",
            "2021-11-23 23:14:59,000 - INFO - joeynmt.training - Epoch 367: total training loss 0.67\n",
            "2021-11-23 23:14:59,000 - INFO - joeynmt.training - EPOCH 368\n",
            "2021-11-23 23:14:59,568 - INFO - joeynmt.training - Epoch 368: total training loss 0.65\n",
            "2021-11-23 23:14:59,569 - INFO - joeynmt.training - EPOCH 369\n",
            "2021-11-23 23:15:00,130 - INFO - joeynmt.training - Epoch 369: total training loss 0.70\n",
            "2021-11-23 23:15:00,130 - INFO - joeynmt.training - EPOCH 370\n",
            "2021-11-23 23:15:00,695 - INFO - joeynmt.training - Epoch 370: total training loss 0.68\n",
            "2021-11-23 23:15:00,695 - INFO - joeynmt.training - EPOCH 371\n",
            "2021-11-23 23:15:01,272 - INFO - joeynmt.training - Epoch 371: total training loss 0.65\n",
            "2021-11-23 23:15:01,272 - INFO - joeynmt.training - EPOCH 372\n",
            "2021-11-23 23:15:01,843 - INFO - joeynmt.training - Epoch 372: total training loss 0.62\n",
            "2021-11-23 23:15:01,844 - INFO - joeynmt.training - EPOCH 373\n",
            "2021-11-23 23:15:02,406 - INFO - joeynmt.training - Epoch 373: total training loss 0.68\n",
            "2021-11-23 23:15:02,406 - INFO - joeynmt.training - EPOCH 374\n",
            "2021-11-23 23:15:02,993 - INFO - joeynmt.training - Epoch 374: total training loss 0.66\n",
            "2021-11-23 23:15:02,993 - INFO - joeynmt.training - EPOCH 375\n",
            "2021-11-23 23:15:03,568 - INFO - joeynmt.training - Epoch 375: total training loss 0.66\n",
            "2021-11-23 23:15:03,569 - INFO - joeynmt.training - EPOCH 376\n",
            "2021-11-23 23:15:04,144 - INFO - joeynmt.training - Epoch 376: total training loss 0.64\n",
            "2021-11-23 23:15:04,144 - INFO - joeynmt.training - EPOCH 377\n",
            "2021-11-23 23:15:04,702 - INFO - joeynmt.training - Epoch 377: total training loss 0.64\n",
            "2021-11-23 23:15:04,702 - INFO - joeynmt.training - EPOCH 378\n",
            "2021-11-23 23:15:05,282 - INFO - joeynmt.training - Epoch 378: total training loss 0.64\n",
            "2021-11-23 23:15:05,283 - INFO - joeynmt.training - EPOCH 379\n",
            "2021-11-23 23:15:05,852 - INFO - joeynmt.training - Epoch 379: total training loss 0.62\n",
            "2021-11-23 23:15:05,853 - INFO - joeynmt.training - EPOCH 380\n",
            "2021-11-23 23:15:06,414 - INFO - joeynmt.training - Epoch 380: total training loss 0.64\n",
            "2021-11-23 23:15:06,415 - INFO - joeynmt.training - EPOCH 381\n",
            "2021-11-23 23:15:06,981 - INFO - joeynmt.training - Epoch 381: total training loss 0.64\n",
            "2021-11-23 23:15:06,982 - INFO - joeynmt.training - EPOCH 382\n",
            "2021-11-23 23:15:07,550 - INFO - joeynmt.training - Epoch 382: total training loss 0.59\n",
            "2021-11-23 23:15:07,551 - INFO - joeynmt.training - EPOCH 383\n",
            "2021-11-23 23:15:08,112 - INFO - joeynmt.training - Epoch 383: total training loss 0.63\n",
            "2021-11-23 23:15:08,113 - INFO - joeynmt.training - EPOCH 384\n",
            "2021-11-23 23:15:08,681 - INFO - joeynmt.training - Epoch 384: total training loss 0.60\n",
            "2021-11-23 23:15:08,681 - INFO - joeynmt.training - EPOCH 385\n",
            "2021-11-23 23:15:09,239 - INFO - joeynmt.training - Epoch 385: total training loss 0.61\n",
            "2021-11-23 23:15:09,240 - INFO - joeynmt.training - EPOCH 386\n",
            "2021-11-23 23:15:09,813 - INFO - joeynmt.training - Epoch 386: total training loss 0.60\n",
            "2021-11-23 23:15:09,814 - INFO - joeynmt.training - EPOCH 387\n",
            "2021-11-23 23:15:10,377 - INFO - joeynmt.training - Epoch 387: total training loss 0.62\n",
            "2021-11-23 23:15:10,378 - INFO - joeynmt.training - EPOCH 388\n",
            "2021-11-23 23:15:10,945 - INFO - joeynmt.training - Epoch 388: total training loss 0.57\n",
            "2021-11-23 23:15:10,945 - INFO - joeynmt.training - EPOCH 389\n",
            "2021-11-23 23:15:11,518 - INFO - joeynmt.training - Epoch 389: total training loss 0.63\n",
            "2021-11-23 23:15:11,518 - INFO - joeynmt.training - EPOCH 390\n",
            "2021-11-23 23:15:12,082 - INFO - joeynmt.training - Epoch 390: total training loss 0.59\n",
            "2021-11-23 23:15:12,082 - INFO - joeynmt.training - EPOCH 391\n",
            "2021-11-23 23:15:12,645 - INFO - joeynmt.training - Epoch 391: total training loss 0.61\n",
            "2021-11-23 23:15:12,646 - INFO - joeynmt.training - EPOCH 392\n",
            "2021-11-23 23:15:13,210 - INFO - joeynmt.training - Epoch 392: total training loss 0.59\n",
            "2021-11-23 23:15:13,210 - INFO - joeynmt.training - EPOCH 393\n",
            "2021-11-23 23:15:13,775 - INFO - joeynmt.training - Epoch 393: total training loss 0.59\n",
            "2021-11-23 23:15:13,775 - INFO - joeynmt.training - EPOCH 394\n",
            "2021-11-23 23:15:14,344 - INFO - joeynmt.training - Epoch 394: total training loss 0.59\n",
            "2021-11-23 23:15:14,344 - INFO - joeynmt.training - EPOCH 395\n",
            "2021-11-23 23:15:14,913 - INFO - joeynmt.training - Epoch 395: total training loss 0.58\n",
            "2021-11-23 23:15:14,914 - INFO - joeynmt.training - EPOCH 396\n",
            "2021-11-23 23:15:15,473 - INFO - joeynmt.training - Epoch 396: total training loss 0.56\n",
            "2021-11-23 23:15:15,473 - INFO - joeynmt.training - EPOCH 397\n",
            "2021-11-23 23:15:16,040 - INFO - joeynmt.training - Epoch 397: total training loss 0.56\n",
            "2021-11-23 23:15:16,040 - INFO - joeynmt.training - EPOCH 398\n",
            "2021-11-23 23:15:16,613 - INFO - joeynmt.training - Epoch 398: total training loss 0.59\n",
            "2021-11-23 23:15:16,614 - INFO - joeynmt.training - EPOCH 399\n",
            "2021-11-23 23:15:17,189 - INFO - joeynmt.training - Epoch 399: total training loss 0.53\n",
            "2021-11-23 23:15:17,190 - INFO - joeynmt.training - EPOCH 400\n",
            "2021-11-23 23:15:17,753 - INFO - joeynmt.training - Epoch 400, Step:      800, Batch Loss:     0.283581, Tokens per Sec:     6713, Lr: 0.000300\n",
            "2021-11-23 23:15:17,753 - INFO - joeynmt.training - Epoch 400: total training loss 0.57\n",
            "2021-11-23 23:15:17,753 - INFO - joeynmt.training - EPOCH 401\n",
            "2021-11-23 23:15:18,318 - INFO - joeynmt.training - Epoch 401: total training loss 0.58\n",
            "2021-11-23 23:15:18,319 - INFO - joeynmt.training - EPOCH 402\n",
            "2021-11-23 23:15:18,888 - INFO - joeynmt.training - Epoch 402: total training loss 0.55\n",
            "2021-11-23 23:15:18,888 - INFO - joeynmt.training - EPOCH 403\n",
            "2021-11-23 23:15:19,458 - INFO - joeynmt.training - Epoch 403: total training loss 0.52\n",
            "2021-11-23 23:15:19,459 - INFO - joeynmt.training - EPOCH 404\n",
            "2021-11-23 23:15:20,026 - INFO - joeynmt.training - Epoch 404: total training loss 0.55\n",
            "2021-11-23 23:15:20,026 - INFO - joeynmt.training - EPOCH 405\n",
            "2021-11-23 23:15:20,605 - INFO - joeynmt.training - Epoch 405: total training loss 0.54\n",
            "2021-11-23 23:15:20,605 - INFO - joeynmt.training - EPOCH 406\n",
            "2021-11-23 23:15:21,171 - INFO - joeynmt.training - Epoch 406: total training loss 0.56\n",
            "2021-11-23 23:15:21,172 - INFO - joeynmt.training - EPOCH 407\n",
            "2021-11-23 23:15:21,754 - INFO - joeynmt.training - Epoch 407: total training loss 0.53\n",
            "2021-11-23 23:15:21,754 - INFO - joeynmt.training - EPOCH 408\n",
            "2021-11-23 23:15:22,324 - INFO - joeynmt.training - Epoch 408: total training loss 0.54\n",
            "2021-11-23 23:15:22,324 - INFO - joeynmt.training - EPOCH 409\n",
            "2021-11-23 23:15:22,884 - INFO - joeynmt.training - Epoch 409: total training loss 0.53\n",
            "2021-11-23 23:15:22,884 - INFO - joeynmt.training - EPOCH 410\n",
            "2021-11-23 23:15:23,448 - INFO - joeynmt.training - Epoch 410: total training loss 0.52\n",
            "2021-11-23 23:15:23,449 - INFO - joeynmt.training - EPOCH 411\n",
            "2021-11-23 23:15:24,012 - INFO - joeynmt.training - Epoch 411: total training loss 0.54\n",
            "2021-11-23 23:15:24,012 - INFO - joeynmt.training - EPOCH 412\n",
            "2021-11-23 23:15:24,585 - INFO - joeynmt.training - Epoch 412: total training loss 0.50\n",
            "2021-11-23 23:15:24,586 - INFO - joeynmt.training - EPOCH 413\n",
            "2021-11-23 23:15:25,141 - INFO - joeynmt.training - Epoch 413: total training loss 0.49\n",
            "2021-11-23 23:15:25,141 - INFO - joeynmt.training - EPOCH 414\n",
            "2021-11-23 23:15:25,697 - INFO - joeynmt.training - Epoch 414: total training loss 0.50\n",
            "2021-11-23 23:15:25,698 - INFO - joeynmt.training - EPOCH 415\n",
            "2021-11-23 23:15:26,268 - INFO - joeynmt.training - Epoch 415: total training loss 0.51\n",
            "2021-11-23 23:15:26,269 - INFO - joeynmt.training - EPOCH 416\n",
            "2021-11-23 23:15:26,831 - INFO - joeynmt.training - Epoch 416: total training loss 0.49\n",
            "2021-11-23 23:15:26,832 - INFO - joeynmt.training - EPOCH 417\n",
            "2021-11-23 23:15:27,394 - INFO - joeynmt.training - Epoch 417: total training loss 0.51\n",
            "2021-11-23 23:15:27,394 - INFO - joeynmt.training - EPOCH 418\n",
            "2021-11-23 23:15:27,952 - INFO - joeynmt.training - Epoch 418: total training loss 0.50\n",
            "2021-11-23 23:15:27,952 - INFO - joeynmt.training - EPOCH 419\n",
            "2021-11-23 23:15:28,510 - INFO - joeynmt.training - Epoch 419: total training loss 0.52\n",
            "2021-11-23 23:15:28,511 - INFO - joeynmt.training - EPOCH 420\n",
            "2021-11-23 23:15:29,090 - INFO - joeynmt.training - Epoch 420: total training loss 0.48\n",
            "2021-11-23 23:15:29,091 - INFO - joeynmt.training - EPOCH 421\n",
            "2021-11-23 23:15:29,670 - INFO - joeynmt.training - Epoch 421: total training loss 0.49\n",
            "2021-11-23 23:15:29,670 - INFO - joeynmt.training - EPOCH 422\n",
            "2021-11-23 23:15:30,243 - INFO - joeynmt.training - Epoch 422: total training loss 0.47\n",
            "2021-11-23 23:15:30,244 - INFO - joeynmt.training - EPOCH 423\n",
            "2021-11-23 23:15:30,816 - INFO - joeynmt.training - Epoch 423: total training loss 0.47\n",
            "2021-11-23 23:15:30,817 - INFO - joeynmt.training - EPOCH 424\n",
            "2021-11-23 23:15:31,383 - INFO - joeynmt.training - Epoch 424: total training loss 0.46\n",
            "2021-11-23 23:15:31,383 - INFO - joeynmt.training - EPOCH 425\n",
            "2021-11-23 23:15:31,946 - INFO - joeynmt.training - Epoch 425: total training loss 0.48\n",
            "2021-11-23 23:15:31,946 - INFO - joeynmt.training - EPOCH 426\n",
            "2021-11-23 23:15:32,510 - INFO - joeynmt.training - Epoch 426: total training loss 0.50\n",
            "2021-11-23 23:15:32,510 - INFO - joeynmt.training - EPOCH 427\n",
            "2021-11-23 23:15:33,075 - INFO - joeynmt.training - Epoch 427: total training loss 0.47\n",
            "2021-11-23 23:15:33,075 - INFO - joeynmt.training - EPOCH 428\n",
            "2021-11-23 23:15:33,635 - INFO - joeynmt.training - Epoch 428: total training loss 0.45\n",
            "2021-11-23 23:15:33,635 - INFO - joeynmt.training - EPOCH 429\n",
            "2021-11-23 23:15:34,202 - INFO - joeynmt.training - Epoch 429: total training loss 0.48\n",
            "2021-11-23 23:15:34,202 - INFO - joeynmt.training - EPOCH 430\n",
            "2021-11-23 23:15:34,776 - INFO - joeynmt.training - Epoch 430: total training loss 0.46\n",
            "2021-11-23 23:15:34,777 - INFO - joeynmt.training - EPOCH 431\n",
            "2021-11-23 23:15:35,348 - INFO - joeynmt.training - Epoch 431: total training loss 0.47\n",
            "2021-11-23 23:15:35,348 - INFO - joeynmt.training - EPOCH 432\n",
            "2021-11-23 23:15:35,914 - INFO - joeynmt.training - Epoch 432: total training loss 0.46\n",
            "2021-11-23 23:15:35,915 - INFO - joeynmt.training - EPOCH 433\n",
            "2021-11-23 23:15:36,485 - INFO - joeynmt.training - Epoch 433: total training loss 0.45\n",
            "2021-11-23 23:15:36,486 - INFO - joeynmt.training - EPOCH 434\n",
            "2021-11-23 23:15:37,054 - INFO - joeynmt.training - Epoch 434: total training loss 0.44\n",
            "2021-11-23 23:15:37,055 - INFO - joeynmt.training - EPOCH 435\n",
            "2021-11-23 23:15:37,633 - INFO - joeynmt.training - Epoch 435: total training loss 0.46\n",
            "2021-11-23 23:15:37,633 - INFO - joeynmt.training - EPOCH 436\n",
            "2021-11-23 23:15:38,197 - INFO - joeynmt.training - Epoch 436: total training loss 0.47\n",
            "2021-11-23 23:15:38,197 - INFO - joeynmt.training - EPOCH 437\n",
            "2021-11-23 23:15:38,758 - INFO - joeynmt.training - Epoch 437: total training loss 0.43\n",
            "2021-11-23 23:15:38,758 - INFO - joeynmt.training - EPOCH 438\n",
            "2021-11-23 23:15:39,324 - INFO - joeynmt.training - Epoch 438: total training loss 0.44\n",
            "2021-11-23 23:15:39,324 - INFO - joeynmt.training - EPOCH 439\n",
            "2021-11-23 23:15:39,892 - INFO - joeynmt.training - Epoch 439: total training loss 0.46\n",
            "2021-11-23 23:15:39,893 - INFO - joeynmt.training - EPOCH 440\n",
            "2021-11-23 23:15:40,449 - INFO - joeynmt.training - Epoch 440: total training loss 0.43\n",
            "2021-11-23 23:15:40,450 - INFO - joeynmt.training - EPOCH 441\n",
            "2021-11-23 23:15:41,025 - INFO - joeynmt.training - Epoch 441: total training loss 0.43\n",
            "2021-11-23 23:15:41,025 - INFO - joeynmt.training - EPOCH 442\n",
            "2021-11-23 23:15:41,597 - INFO - joeynmt.training - Epoch 442: total training loss 0.46\n",
            "2021-11-23 23:15:41,597 - INFO - joeynmt.training - EPOCH 443\n",
            "2021-11-23 23:15:42,168 - INFO - joeynmt.training - Epoch 443: total training loss 0.44\n",
            "2021-11-23 23:15:42,169 - INFO - joeynmt.training - EPOCH 444\n",
            "2021-11-23 23:15:42,733 - INFO - joeynmt.training - Epoch 444: total training loss 0.43\n",
            "2021-11-23 23:15:42,734 - INFO - joeynmt.training - EPOCH 445\n",
            "2021-11-23 23:15:43,300 - INFO - joeynmt.training - Epoch 445: total training loss 0.42\n",
            "2021-11-23 23:15:43,300 - INFO - joeynmt.training - EPOCH 446\n",
            "2021-11-23 23:15:43,869 - INFO - joeynmt.training - Epoch 446: total training loss 0.42\n",
            "2021-11-23 23:15:43,869 - INFO - joeynmt.training - EPOCH 447\n",
            "2021-11-23 23:15:44,436 - INFO - joeynmt.training - Epoch 447: total training loss 0.42\n",
            "2021-11-23 23:15:44,436 - INFO - joeynmt.training - EPOCH 448\n",
            "2021-11-23 23:15:45,014 - INFO - joeynmt.training - Epoch 448: total training loss 0.44\n",
            "2021-11-23 23:15:45,015 - INFO - joeynmt.training - EPOCH 449\n",
            "2021-11-23 23:15:45,579 - INFO - joeynmt.training - Epoch 449: total training loss 0.41\n",
            "2021-11-23 23:15:45,580 - INFO - joeynmt.training - EPOCH 450\n",
            "2021-11-23 23:15:46,142 - INFO - joeynmt.training - Epoch 450, Step:      900, Batch Loss:     0.209637, Tokens per Sec:     6717, Lr: 0.000300\n",
            "2021-11-23 23:15:46,142 - INFO - joeynmt.training - Epoch 450: total training loss 0.40\n",
            "2021-11-23 23:15:46,143 - INFO - joeynmt.training - EPOCH 451\n",
            "2021-11-23 23:15:46,710 - INFO - joeynmt.training - Epoch 451: total training loss 0.42\n",
            "2021-11-23 23:15:46,710 - INFO - joeynmt.training - EPOCH 452\n",
            "2021-11-23 23:15:47,277 - INFO - joeynmt.training - Epoch 452: total training loss 0.42\n",
            "2021-11-23 23:15:47,278 - INFO - joeynmt.training - EPOCH 453\n",
            "2021-11-23 23:15:47,848 - INFO - joeynmt.training - Epoch 453: total training loss 0.39\n",
            "2021-11-23 23:15:47,848 - INFO - joeynmt.training - EPOCH 454\n",
            "2021-11-23 23:15:48,406 - INFO - joeynmt.training - Epoch 454: total training loss 0.42\n",
            "2021-11-23 23:15:48,407 - INFO - joeynmt.training - EPOCH 455\n",
            "2021-11-23 23:15:48,974 - INFO - joeynmt.training - Epoch 455: total training loss 0.41\n",
            "2021-11-23 23:15:48,975 - INFO - joeynmt.training - EPOCH 456\n",
            "2021-11-23 23:15:49,533 - INFO - joeynmt.training - Epoch 456: total training loss 0.40\n",
            "2021-11-23 23:15:49,533 - INFO - joeynmt.training - EPOCH 457\n",
            "2021-11-23 23:15:50,100 - INFO - joeynmt.training - Epoch 457: total training loss 0.39\n",
            "2021-11-23 23:15:50,100 - INFO - joeynmt.training - EPOCH 458\n",
            "2021-11-23 23:15:50,658 - INFO - joeynmt.training - Epoch 458: total training loss 0.41\n",
            "2021-11-23 23:15:50,658 - INFO - joeynmt.training - EPOCH 459\n",
            "2021-11-23 23:15:51,232 - INFO - joeynmt.training - Epoch 459: total training loss 0.41\n",
            "2021-11-23 23:15:51,233 - INFO - joeynmt.training - EPOCH 460\n",
            "2021-11-23 23:15:51,782 - INFO - joeynmt.training - Epoch 460: total training loss 0.41\n",
            "2021-11-23 23:15:51,782 - INFO - joeynmt.training - EPOCH 461\n",
            "2021-11-23 23:15:52,350 - INFO - joeynmt.training - Epoch 461: total training loss 0.39\n",
            "2021-11-23 23:15:52,350 - INFO - joeynmt.training - EPOCH 462\n",
            "2021-11-23 23:15:52,917 - INFO - joeynmt.training - Epoch 462: total training loss 0.40\n",
            "2021-11-23 23:15:52,918 - INFO - joeynmt.training - EPOCH 463\n",
            "2021-11-23 23:15:53,475 - INFO - joeynmt.training - Epoch 463: total training loss 0.40\n",
            "2021-11-23 23:15:53,476 - INFO - joeynmt.training - EPOCH 464\n",
            "2021-11-23 23:15:54,047 - INFO - joeynmt.training - Epoch 464: total training loss 0.37\n",
            "2021-11-23 23:15:54,048 - INFO - joeynmt.training - EPOCH 465\n",
            "2021-11-23 23:15:54,608 - INFO - joeynmt.training - Epoch 465: total training loss 0.37\n",
            "2021-11-23 23:15:54,609 - INFO - joeynmt.training - EPOCH 466\n",
            "2021-11-23 23:15:55,162 - INFO - joeynmt.training - Epoch 466: total training loss 0.35\n",
            "2021-11-23 23:15:55,162 - INFO - joeynmt.training - EPOCH 467\n",
            "2021-11-23 23:15:55,722 - INFO - joeynmt.training - Epoch 467: total training loss 0.39\n",
            "2021-11-23 23:15:55,722 - INFO - joeynmt.training - EPOCH 468\n",
            "2021-11-23 23:15:56,285 - INFO - joeynmt.training - Epoch 468: total training loss 0.38\n",
            "2021-11-23 23:15:56,286 - INFO - joeynmt.training - EPOCH 469\n",
            "2021-11-23 23:15:56,843 - INFO - joeynmt.training - Epoch 469: total training loss 0.39\n",
            "2021-11-23 23:15:56,844 - INFO - joeynmt.training - EPOCH 470\n",
            "2021-11-23 23:15:57,410 - INFO - joeynmt.training - Epoch 470: total training loss 0.38\n",
            "2021-11-23 23:15:57,410 - INFO - joeynmt.training - EPOCH 471\n",
            "2021-11-23 23:15:57,979 - INFO - joeynmt.training - Epoch 471: total training loss 0.38\n",
            "2021-11-23 23:15:57,979 - INFO - joeynmt.training - EPOCH 472\n",
            "2021-11-23 23:15:58,545 - INFO - joeynmt.training - Epoch 472: total training loss 0.40\n",
            "2021-11-23 23:15:58,546 - INFO - joeynmt.training - EPOCH 473\n",
            "2021-11-23 23:15:59,123 - INFO - joeynmt.training - Epoch 473: total training loss 0.36\n",
            "2021-11-23 23:15:59,124 - INFO - joeynmt.training - EPOCH 474\n",
            "2021-11-23 23:15:59,679 - INFO - joeynmt.training - Epoch 474: total training loss 0.37\n",
            "2021-11-23 23:15:59,679 - INFO - joeynmt.training - EPOCH 475\n",
            "2021-11-23 23:16:00,243 - INFO - joeynmt.training - Epoch 475: total training loss 0.36\n",
            "2021-11-23 23:16:00,243 - INFO - joeynmt.training - EPOCH 476\n",
            "2021-11-23 23:16:00,793 - INFO - joeynmt.training - Epoch 476: total training loss 0.36\n",
            "2021-11-23 23:16:00,794 - INFO - joeynmt.training - EPOCH 477\n",
            "2021-11-23 23:16:01,342 - INFO - joeynmt.training - Epoch 477: total training loss 0.33\n",
            "2021-11-23 23:16:01,343 - INFO - joeynmt.training - EPOCH 478\n",
            "2021-11-23 23:16:01,896 - INFO - joeynmt.training - Epoch 478: total training loss 0.35\n",
            "2021-11-23 23:16:01,896 - INFO - joeynmt.training - EPOCH 479\n",
            "2021-11-23 23:16:02,464 - INFO - joeynmt.training - Epoch 479: total training loss 0.38\n",
            "2021-11-23 23:16:02,464 - INFO - joeynmt.training - EPOCH 480\n",
            "2021-11-23 23:16:03,028 - INFO - joeynmt.training - Epoch 480: total training loss 0.38\n",
            "2021-11-23 23:16:03,029 - INFO - joeynmt.training - EPOCH 481\n",
            "2021-11-23 23:16:03,607 - INFO - joeynmt.training - Epoch 481: total training loss 0.37\n",
            "2021-11-23 23:16:03,607 - INFO - joeynmt.training - EPOCH 482\n",
            "2021-11-23 23:16:04,195 - INFO - joeynmt.training - Epoch 482: total training loss 0.38\n",
            "2021-11-23 23:16:04,196 - INFO - joeynmt.training - EPOCH 483\n",
            "2021-11-23 23:16:04,752 - INFO - joeynmt.training - Epoch 483: total training loss 0.36\n",
            "2021-11-23 23:16:04,753 - INFO - joeynmt.training - EPOCH 484\n",
            "2021-11-23 23:16:05,315 - INFO - joeynmt.training - Epoch 484: total training loss 0.37\n",
            "2021-11-23 23:16:05,315 - INFO - joeynmt.training - EPOCH 485\n",
            "2021-11-23 23:16:05,885 - INFO - joeynmt.training - Epoch 485: total training loss 0.36\n",
            "2021-11-23 23:16:05,886 - INFO - joeynmt.training - EPOCH 486\n",
            "2021-11-23 23:16:06,458 - INFO - joeynmt.training - Epoch 486: total training loss 0.36\n",
            "2021-11-23 23:16:06,459 - INFO - joeynmt.training - EPOCH 487\n",
            "2021-11-23 23:16:07,018 - INFO - joeynmt.training - Epoch 487: total training loss 0.33\n",
            "2021-11-23 23:16:07,018 - INFO - joeynmt.training - EPOCH 488\n",
            "2021-11-23 23:16:07,578 - INFO - joeynmt.training - Epoch 488: total training loss 0.34\n",
            "2021-11-23 23:16:07,579 - INFO - joeynmt.training - EPOCH 489\n",
            "2021-11-23 23:16:08,151 - INFO - joeynmt.training - Epoch 489: total training loss 0.33\n",
            "2021-11-23 23:16:08,151 - INFO - joeynmt.training - EPOCH 490\n",
            "2021-11-23 23:16:08,722 - INFO - joeynmt.training - Epoch 490: total training loss 0.36\n",
            "2021-11-23 23:16:08,722 - INFO - joeynmt.training - EPOCH 491\n",
            "2021-11-23 23:16:09,288 - INFO - joeynmt.training - Epoch 491: total training loss 0.34\n",
            "2021-11-23 23:16:09,289 - INFO - joeynmt.training - EPOCH 492\n",
            "2021-11-23 23:16:09,852 - INFO - joeynmt.training - Epoch 492: total training loss 0.34\n",
            "2021-11-23 23:16:09,853 - INFO - joeynmt.training - EPOCH 493\n",
            "2021-11-23 23:16:10,428 - INFO - joeynmt.training - Epoch 493: total training loss 0.34\n",
            "2021-11-23 23:16:10,429 - INFO - joeynmt.training - EPOCH 494\n",
            "2021-11-23 23:16:10,999 - INFO - joeynmt.training - Epoch 494: total training loss 0.34\n",
            "2021-11-23 23:16:11,000 - INFO - joeynmt.training - EPOCH 495\n",
            "2021-11-23 23:16:11,558 - INFO - joeynmt.training - Epoch 495: total training loss 0.36\n",
            "2021-11-23 23:16:11,559 - INFO - joeynmt.training - EPOCH 496\n",
            "2021-11-23 23:16:12,129 - INFO - joeynmt.training - Epoch 496: total training loss 0.35\n",
            "2021-11-23 23:16:12,130 - INFO - joeynmt.training - EPOCH 497\n",
            "2021-11-23 23:16:12,690 - INFO - joeynmt.training - Epoch 497: total training loss 0.35\n",
            "2021-11-23 23:16:12,690 - INFO - joeynmt.training - EPOCH 498\n",
            "2021-11-23 23:16:13,255 - INFO - joeynmt.training - Epoch 498: total training loss 0.33\n",
            "2021-11-23 23:16:13,256 - INFO - joeynmt.training - EPOCH 499\n",
            "2021-11-23 23:16:13,824 - INFO - joeynmt.training - Epoch 499: total training loss 0.34\n",
            "2021-11-23 23:16:13,824 - INFO - joeynmt.training - EPOCH 500\n",
            "2021-11-23 23:16:14,392 - INFO - joeynmt.training - Epoch 500, Step:     1000, Batch Loss:     0.166359, Tokens per Sec:     6689, Lr: 0.000300\n",
            "2021-11-23 23:18:50,735 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-11-23 23:18:51,139 - INFO - joeynmt.training - Example #0\n",
            "2021-11-23 23:18:51,140 - INFO - joeynmt.training - \tSource:     ▁ T h e ▁ r e s e a r c h ▁ b y ▁ t h e ▁ U n i v e r s i t y ▁ o f ▁ E d i n b u r g h ▁ o v e r ▁ a ▁ 3 0 ▁ y e a r ▁ l o o k e d ▁ a t ▁ t h e ▁ s p r e a d ▁ o f ▁ B o v i n e ▁ T u b e r c u l o s i s ▁ ( T B ) ▁ w i t h i n ▁ c o w s ▁ a n d ▁ b a d g e r ▁ p o p u l a t i o n s ▁ i n ▁ G l o u c e s t e r s h i r e .\n",
            "2021-11-23 23:18:51,140 - INFO - joeynmt.training - \tReference:  ▁ U c w a n i n g o ▁ l w e N y u v e s i ▁ y a s e - E d i n b u r g h ▁ o l u t h a t h e ▁ i m i n y a k a ▁ e n g a m a - 3 0 ▁ l u b h e k e ▁ u k u s a b a l a l a ▁ k w e - B o v i n e ▁ T u b e r c u l o s i s ▁ ( i - T B ) ▁ e z i n k o m e n i ▁ n a k u m a - b a d g e r ▁ e G l o u e s t e r s h i r e .\n",
            "2021-11-23 23:18:51,140 - INFO - joeynmt.training - \tHypothesis: ▁<unk>L<unk>o<unk>m<unk>p<unk>e<unk>l<unk>a<unk>n<unk>g<unk>e<unk>▁<unk>e<unk>n<unk>g<unk>e<unk>n<unk>a<unk>z<unk>i<unk>▁<unk>k<unk>w<unk>a<unk>t<unk>o<unk>.\n",
            "2021-11-23 23:18:51,140 - INFO - joeynmt.training - Example #1\n",
            "2021-11-23 23:18:51,140 - INFO - joeynmt.training - \tSource:     ▁ W h e n ▁ I ▁ b o o k e d ▁ a ▁ t h r e e - d a y ▁ s i n g i n g ▁ r e t r e a t ▁ a t ▁ P e t e r ▁ E v a n s ' s ▁ f a r m h o u s e ▁ i n ▁ r u r a l ▁ F r a n c e , ▁ I ▁ h a d ▁ b e e n ▁ a p p r e h e n s i v e ▁ a b o u t ▁ d i s p l a y i n g ▁ m y ▁ p i t i f u l ▁ a b i l i t i e s ▁ t o ▁ s o m e ▁ i n t i m i d a t i n g ▁ O x f o r d ▁ c h o r a l ▁ s c h o l a r ▁ t y p e . ▁ B u t ▁ i n s t e a d ▁ I ▁ f o u n d ▁ m y s e l f ▁ w a r b l i n g ▁ t o ▁ a ▁ s k i n n y ▁ f o r m e r ▁ p u n k ▁ g u i t a r i s t ▁ w i t h ▁ a ▁ s t r o n g ▁ \" s a r f \" ▁ L o n d o n ▁ a c c e n t ▁ a n d ▁ a n ▁ a r r a y ▁ o f ▁ t r u l y ▁ t e r r i b l e ▁ j o k e s .\n",
            "2021-11-23 23:18:51,141 - INFO - joeynmt.training - \tReference:  ▁ N g e n k a t h i ▁ n g i b e k i s e l a ▁ u h l e l o ▁ l w e z i n s u k u ▁ e z i n t a t h u ▁ l o k u v u s e l e l a ▁ i k h o n o ▁ l o m c u l o ▁ e n d l i n i ▁ y a s e p l a z i n i ▁ k a P e t e r ▁ E v a n ▁ e n d a w e n i ▁ e s e m a k h a y a ▁ e F r a n c e , ▁ n g a n g i n o k u z i b a m b a ▁ m a q o n d a n a ▁ n o k u v e z a ▁ a m a k h o n o ▁ a m i ▁ a l u s i z i ▁ k u h l o b o ▁ l w a b a c u l i ▁ b e k w a y a ▁ b a s e - O x f o r d . ▁ K o d w a ▁ e s i k h u n d l e n i ▁ s a l o k h o ▁ n g i z i t h o l e ▁ n g i c u l e l a ▁ p h a n s i ▁ k u k h a l a ▁ i s i g i n g c i ▁ s i s h o ▁ i c u l o ▁ n g o k u g c i z e l e l a ▁ i n d l e l a ▁ y o k u k h u l u m a ▁ y a s e L o n d o n ▁ k u h a m b i s a n a ▁ n a m a h l a y a ▁ a m a b i ▁ k a k h u l u .\n",
            "2021-11-23 23:18:51,141 - INFO - joeynmt.training - \tHypothesis: ▁<unk>U<unk>k<unk>u<unk>k<unk>h<unk>o<unk>n<unk>g<unk>e<unk>n<unk>a<unk>m<unk>a<unk>n<unk>g<unk>u<unk>n<unk>g<unk>e<unk>▁<unk>k<unk>w<unk>e<unk>▁<unk>n<unk>j<unk>o<unk>▁<unk>k<unk>w<unk>e<unk>f<unk>a<unk>n<unk>a<unk>.\n",
            "2021-11-23 23:18:51,141 - INFO - joeynmt.training - Example #2\n",
            "2021-11-23 23:18:51,141 - INFO - joeynmt.training - \tSource:     ▁ I n ▁ 2 0 1 2 , ▁ f o r ▁ e x a m p l e , ▁ m u l t i p l e ▁ g u e s t s ▁ w e r e ▁ i n j u r e d ▁ w h e n ▁ t h e i r ▁ t o u r ▁ b u s ▁ c r a s h e d ▁ i n ▁ S t . ▁ M a r t i n .\n",
            "2021-11-23 23:18:51,141 - INFO - joeynmt.training - \tReference:  ▁ N g o w e z i - 2 0 1 2 , ▁ i s i b o n e l o , ▁ k w a l i m a l a ▁ i n q w a b a ▁ y e z i v a k a s h i ▁ l a p h o ▁ i b h a s i ▁ l a b o ▁ l o k u v a k a s h a ▁ l i s h a y i s a ▁ e - S t . ▁ M a r t i n .\n",
            "2021-11-23 23:18:51,142 - INFO - joeynmt.training - \tHypothesis: ▁<unk>U<unk>k<unk>u<unk>p<unk>h<unk>u<unk>n<unk>o<unk>m<unk>a<unk>▁<unk>e<unk>k<unk>w<unk>e<unk>n<unk>o<unk>m<unk>b<unk>i<unk>n<unk>g<unk>a<unk>l<unk>i<unk>.\n",
            "2021-11-23 23:18:51,142 - INFO - joeynmt.training - Example #3\n",
            "2021-11-23 23:18:51,142 - INFO - joeynmt.training - \tSource:     ▁ \" W e ▁ w e r e ▁ r e a l l y ▁ g o o d ▁ t h a t ▁ t o u r n a m e n t ▁ r i g h t ▁ t h e ▁ w a y ▁ t h r o u g h ▁ t o ▁ t h e ▁ f i n a l . ▁ I ▁ d o n ' t ▁ t h i n k ▁ i t ▁ m o t i v a t e s ▁ o r ▁ c a r r i e s ▁ m e ▁ t o ▁ d o ▁ a n y ▁ b e t t e r , \" ▁ G e n i a ▁ s a i d . ▁ \" I t ' s ▁ j u s t ▁ e m b r a c i n g ▁ i t ▁ a n d ▁ e n j o y i n g ▁ i t . ▁ I t ' s ▁ o n c e ▁ e v e r y ▁ f o u r ▁ y e a r s . ▁ I t ' s ▁ a ▁ d i f f e r e n t ▁ k i n d ▁ o f ▁ b u z z ▁ a n d ▁ a ▁ d i f f e r e n t ▁ k i n d ▁ o f ▁ e n e r g y ▁ w h e n ▁ y o u ▁ a r e ▁ h e r e ▁ w h e n ▁ y o u ▁ a r e ▁ p l a y i n g ▁ a t ▁ a ▁ W o r l d ▁ C u p ▁ b e c a u s e ▁ a l l ▁ t h e ▁ a t t e n t i o n ▁ i s ▁ o n ▁ r u g b y . \"\n",
            "2021-11-23 23:18:51,142 - INFO - joeynmt.training - \tReference:  ▁ \" S i d l a l e ▁ k a h l e ▁ k a k h u l u ▁ k u l o w o ▁ m q h u d e l w a n o ▁ z i b e k w a ▁ n j e ▁ k w a z e ▁ k w a f i k w a ▁ e k u g c i n e n i . ▁ A n g i b o n i ▁ u k u t h i ▁ k u y a k h u t h a z a ▁ n o m a ▁ k u n g i q h u b a ▁ k a n g c o n o , \" ▁ k u s h o ▁ u G e n i a . ▁ \" W u k u k w a m u k e l a ▁ n j e ▁ n o k u k u j a b u l e l a . ▁ K u b a ▁ n j a l o ▁ e m i n y a k e n i ▁ e m i n e . ▁ W u h l o b o ▁ o l u h l u k i l e ▁ l o m s i n d o ▁ f u t h i ▁ u h l o b o ▁ o l u h l u k i l e ▁ l w a m a n d l a ▁ u m a ▁ u l a p h a ▁ u m a ▁ u d l a l a ▁ k w i N d e b e ▁ Y o m h l a b a ▁ n g o b a ▁ k u n a k w a ▁ k a k h u l u ▁ u m d l a l o ▁ w e b h o l a ▁ l o m b h o x o . \"\n",
            "2021-11-23 23:18:51,142 - INFO - joeynmt.training - \tHypothesis: ▁<unk>U<unk>n<unk>g<unk>a<unk>k<unk>u<unk>s<unk>e<unk>:<unk>▁<unk>a<unk>m<unk>b<unk>e<unk>n<unk>g<unk>u<unk>o<unk>▁<unk>k<unk>w<unk>e<unk>n<unk>t<unk>o<unk>z<unk>e<unk>▁<unk>k<unk>u<unk>h<unk>l<unk>e<unk>a<unk>▁<unk>n<unk>g<unk>o<unk>k<unk>u<unk>p<unk>h<unk>e<unk>l<unk>.\n",
            "2021-11-23 23:18:51,143 - INFO - joeynmt.training - Validation result (greedy) at epoch 500, step     1000: bleu:   0.00, loss: 573333.1875, ppl:  24.2069, duration: 156.7506s\n",
            "2021-11-23 23:18:51,146 - INFO - joeynmt.training - Epoch 500: total training loss 0.33\n",
            "2021-11-23 23:18:51,146 - INFO - joeynmt.training - EPOCH 501\n",
            "2021-11-23 23:18:51,725 - INFO - joeynmt.training - Epoch 501: total training loss 0.34\n",
            "2021-11-23 23:18:51,725 - INFO - joeynmt.training - EPOCH 502\n",
            "2021-11-23 23:18:52,286 - INFO - joeynmt.training - Epoch 502: total training loss 0.34\n",
            "2021-11-23 23:18:52,287 - INFO - joeynmt.training - EPOCH 503\n",
            "2021-11-23 23:18:52,856 - INFO - joeynmt.training - Epoch 503: total training loss 0.33\n",
            "2021-11-23 23:18:52,856 - INFO - joeynmt.training - EPOCH 504\n",
            "2021-11-23 23:18:53,423 - INFO - joeynmt.training - Epoch 504: total training loss 0.32\n",
            "2021-11-23 23:18:53,424 - INFO - joeynmt.training - EPOCH 505\n",
            "2021-11-23 23:18:53,990 - INFO - joeynmt.training - Epoch 505: total training loss 0.32\n",
            "2021-11-23 23:18:53,991 - INFO - joeynmt.training - EPOCH 506\n",
            "2021-11-23 23:18:54,556 - INFO - joeynmt.training - Epoch 506: total training loss 0.34\n",
            "2021-11-23 23:18:54,557 - INFO - joeynmt.training - EPOCH 507\n",
            "2021-11-23 23:18:55,133 - INFO - joeynmt.training - Epoch 507: total training loss 0.34\n",
            "2021-11-23 23:18:55,133 - INFO - joeynmt.training - EPOCH 508\n",
            "2021-11-23 23:18:55,701 - INFO - joeynmt.training - Epoch 508: total training loss 0.33\n",
            "2021-11-23 23:18:55,701 - INFO - joeynmt.training - EPOCH 509\n",
            "2021-11-23 23:18:56,273 - INFO - joeynmt.training - Epoch 509: total training loss 0.34\n",
            "2021-11-23 23:18:56,273 - INFO - joeynmt.training - EPOCH 510\n",
            "2021-11-23 23:18:56,845 - INFO - joeynmt.training - Epoch 510: total training loss 0.32\n",
            "2021-11-23 23:18:56,845 - INFO - joeynmt.training - EPOCH 511\n",
            "2021-11-23 23:18:57,399 - INFO - joeynmt.training - Epoch 511: total training loss 0.33\n",
            "2021-11-23 23:18:57,399 - INFO - joeynmt.training - EPOCH 512\n",
            "2021-11-23 23:18:57,959 - INFO - joeynmt.training - Epoch 512: total training loss 0.34\n",
            "2021-11-23 23:18:57,960 - INFO - joeynmt.training - EPOCH 513\n",
            "2021-11-23 23:18:58,526 - INFO - joeynmt.training - Epoch 513: total training loss 0.32\n",
            "2021-11-23 23:18:58,526 - INFO - joeynmt.training - EPOCH 514\n",
            "2021-11-23 23:18:59,089 - INFO - joeynmt.training - Epoch 514: total training loss 0.31\n",
            "2021-11-23 23:18:59,089 - INFO - joeynmt.training - EPOCH 515\n",
            "2021-11-23 23:18:59,663 - INFO - joeynmt.training - Epoch 515: total training loss 0.31\n",
            "2021-11-23 23:18:59,664 - INFO - joeynmt.training - EPOCH 516\n",
            "2021-11-23 23:19:00,241 - INFO - joeynmt.training - Epoch 516: total training loss 0.29\n",
            "2021-11-23 23:19:00,241 - INFO - joeynmt.training - EPOCH 517\n",
            "2021-11-23 23:19:00,809 - INFO - joeynmt.training - Epoch 517: total training loss 0.31\n",
            "2021-11-23 23:19:00,809 - INFO - joeynmt.training - EPOCH 518\n",
            "2021-11-23 23:19:01,381 - INFO - joeynmt.training - Epoch 518: total training loss 0.28\n",
            "2021-11-23 23:19:01,382 - INFO - joeynmt.training - EPOCH 519\n",
            "2021-11-23 23:19:01,933 - INFO - joeynmt.training - Epoch 519: total training loss 0.31\n",
            "2021-11-23 23:19:01,934 - INFO - joeynmt.training - EPOCH 520\n",
            "2021-11-23 23:19:02,497 - INFO - joeynmt.training - Epoch 520: total training loss 0.30\n",
            "2021-11-23 23:19:02,497 - INFO - joeynmt.training - EPOCH 521\n",
            "2021-11-23 23:19:03,079 - INFO - joeynmt.training - Epoch 521: total training loss 0.31\n",
            "2021-11-23 23:19:03,079 - INFO - joeynmt.training - EPOCH 522\n",
            "2021-11-23 23:19:03,646 - INFO - joeynmt.training - Epoch 522: total training loss 0.30\n",
            "2021-11-23 23:19:03,647 - INFO - joeynmt.training - EPOCH 523\n",
            "2021-11-23 23:19:04,209 - INFO - joeynmt.training - Epoch 523: total training loss 0.32\n",
            "2021-11-23 23:19:04,209 - INFO - joeynmt.training - EPOCH 524\n",
            "2021-11-23 23:19:04,772 - INFO - joeynmt.training - Epoch 524: total training loss 0.31\n",
            "2021-11-23 23:19:04,772 - INFO - joeynmt.training - EPOCH 525\n",
            "2021-11-23 23:19:05,335 - INFO - joeynmt.training - Epoch 525: total training loss 0.30\n",
            "2021-11-23 23:19:05,335 - INFO - joeynmt.training - EPOCH 526\n",
            "2021-11-23 23:19:05,907 - INFO - joeynmt.training - Epoch 526: total training loss 0.30\n",
            "2021-11-23 23:19:05,907 - INFO - joeynmt.training - EPOCH 527\n",
            "2021-11-23 23:19:06,484 - INFO - joeynmt.training - Epoch 527: total training loss 0.29\n",
            "2021-11-23 23:19:06,484 - INFO - joeynmt.training - EPOCH 528\n",
            "2021-11-23 23:19:07,062 - INFO - joeynmt.training - Epoch 528: total training loss 0.28\n",
            "2021-11-23 23:19:07,062 - INFO - joeynmt.training - EPOCH 529\n",
            "2021-11-23 23:19:07,631 - INFO - joeynmt.training - Epoch 529: total training loss 0.30\n",
            "2021-11-23 23:19:07,631 - INFO - joeynmt.training - EPOCH 530\n",
            "2021-11-23 23:19:08,198 - INFO - joeynmt.training - Epoch 530: total training loss 0.32\n",
            "2021-11-23 23:19:08,198 - INFO - joeynmt.training - EPOCH 531\n",
            "2021-11-23 23:19:08,773 - INFO - joeynmt.training - Epoch 531: total training loss 0.29\n",
            "2021-11-23 23:19:08,774 - INFO - joeynmt.training - EPOCH 532\n",
            "2021-11-23 23:19:09,346 - INFO - joeynmt.training - Epoch 532: total training loss 0.29\n",
            "2021-11-23 23:19:09,347 - INFO - joeynmt.training - EPOCH 533\n",
            "2021-11-23 23:19:09,916 - INFO - joeynmt.training - Epoch 533: total training loss 0.29\n",
            "2021-11-23 23:19:09,917 - INFO - joeynmt.training - EPOCH 534\n",
            "2021-11-23 23:19:10,483 - INFO - joeynmt.training - Epoch 534: total training loss 0.30\n",
            "2021-11-23 23:19:10,483 - INFO - joeynmt.training - EPOCH 535\n",
            "2021-11-23 23:19:11,048 - INFO - joeynmt.training - Epoch 535: total training loss 0.28\n",
            "2021-11-23 23:19:11,048 - INFO - joeynmt.training - EPOCH 536\n",
            "2021-11-23 23:19:11,619 - INFO - joeynmt.training - Epoch 536: total training loss 0.29\n",
            "2021-11-23 23:19:11,620 - INFO - joeynmt.training - EPOCH 537\n",
            "2021-11-23 23:19:12,198 - INFO - joeynmt.training - Epoch 537: total training loss 0.30\n",
            "2021-11-23 23:19:12,199 - INFO - joeynmt.training - EPOCH 538\n",
            "2021-11-23 23:19:12,761 - INFO - joeynmt.training - Epoch 538: total training loss 0.28\n",
            "2021-11-23 23:19:12,761 - INFO - joeynmt.training - EPOCH 539\n",
            "2021-11-23 23:19:13,323 - INFO - joeynmt.training - Epoch 539: total training loss 0.29\n",
            "2021-11-23 23:19:13,324 - INFO - joeynmt.training - EPOCH 540\n",
            "2021-11-23 23:19:13,881 - INFO - joeynmt.training - Epoch 540: total training loss 0.29\n",
            "2021-11-23 23:19:13,881 - INFO - joeynmt.training - EPOCH 541\n",
            "2021-11-23 23:19:14,456 - INFO - joeynmt.training - Epoch 541: total training loss 0.28\n",
            "2021-11-23 23:19:14,457 - INFO - joeynmt.training - EPOCH 542\n",
            "2021-11-23 23:19:15,026 - INFO - joeynmt.training - Epoch 542: total training loss 0.28\n",
            "2021-11-23 23:19:15,026 - INFO - joeynmt.training - EPOCH 543\n",
            "2021-11-23 23:19:15,590 - INFO - joeynmt.training - Epoch 543: total training loss 0.30\n",
            "2021-11-23 23:19:15,591 - INFO - joeynmt.training - EPOCH 544\n",
            "2021-11-23 23:19:16,150 - INFO - joeynmt.training - Epoch 544: total training loss 0.29\n",
            "2021-11-23 23:19:16,151 - INFO - joeynmt.training - EPOCH 545\n",
            "2021-11-23 23:19:16,721 - INFO - joeynmt.training - Epoch 545: total training loss 0.29\n",
            "2021-11-23 23:19:16,721 - INFO - joeynmt.training - EPOCH 546\n",
            "2021-11-23 23:19:17,301 - INFO - joeynmt.training - Epoch 546: total training loss 0.26\n",
            "2021-11-23 23:19:17,302 - INFO - joeynmt.training - EPOCH 547\n",
            "2021-11-23 23:19:17,860 - INFO - joeynmt.training - Epoch 547: total training loss 0.28\n",
            "2021-11-23 23:19:17,860 - INFO - joeynmt.training - EPOCH 548\n",
            "2021-11-23 23:19:18,429 - INFO - joeynmt.training - Epoch 548: total training loss 0.28\n",
            "2021-11-23 23:19:18,430 - INFO - joeynmt.training - EPOCH 549\n",
            "2021-11-23 23:19:18,997 - INFO - joeynmt.training - Epoch 549: total training loss 0.28\n",
            "2021-11-23 23:19:18,997 - INFO - joeynmt.training - EPOCH 550\n",
            "2021-11-23 23:19:19,567 - INFO - joeynmt.training - Epoch 550, Step:     1100, Batch Loss:     0.125562, Tokens per Sec:     6621, Lr: 0.000300\n",
            "2021-11-23 23:19:19,568 - INFO - joeynmt.training - Epoch 550: total training loss 0.26\n",
            "2021-11-23 23:19:19,568 - INFO - joeynmt.training - EPOCH 551\n",
            "2021-11-23 23:19:20,140 - INFO - joeynmt.training - Epoch 551: total training loss 0.27\n",
            "2021-11-23 23:19:20,141 - INFO - joeynmt.training - EPOCH 552\n",
            "2021-11-23 23:19:20,711 - INFO - joeynmt.training - Epoch 552: total training loss 0.29\n",
            "2021-11-23 23:19:20,711 - INFO - joeynmt.training - EPOCH 553\n",
            "2021-11-23 23:19:21,285 - INFO - joeynmt.training - Epoch 553: total training loss 0.26\n",
            "2021-11-23 23:19:21,285 - INFO - joeynmt.training - EPOCH 554\n",
            "2021-11-23 23:19:21,864 - INFO - joeynmt.training - Epoch 554: total training loss 0.28\n",
            "2021-11-23 23:19:21,864 - INFO - joeynmt.training - EPOCH 555\n",
            "2021-11-23 23:19:22,435 - INFO - joeynmt.training - Epoch 555: total training loss 0.26\n",
            "2021-11-23 23:19:22,435 - INFO - joeynmt.training - EPOCH 556\n",
            "2021-11-23 23:19:23,006 - INFO - joeynmt.training - Epoch 556: total training loss 0.29\n",
            "2021-11-23 23:19:23,007 - INFO - joeynmt.training - EPOCH 557\n",
            "2021-11-23 23:19:23,582 - INFO - joeynmt.training - Epoch 557: total training loss 0.27\n",
            "2021-11-23 23:19:23,583 - INFO - joeynmt.training - EPOCH 558\n",
            "2021-11-23 23:19:24,153 - INFO - joeynmt.training - Epoch 558: total training loss 0.28\n",
            "2021-11-23 23:19:24,153 - INFO - joeynmt.training - EPOCH 559\n",
            "2021-11-23 23:19:24,716 - INFO - joeynmt.training - Epoch 559: total training loss 0.26\n",
            "2021-11-23 23:19:24,716 - INFO - joeynmt.training - EPOCH 560\n",
            "2021-11-23 23:19:25,280 - INFO - joeynmt.training - Epoch 560: total training loss 0.27\n",
            "2021-11-23 23:19:25,280 - INFO - joeynmt.training - EPOCH 561\n",
            "2021-11-23 23:19:25,845 - INFO - joeynmt.training - Epoch 561: total training loss 0.26\n",
            "2021-11-23 23:19:25,845 - INFO - joeynmt.training - EPOCH 562\n",
            "2021-11-23 23:19:26,423 - INFO - joeynmt.training - Epoch 562: total training loss 0.25\n",
            "2021-11-23 23:19:26,424 - INFO - joeynmt.training - EPOCH 563\n",
            "2021-11-23 23:19:26,986 - INFO - joeynmt.training - Epoch 563: total training loss 0.25\n",
            "2021-11-23 23:19:26,986 - INFO - joeynmt.training - EPOCH 564\n",
            "2021-11-23 23:19:27,553 - INFO - joeynmt.training - Epoch 564: total training loss 0.26\n",
            "2021-11-23 23:19:27,554 - INFO - joeynmt.training - EPOCH 565\n",
            "2021-11-23 23:19:28,118 - INFO - joeynmt.training - Epoch 565: total training loss 0.26\n",
            "2021-11-23 23:19:28,118 - INFO - joeynmt.training - EPOCH 566\n",
            "2021-11-23 23:19:28,688 - INFO - joeynmt.training - Epoch 566: total training loss 0.26\n",
            "2021-11-23 23:19:28,688 - INFO - joeynmt.training - EPOCH 567\n",
            "2021-11-23 23:19:29,257 - INFO - joeynmt.training - Epoch 567: total training loss 0.28\n",
            "2021-11-23 23:19:29,258 - INFO - joeynmt.training - EPOCH 568\n",
            "2021-11-23 23:19:29,824 - INFO - joeynmt.training - Epoch 568: total training loss 0.25\n",
            "2021-11-23 23:19:29,825 - INFO - joeynmt.training - EPOCH 569\n",
            "2021-11-23 23:19:30,396 - INFO - joeynmt.training - Epoch 569: total training loss 0.26\n",
            "2021-11-23 23:19:30,397 - INFO - joeynmt.training - EPOCH 570\n",
            "2021-11-23 23:19:30,951 - INFO - joeynmt.training - Epoch 570: total training loss 0.25\n",
            "2021-11-23 23:19:30,951 - INFO - joeynmt.training - EPOCH 571\n",
            "2021-11-23 23:19:31,527 - INFO - joeynmt.training - Epoch 571: total training loss 0.25\n",
            "2021-11-23 23:19:31,527 - INFO - joeynmt.training - EPOCH 572\n",
            "2021-11-23 23:19:32,092 - INFO - joeynmt.training - Epoch 572: total training loss 0.26\n",
            "2021-11-23 23:19:32,092 - INFO - joeynmt.training - EPOCH 573\n",
            "2021-11-23 23:19:32,658 - INFO - joeynmt.training - Epoch 573: total training loss 0.26\n",
            "2021-11-23 23:19:32,659 - INFO - joeynmt.training - EPOCH 574\n",
            "2021-11-23 23:19:33,232 - INFO - joeynmt.training - Epoch 574: total training loss 0.24\n",
            "2021-11-23 23:19:33,232 - INFO - joeynmt.training - EPOCH 575\n",
            "2021-11-23 23:19:33,800 - INFO - joeynmt.training - Epoch 575: total training loss 0.24\n",
            "2021-11-23 23:19:33,801 - INFO - joeynmt.training - EPOCH 576\n",
            "2021-11-23 23:19:34,362 - INFO - joeynmt.training - Epoch 576: total training loss 0.25\n",
            "2021-11-23 23:19:34,362 - INFO - joeynmt.training - EPOCH 577\n",
            "2021-11-23 23:19:34,931 - INFO - joeynmt.training - Epoch 577: total training loss 0.26\n",
            "2021-11-23 23:19:34,932 - INFO - joeynmt.training - EPOCH 578\n",
            "2021-11-23 23:19:35,499 - INFO - joeynmt.training - Epoch 578: total training loss 0.24\n",
            "2021-11-23 23:19:35,499 - INFO - joeynmt.training - EPOCH 579\n",
            "2021-11-23 23:19:36,068 - INFO - joeynmt.training - Epoch 579: total training loss 0.25\n",
            "2021-11-23 23:19:36,069 - INFO - joeynmt.training - EPOCH 580\n",
            "2021-11-23 23:19:36,640 - INFO - joeynmt.training - Epoch 580: total training loss 0.23\n",
            "2021-11-23 23:19:36,640 - INFO - joeynmt.training - EPOCH 581\n",
            "2021-11-23 23:19:37,225 - INFO - joeynmt.training - Epoch 581: total training loss 0.23\n",
            "2021-11-23 23:19:37,225 - INFO - joeynmt.training - EPOCH 582\n",
            "2021-11-23 23:19:37,801 - INFO - joeynmt.training - Epoch 582: total training loss 0.27\n",
            "2021-11-23 23:19:37,802 - INFO - joeynmt.training - EPOCH 583\n",
            "2021-11-23 23:19:38,361 - INFO - joeynmt.training - Epoch 583: total training loss 0.24\n",
            "2021-11-23 23:19:38,362 - INFO - joeynmt.training - EPOCH 584\n",
            "2021-11-23 23:19:38,924 - INFO - joeynmt.training - Epoch 584: total training loss 0.27\n",
            "2021-11-23 23:19:38,925 - INFO - joeynmt.training - EPOCH 585\n",
            "2021-11-23 23:19:39,494 - INFO - joeynmt.training - Epoch 585: total training loss 0.25\n",
            "2021-11-23 23:19:39,494 - INFO - joeynmt.training - EPOCH 586\n",
            "2021-11-23 23:19:40,061 - INFO - joeynmt.training - Epoch 586: total training loss 0.24\n",
            "2021-11-23 23:19:40,061 - INFO - joeynmt.training - EPOCH 587\n",
            "2021-11-23 23:19:40,635 - INFO - joeynmt.training - Epoch 587: total training loss 0.24\n",
            "2021-11-23 23:19:40,636 - INFO - joeynmt.training - EPOCH 588\n",
            "2021-11-23 23:19:41,202 - INFO - joeynmt.training - Epoch 588: total training loss 0.25\n",
            "2021-11-23 23:19:41,202 - INFO - joeynmt.training - EPOCH 589\n",
            "2021-11-23 23:19:41,782 - INFO - joeynmt.training - Epoch 589: total training loss 0.24\n",
            "2021-11-23 23:19:41,782 - INFO - joeynmt.training - EPOCH 590\n",
            "2021-11-23 23:19:42,356 - INFO - joeynmt.training - Epoch 590: total training loss 0.24\n",
            "2021-11-23 23:19:42,356 - INFO - joeynmt.training - EPOCH 591\n",
            "2021-11-23 23:19:42,929 - INFO - joeynmt.training - Epoch 591: total training loss 0.24\n",
            "2021-11-23 23:19:42,930 - INFO - joeynmt.training - EPOCH 592\n",
            "2021-11-23 23:19:43,491 - INFO - joeynmt.training - Epoch 592: total training loss 0.22\n",
            "2021-11-23 23:19:43,491 - INFO - joeynmt.training - EPOCH 593\n",
            "2021-11-23 23:19:44,075 - INFO - joeynmt.training - Epoch 593: total training loss 0.24\n",
            "2021-11-23 23:19:44,075 - INFO - joeynmt.training - EPOCH 594\n",
            "2021-11-23 23:19:44,643 - INFO - joeynmt.training - Epoch 594: total training loss 0.23\n",
            "2021-11-23 23:19:44,643 - INFO - joeynmt.training - EPOCH 595\n",
            "2021-11-23 23:19:45,208 - INFO - joeynmt.training - Epoch 595: total training loss 0.25\n",
            "2021-11-23 23:19:45,208 - INFO - joeynmt.training - EPOCH 596\n",
            "2021-11-23 23:19:45,781 - INFO - joeynmt.training - Epoch 596: total training loss 0.23\n",
            "2021-11-23 23:19:45,781 - INFO - joeynmt.training - EPOCH 597\n",
            "2021-11-23 23:19:46,354 - INFO - joeynmt.training - Epoch 597: total training loss 0.24\n",
            "2021-11-23 23:19:46,354 - INFO - joeynmt.training - EPOCH 598\n",
            "2021-11-23 23:19:46,935 - INFO - joeynmt.training - Epoch 598: total training loss 0.22\n",
            "2021-11-23 23:19:46,935 - INFO - joeynmt.training - EPOCH 599\n",
            "2021-11-23 23:19:47,505 - INFO - joeynmt.training - Epoch 599: total training loss 0.23\n",
            "2021-11-23 23:19:47,505 - INFO - joeynmt.training - EPOCH 600\n",
            "2021-11-23 23:19:48,082 - INFO - joeynmt.training - Epoch 600, Step:     1200, Batch Loss:     0.109271, Tokens per Sec:     6549, Lr: 0.000300\n",
            "2021-11-23 23:19:48,082 - INFO - joeynmt.training - Epoch 600: total training loss 0.24\n",
            "2021-11-23 23:19:48,083 - INFO - joeynmt.training - EPOCH 601\n",
            "2021-11-23 23:19:48,659 - INFO - joeynmt.training - Epoch 601: total training loss 0.23\n",
            "2021-11-23 23:19:48,659 - INFO - joeynmt.training - EPOCH 602\n",
            "2021-11-23 23:19:49,225 - INFO - joeynmt.training - Epoch 602: total training loss 0.23\n",
            "2021-11-23 23:19:49,225 - INFO - joeynmt.training - EPOCH 603\n",
            "2021-11-23 23:19:49,797 - INFO - joeynmt.training - Epoch 603: total training loss 0.24\n",
            "2021-11-23 23:19:49,797 - INFO - joeynmt.training - EPOCH 604\n",
            "2021-11-23 23:19:50,366 - INFO - joeynmt.training - Epoch 604: total training loss 0.24\n",
            "2021-11-23 23:19:50,367 - INFO - joeynmt.training - EPOCH 605\n",
            "2021-11-23 23:19:50,923 - INFO - joeynmt.training - Epoch 605: total training loss 0.22\n",
            "2021-11-23 23:19:50,924 - INFO - joeynmt.training - EPOCH 606\n",
            "2021-11-23 23:19:51,491 - INFO - joeynmt.training - Epoch 606: total training loss 0.24\n",
            "2021-11-23 23:19:51,491 - INFO - joeynmt.training - EPOCH 607\n",
            "2021-11-23 23:19:52,062 - INFO - joeynmt.training - Epoch 607: total training loss 0.23\n",
            "2021-11-23 23:19:52,063 - INFO - joeynmt.training - EPOCH 608\n",
            "2021-11-23 23:19:52,625 - INFO - joeynmt.training - Epoch 608: total training loss 0.21\n",
            "2021-11-23 23:19:52,626 - INFO - joeynmt.training - EPOCH 609\n",
            "2021-11-23 23:19:53,201 - INFO - joeynmt.training - Epoch 609: total training loss 0.23\n",
            "2021-11-23 23:19:53,202 - INFO - joeynmt.training - EPOCH 610\n",
            "2021-11-23 23:19:53,779 - INFO - joeynmt.training - Epoch 610: total training loss 0.22\n",
            "2021-11-23 23:19:53,779 - INFO - joeynmt.training - EPOCH 611\n",
            "2021-11-23 23:19:54,336 - INFO - joeynmt.training - Epoch 611: total training loss 0.22\n",
            "2021-11-23 23:19:54,336 - INFO - joeynmt.training - EPOCH 612\n",
            "2021-11-23 23:19:54,901 - INFO - joeynmt.training - Epoch 612: total training loss 0.22\n",
            "2021-11-23 23:19:54,902 - INFO - joeynmt.training - EPOCH 613\n",
            "2021-11-23 23:19:55,475 - INFO - joeynmt.training - Epoch 613: total training loss 0.25\n",
            "2021-11-23 23:19:55,475 - INFO - joeynmt.training - EPOCH 614\n",
            "2021-11-23 23:19:56,038 - INFO - joeynmt.training - Epoch 614: total training loss 0.21\n",
            "2021-11-23 23:19:56,038 - INFO - joeynmt.training - EPOCH 615\n",
            "2021-11-23 23:19:56,604 - INFO - joeynmt.training - Epoch 615: total training loss 0.22\n",
            "2021-11-23 23:19:56,605 - INFO - joeynmt.training - EPOCH 616\n",
            "2021-11-23 23:19:57,168 - INFO - joeynmt.training - Epoch 616: total training loss 0.22\n",
            "2021-11-23 23:19:57,168 - INFO - joeynmt.training - EPOCH 617\n",
            "2021-11-23 23:19:57,738 - INFO - joeynmt.training - Epoch 617: total training loss 0.22\n",
            "2021-11-23 23:19:57,738 - INFO - joeynmt.training - EPOCH 618\n",
            "2021-11-23 23:19:58,303 - INFO - joeynmt.training - Epoch 618: total training loss 0.22\n",
            "2021-11-23 23:19:58,303 - INFO - joeynmt.training - EPOCH 619\n",
            "2021-11-23 23:19:58,868 - INFO - joeynmt.training - Epoch 619: total training loss 0.22\n",
            "2021-11-23 23:19:58,869 - INFO - joeynmt.training - EPOCH 620\n",
            "2021-11-23 23:19:59,433 - INFO - joeynmt.training - Epoch 620: total training loss 0.22\n",
            "2021-11-23 23:19:59,433 - INFO - joeynmt.training - EPOCH 621\n",
            "2021-11-23 23:19:59,998 - INFO - joeynmt.training - Epoch 621: total training loss 0.23\n",
            "2021-11-23 23:19:59,998 - INFO - joeynmt.training - EPOCH 622\n",
            "2021-11-23 23:20:00,565 - INFO - joeynmt.training - Epoch 622: total training loss 0.21\n",
            "2021-11-23 23:20:00,565 - INFO - joeynmt.training - EPOCH 623\n",
            "2021-11-23 23:20:01,126 - INFO - joeynmt.training - Epoch 623: total training loss 0.23\n",
            "2021-11-23 23:20:01,126 - INFO - joeynmt.training - EPOCH 624\n",
            "2021-11-23 23:20:01,694 - INFO - joeynmt.training - Epoch 624: total training loss 0.21\n",
            "2021-11-23 23:20:01,695 - INFO - joeynmt.training - EPOCH 625\n",
            "2021-11-23 23:20:02,254 - INFO - joeynmt.training - Epoch 625: total training loss 0.21\n",
            "2021-11-23 23:20:02,255 - INFO - joeynmt.training - EPOCH 626\n",
            "2021-11-23 23:20:02,840 - INFO - joeynmt.training - Epoch 626: total training loss 0.23\n",
            "2021-11-23 23:20:02,840 - INFO - joeynmt.training - EPOCH 627\n",
            "2021-11-23 23:20:03,403 - INFO - joeynmt.training - Epoch 627: total training loss 0.22\n",
            "2021-11-23 23:20:03,404 - INFO - joeynmt.training - EPOCH 628\n",
            "2021-11-23 23:20:03,958 - INFO - joeynmt.training - Epoch 628: total training loss 0.21\n",
            "2021-11-23 23:20:03,958 - INFO - joeynmt.training - EPOCH 629\n",
            "2021-11-23 23:20:04,532 - INFO - joeynmt.training - Epoch 629: total training loss 0.21\n",
            "2021-11-23 23:20:04,533 - INFO - joeynmt.training - EPOCH 630\n",
            "2021-11-23 23:20:05,103 - INFO - joeynmt.training - Epoch 630: total training loss 0.21\n",
            "2021-11-23 23:20:05,104 - INFO - joeynmt.training - EPOCH 631\n",
            "2021-11-23 23:20:05,666 - INFO - joeynmt.training - Epoch 631: total training loss 0.21\n",
            "2021-11-23 23:20:05,667 - INFO - joeynmt.training - EPOCH 632\n",
            "2021-11-23 23:20:06,240 - INFO - joeynmt.training - Epoch 632: total training loss 0.21\n",
            "2021-11-23 23:20:06,241 - INFO - joeynmt.training - EPOCH 633\n",
            "2021-11-23 23:20:06,804 - INFO - joeynmt.training - Epoch 633: total training loss 0.21\n",
            "2021-11-23 23:20:06,804 - INFO - joeynmt.training - EPOCH 634\n",
            "2021-11-23 23:20:07,383 - INFO - joeynmt.training - Epoch 634: total training loss 0.21\n",
            "2021-11-23 23:20:07,383 - INFO - joeynmt.training - EPOCH 635\n",
            "2021-11-23 23:20:07,957 - INFO - joeynmt.training - Epoch 635: total training loss 0.20\n",
            "2021-11-23 23:20:07,958 - INFO - joeynmt.training - EPOCH 636\n",
            "2021-11-23 23:20:08,521 - INFO - joeynmt.training - Epoch 636: total training loss 0.21\n",
            "2021-11-23 23:20:08,522 - INFO - joeynmt.training - EPOCH 637\n",
            "2021-11-23 23:20:09,098 - INFO - joeynmt.training - Epoch 637: total training loss 0.22\n",
            "2021-11-23 23:20:09,099 - INFO - joeynmt.training - EPOCH 638\n",
            "2021-11-23 23:20:09,676 - INFO - joeynmt.training - Epoch 638: total training loss 0.21\n",
            "2021-11-23 23:20:09,676 - INFO - joeynmt.training - EPOCH 639\n",
            "2021-11-23 23:20:10,249 - INFO - joeynmt.training - Epoch 639: total training loss 0.21\n",
            "2021-11-23 23:20:10,250 - INFO - joeynmt.training - EPOCH 640\n",
            "2021-11-23 23:20:10,813 - INFO - joeynmt.training - Epoch 640: total training loss 0.21\n",
            "2021-11-23 23:20:10,813 - INFO - joeynmt.training - EPOCH 641\n",
            "2021-11-23 23:20:11,383 - INFO - joeynmt.training - Epoch 641: total training loss 0.20\n",
            "2021-11-23 23:20:11,383 - INFO - joeynmt.training - EPOCH 642\n",
            "2021-11-23 23:20:11,942 - INFO - joeynmt.training - Epoch 642: total training loss 0.21\n",
            "2021-11-23 23:20:11,942 - INFO - joeynmt.training - EPOCH 643\n",
            "2021-11-23 23:20:12,497 - INFO - joeynmt.training - Epoch 643: total training loss 0.21\n",
            "2021-11-23 23:20:12,497 - INFO - joeynmt.training - EPOCH 644\n",
            "2021-11-23 23:20:13,063 - INFO - joeynmt.training - Epoch 644: total training loss 0.20\n",
            "2021-11-23 23:20:13,064 - INFO - joeynmt.training - EPOCH 645\n",
            "2021-11-23 23:20:13,632 - INFO - joeynmt.training - Epoch 645: total training loss 0.21\n",
            "2021-11-23 23:20:13,632 - INFO - joeynmt.training - EPOCH 646\n",
            "2021-11-23 23:20:14,193 - INFO - joeynmt.training - Epoch 646: total training loss 0.20\n",
            "2021-11-23 23:20:14,194 - INFO - joeynmt.training - EPOCH 647\n",
            "2021-11-23 23:20:14,760 - INFO - joeynmt.training - Epoch 647: total training loss 0.20\n",
            "2021-11-23 23:20:14,760 - INFO - joeynmt.training - EPOCH 648\n",
            "2021-11-23 23:20:15,320 - INFO - joeynmt.training - Epoch 648: total training loss 0.20\n",
            "2021-11-23 23:20:15,321 - INFO - joeynmt.training - EPOCH 649\n",
            "2021-11-23 23:20:15,878 - INFO - joeynmt.training - Epoch 649: total training loss 0.22\n",
            "2021-11-23 23:20:15,879 - INFO - joeynmt.training - EPOCH 650\n",
            "2021-11-23 23:20:16,436 - INFO - joeynmt.training - Epoch 650, Step:     1300, Batch Loss:     0.095823, Tokens per Sec:     6777, Lr: 0.000300\n",
            "2021-11-23 23:20:16,436 - INFO - joeynmt.training - Epoch 650: total training loss 0.19\n",
            "2021-11-23 23:20:16,437 - INFO - joeynmt.training - EPOCH 651\n",
            "2021-11-23 23:20:16,995 - INFO - joeynmt.training - Epoch 651: total training loss 0.19\n",
            "2021-11-23 23:20:16,995 - INFO - joeynmt.training - EPOCH 652\n",
            "2021-11-23 23:20:17,566 - INFO - joeynmt.training - Epoch 652: total training loss 0.21\n",
            "2021-11-23 23:20:17,566 - INFO - joeynmt.training - EPOCH 653\n",
            "2021-11-23 23:20:18,130 - INFO - joeynmt.training - Epoch 653: total training loss 0.20\n",
            "2021-11-23 23:20:18,131 - INFO - joeynmt.training - EPOCH 654\n",
            "2021-11-23 23:20:18,703 - INFO - joeynmt.training - Epoch 654: total training loss 0.20\n",
            "2021-11-23 23:20:18,704 - INFO - joeynmt.training - EPOCH 655\n",
            "2021-11-23 23:20:19,282 - INFO - joeynmt.training - Epoch 655: total training loss 0.20\n",
            "2021-11-23 23:20:19,282 - INFO - joeynmt.training - EPOCH 656\n",
            "2021-11-23 23:20:19,851 - INFO - joeynmt.training - Epoch 656: total training loss 0.20\n",
            "2021-11-23 23:20:19,851 - INFO - joeynmt.training - EPOCH 657\n",
            "2021-11-23 23:20:20,418 - INFO - joeynmt.training - Epoch 657: total training loss 0.20\n",
            "2021-11-23 23:20:20,418 - INFO - joeynmt.training - EPOCH 658\n",
            "2021-11-23 23:20:20,990 - INFO - joeynmt.training - Epoch 658: total training loss 0.19\n",
            "2021-11-23 23:20:20,991 - INFO - joeynmt.training - EPOCH 659\n",
            "2021-11-23 23:20:21,554 - INFO - joeynmt.training - Epoch 659: total training loss 0.20\n",
            "2021-11-23 23:20:21,555 - INFO - joeynmt.training - EPOCH 660\n",
            "2021-11-23 23:20:22,130 - INFO - joeynmt.training - Epoch 660: total training loss 0.20\n",
            "2021-11-23 23:20:22,130 - INFO - joeynmt.training - EPOCH 661\n",
            "2021-11-23 23:20:22,702 - INFO - joeynmt.training - Epoch 661: total training loss 0.19\n",
            "2021-11-23 23:20:22,703 - INFO - joeynmt.training - EPOCH 662\n",
            "2021-11-23 23:20:23,275 - INFO - joeynmt.training - Epoch 662: total training loss 0.20\n",
            "2021-11-23 23:20:23,276 - INFO - joeynmt.training - EPOCH 663\n",
            "2021-11-23 23:20:23,839 - INFO - joeynmt.training - Epoch 663: total training loss 0.19\n",
            "2021-11-23 23:20:23,840 - INFO - joeynmt.training - EPOCH 664\n",
            "2021-11-23 23:20:24,416 - INFO - joeynmt.training - Epoch 664: total training loss 0.19\n",
            "2021-11-23 23:20:24,417 - INFO - joeynmt.training - EPOCH 665\n",
            "2021-11-23 23:20:24,979 - INFO - joeynmt.training - Epoch 665: total training loss 0.19\n",
            "2021-11-23 23:20:24,979 - INFO - joeynmt.training - EPOCH 666\n",
            "2021-11-23 23:20:25,547 - INFO - joeynmt.training - Epoch 666: total training loss 0.19\n",
            "2021-11-23 23:20:25,548 - INFO - joeynmt.training - EPOCH 667\n",
            "2021-11-23 23:20:26,111 - INFO - joeynmt.training - Epoch 667: total training loss 0.19\n",
            "2021-11-23 23:20:26,111 - INFO - joeynmt.training - EPOCH 668\n",
            "2021-11-23 23:20:26,681 - INFO - joeynmt.training - Epoch 668: total training loss 0.20\n",
            "2021-11-23 23:20:26,682 - INFO - joeynmt.training - EPOCH 669\n",
            "2021-11-23 23:20:27,252 - INFO - joeynmt.training - Epoch 669: total training loss 0.19\n",
            "2021-11-23 23:20:27,253 - INFO - joeynmt.training - EPOCH 670\n",
            "2021-11-23 23:20:27,827 - INFO - joeynmt.training - Epoch 670: total training loss 0.18\n",
            "2021-11-23 23:20:27,827 - INFO - joeynmt.training - EPOCH 671\n",
            "2021-11-23 23:20:28,398 - INFO - joeynmt.training - Epoch 671: total training loss 0.19\n",
            "2021-11-23 23:20:28,398 - INFO - joeynmt.training - EPOCH 672\n",
            "2021-11-23 23:20:28,963 - INFO - joeynmt.training - Epoch 672: total training loss 0.19\n",
            "2021-11-23 23:20:28,964 - INFO - joeynmt.training - EPOCH 673\n",
            "2021-11-23 23:20:29,537 - INFO - joeynmt.training - Epoch 673: total training loss 0.19\n",
            "2021-11-23 23:20:29,538 - INFO - joeynmt.training - EPOCH 674\n",
            "2021-11-23 23:20:30,104 - INFO - joeynmt.training - Epoch 674: total training loss 0.19\n",
            "2021-11-23 23:20:30,105 - INFO - joeynmt.training - EPOCH 675\n",
            "2021-11-23 23:20:30,670 - INFO - joeynmt.training - Epoch 675: total training loss 0.19\n",
            "2021-11-23 23:20:30,671 - INFO - joeynmt.training - EPOCH 676\n",
            "2021-11-23 23:20:31,239 - INFO - joeynmt.training - Epoch 676: total training loss 0.19\n",
            "2021-11-23 23:20:31,240 - INFO - joeynmt.training - EPOCH 677\n",
            "2021-11-23 23:20:31,808 - INFO - joeynmt.training - Epoch 677: total training loss 0.19\n",
            "2021-11-23 23:20:31,808 - INFO - joeynmt.training - EPOCH 678\n",
            "2021-11-23 23:20:32,370 - INFO - joeynmt.training - Epoch 678: total training loss 0.19\n",
            "2021-11-23 23:20:32,370 - INFO - joeynmt.training - EPOCH 679\n",
            "2021-11-23 23:20:32,942 - INFO - joeynmt.training - Epoch 679: total training loss 0.19\n",
            "2021-11-23 23:20:32,942 - INFO - joeynmt.training - EPOCH 680\n",
            "2021-11-23 23:20:33,506 - INFO - joeynmt.training - Epoch 680: total training loss 0.18\n",
            "2021-11-23 23:20:33,507 - INFO - joeynmt.training - EPOCH 681\n",
            "2021-11-23 23:20:34,086 - INFO - joeynmt.training - Epoch 681: total training loss 0.18\n",
            "2021-11-23 23:20:34,086 - INFO - joeynmt.training - EPOCH 682\n",
            "2021-11-23 23:20:34,671 - INFO - joeynmt.training - Epoch 682: total training loss 0.19\n",
            "2021-11-23 23:20:34,672 - INFO - joeynmt.training - EPOCH 683\n",
            "2021-11-23 23:20:35,241 - INFO - joeynmt.training - Epoch 683: total training loss 0.19\n",
            "2021-11-23 23:20:35,241 - INFO - joeynmt.training - EPOCH 684\n",
            "2021-11-23 23:20:35,809 - INFO - joeynmt.training - Epoch 684: total training loss 0.19\n",
            "2021-11-23 23:20:35,809 - INFO - joeynmt.training - EPOCH 685\n",
            "2021-11-23 23:20:36,377 - INFO - joeynmt.training - Epoch 685: total training loss 0.18\n",
            "2021-11-23 23:20:36,377 - INFO - joeynmt.training - EPOCH 686\n",
            "2021-11-23 23:20:36,940 - INFO - joeynmt.training - Epoch 686: total training loss 0.18\n",
            "2021-11-23 23:20:36,940 - INFO - joeynmt.training - EPOCH 687\n",
            "2021-11-23 23:20:37,515 - INFO - joeynmt.training - Epoch 687: total training loss 0.17\n",
            "2021-11-23 23:20:37,515 - INFO - joeynmt.training - EPOCH 688\n",
            "2021-11-23 23:20:38,082 - INFO - joeynmt.training - Epoch 688: total training loss 0.19\n",
            "2021-11-23 23:20:38,083 - INFO - joeynmt.training - EPOCH 689\n",
            "2021-11-23 23:20:38,649 - INFO - joeynmt.training - Epoch 689: total training loss 0.19\n",
            "2021-11-23 23:20:38,650 - INFO - joeynmt.training - EPOCH 690\n",
            "2021-11-23 23:20:39,222 - INFO - joeynmt.training - Epoch 690: total training loss 0.18\n",
            "2021-11-23 23:20:39,222 - INFO - joeynmt.training - EPOCH 691\n",
            "2021-11-23 23:20:39,783 - INFO - joeynmt.training - Epoch 691: total training loss 0.19\n",
            "2021-11-23 23:20:39,784 - INFO - joeynmt.training - EPOCH 692\n",
            "2021-11-23 23:20:40,353 - INFO - joeynmt.training - Epoch 692: total training loss 0.19\n",
            "2021-11-23 23:20:40,353 - INFO - joeynmt.training - EPOCH 693\n",
            "2021-11-23 23:20:40,917 - INFO - joeynmt.training - Epoch 693: total training loss 0.18\n",
            "2021-11-23 23:20:40,917 - INFO - joeynmt.training - EPOCH 694\n",
            "2021-11-23 23:20:41,488 - INFO - joeynmt.training - Epoch 694: total training loss 0.17\n",
            "2021-11-23 23:20:41,488 - INFO - joeynmt.training - EPOCH 695\n",
            "2021-11-23 23:20:42,054 - INFO - joeynmt.training - Epoch 695: total training loss 0.18\n",
            "2021-11-23 23:20:42,055 - INFO - joeynmt.training - EPOCH 696\n",
            "2021-11-23 23:20:42,638 - INFO - joeynmt.training - Epoch 696: total training loss 0.19\n",
            "2021-11-23 23:20:42,638 - INFO - joeynmt.training - EPOCH 697\n",
            "2021-11-23 23:20:43,205 - INFO - joeynmt.training - Epoch 697: total training loss 0.19\n",
            "2021-11-23 23:20:43,206 - INFO - joeynmt.training - EPOCH 698\n",
            "2021-11-23 23:20:43,787 - INFO - joeynmt.training - Epoch 698: total training loss 0.17\n",
            "2021-11-23 23:20:43,788 - INFO - joeynmt.training - EPOCH 699\n",
            "2021-11-23 23:20:44,352 - INFO - joeynmt.training - Epoch 699: total training loss 0.18\n",
            "2021-11-23 23:20:44,352 - INFO - joeynmt.training - EPOCH 700\n",
            "2021-11-23 23:20:44,928 - INFO - joeynmt.training - Epoch 700, Step:     1400, Batch Loss:     0.082683, Tokens per Sec:     6561, Lr: 0.000300\n",
            "2021-11-23 23:20:44,928 - INFO - joeynmt.training - Epoch 700: total training loss 0.18\n",
            "2021-11-23 23:20:44,929 - INFO - joeynmt.training - EPOCH 701\n",
            "2021-11-23 23:20:45,496 - INFO - joeynmt.training - Epoch 701: total training loss 0.19\n",
            "2021-11-23 23:20:45,497 - INFO - joeynmt.training - EPOCH 702\n",
            "2021-11-23 23:20:46,060 - INFO - joeynmt.training - Epoch 702: total training loss 0.18\n",
            "2021-11-23 23:20:46,061 - INFO - joeynmt.training - EPOCH 703\n",
            "2021-11-23 23:20:46,642 - INFO - joeynmt.training - Epoch 703: total training loss 0.18\n",
            "2021-11-23 23:20:46,642 - INFO - joeynmt.training - EPOCH 704\n",
            "2021-11-23 23:20:47,202 - INFO - joeynmt.training - Epoch 704: total training loss 0.17\n",
            "2021-11-23 23:20:47,202 - INFO - joeynmt.training - EPOCH 705\n",
            "2021-11-23 23:20:47,757 - INFO - joeynmt.training - Epoch 705: total training loss 0.17\n",
            "2021-11-23 23:20:47,757 - INFO - joeynmt.training - EPOCH 706\n",
            "2021-11-23 23:20:48,314 - INFO - joeynmt.training - Epoch 706: total training loss 0.18\n",
            "2021-11-23 23:20:48,315 - INFO - joeynmt.training - EPOCH 707\n",
            "2021-11-23 23:20:48,884 - INFO - joeynmt.training - Epoch 707: total training loss 0.16\n",
            "2021-11-23 23:20:48,885 - INFO - joeynmt.training - EPOCH 708\n",
            "2021-11-23 23:20:49,440 - INFO - joeynmt.training - Epoch 708: total training loss 0.17\n",
            "2021-11-23 23:20:49,441 - INFO - joeynmt.training - EPOCH 709\n",
            "2021-11-23 23:20:50,010 - INFO - joeynmt.training - Epoch 709: total training loss 0.18\n",
            "2021-11-23 23:20:50,011 - INFO - joeynmt.training - EPOCH 710\n",
            "2021-11-23 23:20:50,578 - INFO - joeynmt.training - Epoch 710: total training loss 0.17\n",
            "2021-11-23 23:20:50,578 - INFO - joeynmt.training - EPOCH 711\n",
            "2021-11-23 23:20:51,143 - INFO - joeynmt.training - Epoch 711: total training loss 0.18\n",
            "2021-11-23 23:20:51,143 - INFO - joeynmt.training - EPOCH 712\n",
            "2021-11-23 23:20:51,713 - INFO - joeynmt.training - Epoch 712: total training loss 0.19\n",
            "2021-11-23 23:20:51,713 - INFO - joeynmt.training - EPOCH 713\n",
            "2021-11-23 23:20:52,276 - INFO - joeynmt.training - Epoch 713: total training loss 0.17\n",
            "2021-11-23 23:20:52,277 - INFO - joeynmt.training - EPOCH 714\n",
            "2021-11-23 23:20:52,840 - INFO - joeynmt.training - Epoch 714: total training loss 0.18\n",
            "2021-11-23 23:20:52,840 - INFO - joeynmt.training - EPOCH 715\n",
            "2021-11-23 23:20:53,410 - INFO - joeynmt.training - Epoch 715: total training loss 0.17\n",
            "2021-11-23 23:20:53,411 - INFO - joeynmt.training - EPOCH 716\n",
            "2021-11-23 23:20:53,964 - INFO - joeynmt.training - Epoch 716: total training loss 0.17\n",
            "2021-11-23 23:20:53,964 - INFO - joeynmt.training - EPOCH 717\n",
            "2021-11-23 23:20:54,527 - INFO - joeynmt.training - Epoch 717: total training loss 0.16\n",
            "2021-11-23 23:20:54,528 - INFO - joeynmt.training - EPOCH 718\n",
            "2021-11-23 23:20:55,097 - INFO - joeynmt.training - Epoch 718: total training loss 0.17\n",
            "2021-11-23 23:20:55,097 - INFO - joeynmt.training - EPOCH 719\n",
            "2021-11-23 23:20:55,676 - INFO - joeynmt.training - Epoch 719: total training loss 0.18\n",
            "2021-11-23 23:20:55,676 - INFO - joeynmt.training - EPOCH 720\n",
            "2021-11-23 23:20:56,240 - INFO - joeynmt.training - Epoch 720: total training loss 0.18\n",
            "2021-11-23 23:20:56,240 - INFO - joeynmt.training - EPOCH 721\n",
            "2021-11-23 23:20:56,805 - INFO - joeynmt.training - Epoch 721: total training loss 0.16\n",
            "2021-11-23 23:20:56,805 - INFO - joeynmt.training - EPOCH 722\n",
            "2021-11-23 23:20:57,369 - INFO - joeynmt.training - Epoch 722: total training loss 0.18\n",
            "2021-11-23 23:20:57,369 - INFO - joeynmt.training - EPOCH 723\n",
            "2021-11-23 23:20:57,927 - INFO - joeynmt.training - Epoch 723: total training loss 0.18\n",
            "2021-11-23 23:20:57,928 - INFO - joeynmt.training - EPOCH 724\n",
            "2021-11-23 23:20:58,493 - INFO - joeynmt.training - Epoch 724: total training loss 0.18\n",
            "2021-11-23 23:20:58,494 - INFO - joeynmt.training - EPOCH 725\n",
            "2021-11-23 23:20:59,061 - INFO - joeynmt.training - Epoch 725: total training loss 0.17\n",
            "2021-11-23 23:20:59,061 - INFO - joeynmt.training - EPOCH 726\n",
            "2021-11-23 23:20:59,624 - INFO - joeynmt.training - Epoch 726: total training loss 0.17\n",
            "2021-11-23 23:20:59,624 - INFO - joeynmt.training - EPOCH 727\n",
            "2021-11-23 23:21:00,203 - INFO - joeynmt.training - Epoch 727: total training loss 0.16\n",
            "2021-11-23 23:21:00,204 - INFO - joeynmt.training - EPOCH 728\n",
            "2021-11-23 23:21:00,778 - INFO - joeynmt.training - Epoch 728: total training loss 0.17\n",
            "2021-11-23 23:21:00,779 - INFO - joeynmt.training - EPOCH 729\n",
            "2021-11-23 23:21:01,352 - INFO - joeynmt.training - Epoch 729: total training loss 0.16\n",
            "2021-11-23 23:21:01,353 - INFO - joeynmt.training - EPOCH 730\n",
            "2021-11-23 23:21:01,919 - INFO - joeynmt.training - Epoch 730: total training loss 0.17\n",
            "2021-11-23 23:21:01,920 - INFO - joeynmt.training - EPOCH 731\n",
            "2021-11-23 23:21:02,480 - INFO - joeynmt.training - Epoch 731: total training loss 0.16\n",
            "2021-11-23 23:21:02,480 - INFO - joeynmt.training - EPOCH 732\n",
            "2021-11-23 23:21:03,046 - INFO - joeynmt.training - Epoch 732: total training loss 0.16\n",
            "2021-11-23 23:21:03,046 - INFO - joeynmt.training - EPOCH 733\n",
            "2021-11-23 23:21:03,620 - INFO - joeynmt.training - Epoch 733: total training loss 0.16\n",
            "2021-11-23 23:21:03,621 - INFO - joeynmt.training - EPOCH 734\n",
            "2021-11-23 23:21:04,189 - INFO - joeynmt.training - Epoch 734: total training loss 0.18\n",
            "2021-11-23 23:21:04,189 - INFO - joeynmt.training - EPOCH 735\n",
            "2021-11-23 23:21:04,764 - INFO - joeynmt.training - Epoch 735: total training loss 0.16\n",
            "2021-11-23 23:21:04,765 - INFO - joeynmt.training - EPOCH 736\n",
            "2021-11-23 23:21:05,338 - INFO - joeynmt.training - Epoch 736: total training loss 0.16\n",
            "2021-11-23 23:21:05,338 - INFO - joeynmt.training - EPOCH 737\n",
            "2021-11-23 23:21:05,921 - INFO - joeynmt.training - Epoch 737: total training loss 0.16\n",
            "2021-11-23 23:21:05,922 - INFO - joeynmt.training - EPOCH 738\n",
            "2021-11-23 23:21:06,483 - INFO - joeynmt.training - Epoch 738: total training loss 0.16\n",
            "2021-11-23 23:21:06,483 - INFO - joeynmt.training - EPOCH 739\n",
            "2021-11-23 23:21:07,050 - INFO - joeynmt.training - Epoch 739: total training loss 0.17\n",
            "2021-11-23 23:21:07,050 - INFO - joeynmt.training - EPOCH 740\n",
            "2021-11-23 23:21:07,616 - INFO - joeynmt.training - Epoch 740: total training loss 0.17\n",
            "2021-11-23 23:21:07,616 - INFO - joeynmt.training - EPOCH 741\n",
            "2021-11-23 23:21:08,182 - INFO - joeynmt.training - Epoch 741: total training loss 0.16\n",
            "2021-11-23 23:21:08,183 - INFO - joeynmt.training - EPOCH 742\n",
            "2021-11-23 23:21:08,748 - INFO - joeynmt.training - Epoch 742: total training loss 0.17\n",
            "2021-11-23 23:21:08,748 - INFO - joeynmt.training - EPOCH 743\n",
            "2021-11-23 23:21:09,314 - INFO - joeynmt.training - Epoch 743: total training loss 0.16\n",
            "2021-11-23 23:21:09,315 - INFO - joeynmt.training - EPOCH 744\n",
            "2021-11-23 23:21:09,889 - INFO - joeynmt.training - Epoch 744: total training loss 0.15\n",
            "2021-11-23 23:21:09,890 - INFO - joeynmt.training - EPOCH 745\n",
            "2021-11-23 23:21:10,445 - INFO - joeynmt.training - Epoch 745: total training loss 0.17\n",
            "2021-11-23 23:21:10,446 - INFO - joeynmt.training - EPOCH 746\n",
            "2021-11-23 23:21:11,009 - INFO - joeynmt.training - Epoch 746: total training loss 0.16\n",
            "2021-11-23 23:21:11,009 - INFO - joeynmt.training - EPOCH 747\n",
            "2021-11-23 23:21:11,582 - INFO - joeynmt.training - Epoch 747: total training loss 0.17\n",
            "2021-11-23 23:21:11,583 - INFO - joeynmt.training - EPOCH 748\n",
            "2021-11-23 23:21:12,154 - INFO - joeynmt.training - Epoch 748: total training loss 0.17\n",
            "2021-11-23 23:21:12,154 - INFO - joeynmt.training - EPOCH 749\n",
            "2021-11-23 23:21:12,726 - INFO - joeynmt.training - Epoch 749: total training loss 0.16\n",
            "2021-11-23 23:21:12,726 - INFO - joeynmt.training - EPOCH 750\n",
            "2021-11-23 23:21:13,301 - INFO - joeynmt.training - Epoch 750, Step:     1500, Batch Loss:     0.068716, Tokens per Sec:     6574, Lr: 0.000300\n",
            "2021-11-23 23:21:13,302 - INFO - joeynmt.training - Epoch 750: total training loss 0.15\n",
            "2021-11-23 23:21:13,302 - INFO - joeynmt.training - EPOCH 751\n",
            "2021-11-23 23:21:13,871 - INFO - joeynmt.training - Epoch 751: total training loss 0.16\n",
            "2021-11-23 23:21:13,872 - INFO - joeynmt.training - EPOCH 752\n",
            "2021-11-23 23:21:14,446 - INFO - joeynmt.training - Epoch 752: total training loss 0.16\n",
            "2021-11-23 23:21:14,447 - INFO - joeynmt.training - EPOCH 753\n",
            "2021-11-23 23:21:15,016 - INFO - joeynmt.training - Epoch 753: total training loss 0.17\n",
            "2021-11-23 23:21:15,017 - INFO - joeynmt.training - EPOCH 754\n",
            "2021-11-23 23:21:15,582 - INFO - joeynmt.training - Epoch 754: total training loss 0.16\n",
            "2021-11-23 23:21:15,582 - INFO - joeynmt.training - EPOCH 755\n",
            "2021-11-23 23:21:16,156 - INFO - joeynmt.training - Epoch 755: total training loss 0.16\n",
            "2021-11-23 23:21:16,157 - INFO - joeynmt.training - EPOCH 756\n",
            "2021-11-23 23:21:16,729 - INFO - joeynmt.training - Epoch 756: total training loss 0.15\n",
            "2021-11-23 23:21:16,730 - INFO - joeynmt.training - EPOCH 757\n",
            "2021-11-23 23:21:17,297 - INFO - joeynmt.training - Epoch 757: total training loss 0.16\n",
            "2021-11-23 23:21:17,298 - INFO - joeynmt.training - EPOCH 758\n",
            "2021-11-23 23:21:17,867 - INFO - joeynmt.training - Epoch 758: total training loss 0.15\n",
            "2021-11-23 23:21:17,868 - INFO - joeynmt.training - EPOCH 759\n",
            "2021-11-23 23:21:18,449 - INFO - joeynmt.training - Epoch 759: total training loss 0.16\n",
            "2021-11-23 23:21:18,449 - INFO - joeynmt.training - EPOCH 760\n",
            "2021-11-23 23:21:19,024 - INFO - joeynmt.training - Epoch 760: total training loss 0.16\n",
            "2021-11-23 23:21:19,025 - INFO - joeynmt.training - EPOCH 761\n",
            "2021-11-23 23:21:19,595 - INFO - joeynmt.training - Epoch 761: total training loss 0.15\n",
            "2021-11-23 23:21:19,595 - INFO - joeynmt.training - EPOCH 762\n",
            "2021-11-23 23:21:20,162 - INFO - joeynmt.training - Epoch 762: total training loss 0.15\n",
            "2021-11-23 23:21:20,163 - INFO - joeynmt.training - EPOCH 763\n",
            "2021-11-23 23:21:20,717 - INFO - joeynmt.training - Epoch 763: total training loss 0.16\n",
            "2021-11-23 23:21:20,718 - INFO - joeynmt.training - EPOCH 764\n",
            "2021-11-23 23:21:21,282 - INFO - joeynmt.training - Epoch 764: total training loss 0.15\n",
            "2021-11-23 23:21:21,283 - INFO - joeynmt.training - EPOCH 765\n",
            "2021-11-23 23:21:21,855 - INFO - joeynmt.training - Epoch 765: total training loss 0.16\n",
            "2021-11-23 23:21:21,855 - INFO - joeynmt.training - EPOCH 766\n",
            "2021-11-23 23:21:22,431 - INFO - joeynmt.training - Epoch 766: total training loss 0.16\n",
            "2021-11-23 23:21:22,432 - INFO - joeynmt.training - EPOCH 767\n",
            "2021-11-23 23:21:22,998 - INFO - joeynmt.training - Epoch 767: total training loss 0.16\n",
            "2021-11-23 23:21:22,998 - INFO - joeynmt.training - EPOCH 768\n",
            "2021-11-23 23:21:23,576 - INFO - joeynmt.training - Epoch 768: total training loss 0.17\n",
            "2021-11-23 23:21:23,576 - INFO - joeynmt.training - EPOCH 769\n",
            "2021-11-23 23:21:24,142 - INFO - joeynmt.training - Epoch 769: total training loss 0.15\n",
            "2021-11-23 23:21:24,142 - INFO - joeynmt.training - EPOCH 770\n",
            "2021-11-23 23:21:24,715 - INFO - joeynmt.training - Epoch 770: total training loss 0.15\n",
            "2021-11-23 23:21:24,715 - INFO - joeynmt.training - EPOCH 771\n",
            "2021-11-23 23:21:25,276 - INFO - joeynmt.training - Epoch 771: total training loss 0.16\n",
            "2021-11-23 23:21:25,277 - INFO - joeynmt.training - EPOCH 772\n",
            "2021-11-23 23:21:25,854 - INFO - joeynmt.training - Epoch 772: total training loss 0.16\n",
            "2021-11-23 23:21:25,854 - INFO - joeynmt.training - EPOCH 773\n",
            "2021-11-23 23:21:26,433 - INFO - joeynmt.training - Epoch 773: total training loss 0.15\n",
            "2021-11-23 23:21:26,433 - INFO - joeynmt.training - EPOCH 774\n",
            "2021-11-23 23:21:26,998 - INFO - joeynmt.training - Epoch 774: total training loss 0.15\n",
            "2021-11-23 23:21:26,999 - INFO - joeynmt.training - EPOCH 775\n",
            "2021-11-23 23:21:27,575 - INFO - joeynmt.training - Epoch 775: total training loss 0.16\n",
            "2021-11-23 23:21:27,576 - INFO - joeynmt.training - EPOCH 776\n",
            "2021-11-23 23:21:28,141 - INFO - joeynmt.training - Epoch 776: total training loss 0.15\n",
            "2021-11-23 23:21:28,141 - INFO - joeynmt.training - EPOCH 777\n",
            "2021-11-23 23:21:28,713 - INFO - joeynmt.training - Epoch 777: total training loss 0.16\n",
            "2021-11-23 23:21:28,713 - INFO - joeynmt.training - EPOCH 778\n",
            "2021-11-23 23:21:29,290 - INFO - joeynmt.training - Epoch 778: total training loss 0.15\n",
            "2021-11-23 23:21:29,291 - INFO - joeynmt.training - EPOCH 779\n",
            "2021-11-23 23:21:29,860 - INFO - joeynmt.training - Epoch 779: total training loss 0.15\n",
            "2021-11-23 23:21:29,860 - INFO - joeynmt.training - EPOCH 780\n",
            "2021-11-23 23:21:30,425 - INFO - joeynmt.training - Epoch 780: total training loss 0.15\n",
            "2021-11-23 23:21:30,425 - INFO - joeynmt.training - EPOCH 781\n",
            "2021-11-23 23:21:30,992 - INFO - joeynmt.training - Epoch 781: total training loss 0.15\n",
            "2021-11-23 23:21:30,992 - INFO - joeynmt.training - EPOCH 782\n",
            "2021-11-23 23:21:31,559 - INFO - joeynmt.training - Epoch 782: total training loss 0.15\n",
            "2021-11-23 23:21:31,560 - INFO - joeynmt.training - EPOCH 783\n",
            "2021-11-23 23:21:32,124 - INFO - joeynmt.training - Epoch 783: total training loss 0.15\n",
            "2021-11-23 23:21:32,124 - INFO - joeynmt.training - EPOCH 784\n",
            "2021-11-23 23:21:32,697 - INFO - joeynmt.training - Epoch 784: total training loss 0.15\n",
            "2021-11-23 23:21:32,697 - INFO - joeynmt.training - EPOCH 785\n",
            "2021-11-23 23:21:33,268 - INFO - joeynmt.training - Epoch 785: total training loss 0.15\n",
            "2021-11-23 23:21:33,269 - INFO - joeynmt.training - EPOCH 786\n",
            "2021-11-23 23:21:33,828 - INFO - joeynmt.training - Epoch 786: total training loss 0.14\n",
            "2021-11-23 23:21:33,829 - INFO - joeynmt.training - EPOCH 787\n",
            "2021-11-23 23:21:34,398 - INFO - joeynmt.training - Epoch 787: total training loss 0.15\n",
            "2021-11-23 23:21:34,398 - INFO - joeynmt.training - EPOCH 788\n",
            "2021-11-23 23:21:34,963 - INFO - joeynmt.training - Epoch 788: total training loss 0.15\n",
            "2021-11-23 23:21:34,963 - INFO - joeynmt.training - EPOCH 789\n",
            "2021-11-23 23:21:35,533 - INFO - joeynmt.training - Epoch 789: total training loss 0.15\n",
            "2021-11-23 23:21:35,533 - INFO - joeynmt.training - EPOCH 790\n",
            "2021-11-23 23:21:36,113 - INFO - joeynmt.training - Epoch 790: total training loss 0.14\n",
            "2021-11-23 23:21:36,113 - INFO - joeynmt.training - EPOCH 791\n",
            "2021-11-23 23:21:36,680 - INFO - joeynmt.training - Epoch 791: total training loss 0.15\n",
            "2021-11-23 23:21:36,680 - INFO - joeynmt.training - EPOCH 792\n",
            "2021-11-23 23:21:37,249 - INFO - joeynmt.training - Epoch 792: total training loss 0.15\n",
            "2021-11-23 23:21:37,250 - INFO - joeynmt.training - EPOCH 793\n",
            "2021-11-23 23:21:37,809 - INFO - joeynmt.training - Epoch 793: total training loss 0.14\n",
            "2021-11-23 23:21:37,809 - INFO - joeynmt.training - EPOCH 794\n",
            "2021-11-23 23:21:38,374 - INFO - joeynmt.training - Epoch 794: total training loss 0.14\n",
            "2021-11-23 23:21:38,375 - INFO - joeynmt.training - EPOCH 795\n",
            "2021-11-23 23:21:38,949 - INFO - joeynmt.training - Epoch 795: total training loss 0.14\n",
            "2021-11-23 23:21:38,950 - INFO - joeynmt.training - EPOCH 796\n",
            "2021-11-23 23:21:39,518 - INFO - joeynmt.training - Epoch 796: total training loss 0.15\n",
            "2021-11-23 23:21:39,519 - INFO - joeynmt.training - EPOCH 797\n",
            "2021-11-23 23:21:40,090 - INFO - joeynmt.training - Epoch 797: total training loss 0.14\n",
            "2021-11-23 23:21:40,091 - INFO - joeynmt.training - EPOCH 798\n",
            "2021-11-23 23:21:40,658 - INFO - joeynmt.training - Epoch 798: total training loss 0.16\n",
            "2021-11-23 23:21:40,659 - INFO - joeynmt.training - EPOCH 799\n",
            "2021-11-23 23:21:41,227 - INFO - joeynmt.training - Epoch 799: total training loss 0.14\n",
            "2021-11-23 23:21:41,227 - INFO - joeynmt.training - EPOCH 800\n",
            "2021-11-23 23:21:41,794 - INFO - joeynmt.training - Epoch 800, Step:     1600, Batch Loss:     0.066980, Tokens per Sec:     6659, Lr: 0.000300\n",
            "2021-11-23 23:21:41,794 - INFO - joeynmt.training - Epoch 800: total training loss 0.14\n",
            "2021-11-23 23:21:41,794 - INFO - joeynmt.training - EPOCH 801\n",
            "2021-11-23 23:21:42,354 - INFO - joeynmt.training - Epoch 801: total training loss 0.14\n",
            "2021-11-23 23:21:42,355 - INFO - joeynmt.training - EPOCH 802\n",
            "2021-11-23 23:21:42,930 - INFO - joeynmt.training - Epoch 802: total training loss 0.14\n",
            "2021-11-23 23:21:42,931 - INFO - joeynmt.training - EPOCH 803\n",
            "2021-11-23 23:21:43,501 - INFO - joeynmt.training - Epoch 803: total training loss 0.15\n",
            "2021-11-23 23:21:43,501 - INFO - joeynmt.training - EPOCH 804\n",
            "2021-11-23 23:21:44,061 - INFO - joeynmt.training - Epoch 804: total training loss 0.15\n",
            "2021-11-23 23:21:44,061 - INFO - joeynmt.training - EPOCH 805\n",
            "2021-11-23 23:21:44,622 - INFO - joeynmt.training - Epoch 805: total training loss 0.13\n",
            "2021-11-23 23:21:44,623 - INFO - joeynmt.training - EPOCH 806\n",
            "2021-11-23 23:21:45,186 - INFO - joeynmt.training - Epoch 806: total training loss 0.14\n",
            "2021-11-23 23:21:45,186 - INFO - joeynmt.training - EPOCH 807\n",
            "2021-11-23 23:21:45,755 - INFO - joeynmt.training - Epoch 807: total training loss 0.15\n",
            "2021-11-23 23:21:45,755 - INFO - joeynmt.training - EPOCH 808\n",
            "2021-11-23 23:21:46,325 - INFO - joeynmt.training - Epoch 808: total training loss 0.14\n",
            "2021-11-23 23:21:46,325 - INFO - joeynmt.training - EPOCH 809\n",
            "2021-11-23 23:21:46,892 - INFO - joeynmt.training - Epoch 809: total training loss 0.14\n",
            "2021-11-23 23:21:46,892 - INFO - joeynmt.training - EPOCH 810\n",
            "2021-11-23 23:21:47,464 - INFO - joeynmt.training - Epoch 810: total training loss 0.14\n",
            "2021-11-23 23:21:47,465 - INFO - joeynmt.training - EPOCH 811\n",
            "2021-11-23 23:21:48,030 - INFO - joeynmt.training - Epoch 811: total training loss 0.14\n",
            "2021-11-23 23:21:48,031 - INFO - joeynmt.training - EPOCH 812\n",
            "2021-11-23 23:21:48,598 - INFO - joeynmt.training - Epoch 812: total training loss 0.14\n",
            "2021-11-23 23:21:48,598 - INFO - joeynmt.training - EPOCH 813\n",
            "2021-11-23 23:21:49,165 - INFO - joeynmt.training - Epoch 813: total training loss 0.14\n",
            "2021-11-23 23:21:49,166 - INFO - joeynmt.training - EPOCH 814\n",
            "2021-11-23 23:21:49,735 - INFO - joeynmt.training - Epoch 814: total training loss 0.14\n",
            "2021-11-23 23:21:49,736 - INFO - joeynmt.training - EPOCH 815\n",
            "2021-11-23 23:21:50,298 - INFO - joeynmt.training - Epoch 815: total training loss 0.14\n",
            "2021-11-23 23:21:50,298 - INFO - joeynmt.training - EPOCH 816\n",
            "2021-11-23 23:21:50,877 - INFO - joeynmt.training - Epoch 816: total training loss 0.15\n",
            "2021-11-23 23:21:50,877 - INFO - joeynmt.training - EPOCH 817\n",
            "2021-11-23 23:21:51,446 - INFO - joeynmt.training - Epoch 817: total training loss 0.14\n",
            "2021-11-23 23:21:51,447 - INFO - joeynmt.training - EPOCH 818\n",
            "2021-11-23 23:21:52,006 - INFO - joeynmt.training - Epoch 818: total training loss 0.14\n",
            "2021-11-23 23:21:52,006 - INFO - joeynmt.training - EPOCH 819\n",
            "2021-11-23 23:21:52,576 - INFO - joeynmt.training - Epoch 819: total training loss 0.15\n",
            "2021-11-23 23:21:52,577 - INFO - joeynmt.training - EPOCH 820\n",
            "2021-11-23 23:21:53,145 - INFO - joeynmt.training - Epoch 820: total training loss 0.16\n",
            "2021-11-23 23:21:53,145 - INFO - joeynmt.training - EPOCH 821\n",
            "2021-11-23 23:21:53,712 - INFO - joeynmt.training - Epoch 821: total training loss 0.15\n",
            "2021-11-23 23:21:53,712 - INFO - joeynmt.training - EPOCH 822\n",
            "2021-11-23 23:21:54,280 - INFO - joeynmt.training - Epoch 822: total training loss 0.14\n",
            "2021-11-23 23:21:54,280 - INFO - joeynmt.training - EPOCH 823\n",
            "2021-11-23 23:21:54,857 - INFO - joeynmt.training - Epoch 823: total training loss 0.15\n",
            "2021-11-23 23:21:54,857 - INFO - joeynmt.training - EPOCH 824\n",
            "2021-11-23 23:21:55,421 - INFO - joeynmt.training - Epoch 824: total training loss 0.14\n",
            "2021-11-23 23:21:55,421 - INFO - joeynmt.training - EPOCH 825\n",
            "2021-11-23 23:21:55,986 - INFO - joeynmt.training - Epoch 825: total training loss 0.13\n",
            "2021-11-23 23:21:55,986 - INFO - joeynmt.training - EPOCH 826\n",
            "2021-11-23 23:21:56,548 - INFO - joeynmt.training - Epoch 826: total training loss 0.14\n",
            "2021-11-23 23:21:56,549 - INFO - joeynmt.training - EPOCH 827\n",
            "2021-11-23 23:21:57,117 - INFO - joeynmt.training - Epoch 827: total training loss 0.14\n",
            "2021-11-23 23:21:57,118 - INFO - joeynmt.training - EPOCH 828\n",
            "2021-11-23 23:21:57,698 - INFO - joeynmt.training - Epoch 828: total training loss 0.14\n",
            "2021-11-23 23:21:57,699 - INFO - joeynmt.training - EPOCH 829\n",
            "2021-11-23 23:21:58,264 - INFO - joeynmt.training - Epoch 829: total training loss 0.14\n",
            "2021-11-23 23:21:58,264 - INFO - joeynmt.training - EPOCH 830\n",
            "2021-11-23 23:21:58,828 - INFO - joeynmt.training - Epoch 830: total training loss 0.14\n",
            "2021-11-23 23:21:58,828 - INFO - joeynmt.training - EPOCH 831\n",
            "2021-11-23 23:21:59,402 - INFO - joeynmt.training - Epoch 831: total training loss 0.13\n",
            "2021-11-23 23:21:59,402 - INFO - joeynmt.training - EPOCH 832\n",
            "2021-11-23 23:21:59,961 - INFO - joeynmt.training - Epoch 832: total training loss 0.13\n",
            "2021-11-23 23:21:59,962 - INFO - joeynmt.training - EPOCH 833\n",
            "2021-11-23 23:22:00,532 - INFO - joeynmt.training - Epoch 833: total training loss 0.14\n",
            "2021-11-23 23:22:00,532 - INFO - joeynmt.training - EPOCH 834\n",
            "2021-11-23 23:22:01,105 - INFO - joeynmt.training - Epoch 834: total training loss 0.14\n",
            "2021-11-23 23:22:01,106 - INFO - joeynmt.training - EPOCH 835\n",
            "2021-11-23 23:22:01,667 - INFO - joeynmt.training - Epoch 835: total training loss 0.13\n",
            "2021-11-23 23:22:01,667 - INFO - joeynmt.training - EPOCH 836\n",
            "2021-11-23 23:22:02,235 - INFO - joeynmt.training - Epoch 836: total training loss 0.13\n",
            "2021-11-23 23:22:02,235 - INFO - joeynmt.training - EPOCH 837\n",
            "2021-11-23 23:22:02,793 - INFO - joeynmt.training - Epoch 837: total training loss 0.13\n",
            "2021-11-23 23:22:02,794 - INFO - joeynmt.training - EPOCH 838\n",
            "2021-11-23 23:22:03,352 - INFO - joeynmt.training - Epoch 838: total training loss 0.13\n",
            "2021-11-23 23:22:03,352 - INFO - joeynmt.training - EPOCH 839\n",
            "2021-11-23 23:22:03,922 - INFO - joeynmt.training - Epoch 839: total training loss 0.14\n",
            "2021-11-23 23:22:03,923 - INFO - joeynmt.training - EPOCH 840\n",
            "2021-11-23 23:22:04,490 - INFO - joeynmt.training - Epoch 840: total training loss 0.14\n",
            "2021-11-23 23:22:04,490 - INFO - joeynmt.training - EPOCH 841\n",
            "2021-11-23 23:22:05,052 - INFO - joeynmt.training - Epoch 841: total training loss 0.13\n",
            "2021-11-23 23:22:05,053 - INFO - joeynmt.training - EPOCH 842\n",
            "2021-11-23 23:22:05,632 - INFO - joeynmt.training - Epoch 842: total training loss 0.12\n",
            "2021-11-23 23:22:05,632 - INFO - joeynmt.training - EPOCH 843\n",
            "2021-11-23 23:22:06,200 - INFO - joeynmt.training - Epoch 843: total training loss 0.13\n",
            "2021-11-23 23:22:06,200 - INFO - joeynmt.training - EPOCH 844\n",
            "2021-11-23 23:22:06,760 - INFO - joeynmt.training - Epoch 844: total training loss 0.13\n",
            "2021-11-23 23:22:06,761 - INFO - joeynmt.training - EPOCH 845\n",
            "2021-11-23 23:22:07,324 - INFO - joeynmt.training - Epoch 845: total training loss 0.13\n",
            "2021-11-23 23:22:07,324 - INFO - joeynmt.training - EPOCH 846\n",
            "2021-11-23 23:22:07,907 - INFO - joeynmt.training - Epoch 846: total training loss 0.13\n",
            "2021-11-23 23:22:07,907 - INFO - joeynmt.training - EPOCH 847\n",
            "2021-11-23 23:22:08,472 - INFO - joeynmt.training - Epoch 847: total training loss 0.13\n",
            "2021-11-23 23:22:08,472 - INFO - joeynmt.training - EPOCH 848\n",
            "2021-11-23 23:22:09,036 - INFO - joeynmt.training - Epoch 848: total training loss 0.13\n",
            "2021-11-23 23:22:09,037 - INFO - joeynmt.training - EPOCH 849\n",
            "2021-11-23 23:22:09,608 - INFO - joeynmt.training - Epoch 849: total training loss 0.13\n",
            "2021-11-23 23:22:09,609 - INFO - joeynmt.training - EPOCH 850\n",
            "2021-11-23 23:22:10,171 - INFO - joeynmt.training - Epoch 850, Step:     1700, Batch Loss:     0.058879, Tokens per Sec:     6710, Lr: 0.000300\n",
            "2021-11-23 23:22:10,172 - INFO - joeynmt.training - Epoch 850: total training loss 0.13\n",
            "2021-11-23 23:22:10,172 - INFO - joeynmt.training - EPOCH 851\n",
            "2021-11-23 23:22:10,738 - INFO - joeynmt.training - Epoch 851: total training loss 0.12\n",
            "2021-11-23 23:22:10,738 - INFO - joeynmt.training - EPOCH 852\n",
            "2021-11-23 23:22:11,314 - INFO - joeynmt.training - Epoch 852: total training loss 0.13\n",
            "2021-11-23 23:22:11,314 - INFO - joeynmt.training - EPOCH 853\n",
            "2021-11-23 23:22:11,875 - INFO - joeynmt.training - Epoch 853: total training loss 0.13\n",
            "2021-11-23 23:22:11,875 - INFO - joeynmt.training - EPOCH 854\n",
            "2021-11-23 23:22:12,442 - INFO - joeynmt.training - Epoch 854: total training loss 0.13\n",
            "2021-11-23 23:22:12,442 - INFO - joeynmt.training - EPOCH 855\n",
            "2021-11-23 23:22:13,011 - INFO - joeynmt.training - Epoch 855: total training loss 0.13\n",
            "2021-11-23 23:22:13,012 - INFO - joeynmt.training - EPOCH 856\n",
            "2021-11-23 23:22:13,577 - INFO - joeynmt.training - Epoch 856: total training loss 0.13\n",
            "2021-11-23 23:22:13,578 - INFO - joeynmt.training - EPOCH 857\n",
            "2021-11-23 23:22:14,159 - INFO - joeynmt.training - Epoch 857: total training loss 0.13\n",
            "2021-11-23 23:22:14,159 - INFO - joeynmt.training - EPOCH 858\n",
            "2021-11-23 23:22:14,718 - INFO - joeynmt.training - Epoch 858: total training loss 0.13\n",
            "2021-11-23 23:22:14,719 - INFO - joeynmt.training - EPOCH 859\n",
            "2021-11-23 23:22:15,286 - INFO - joeynmt.training - Epoch 859: total training loss 0.13\n",
            "2021-11-23 23:22:15,287 - INFO - joeynmt.training - EPOCH 860\n",
            "2021-11-23 23:22:15,853 - INFO - joeynmt.training - Epoch 860: total training loss 0.13\n",
            "2021-11-23 23:22:15,854 - INFO - joeynmt.training - EPOCH 861\n",
            "2021-11-23 23:22:16,422 - INFO - joeynmt.training - Epoch 861: total training loss 0.12\n",
            "2021-11-23 23:22:16,423 - INFO - joeynmt.training - EPOCH 862\n",
            "2021-11-23 23:22:16,998 - INFO - joeynmt.training - Epoch 862: total training loss 0.14\n",
            "2021-11-23 23:22:16,998 - INFO - joeynmt.training - EPOCH 863\n",
            "2021-11-23 23:22:17,563 - INFO - joeynmt.training - Epoch 863: total training loss 0.13\n",
            "2021-11-23 23:22:17,564 - INFO - joeynmt.training - EPOCH 864\n",
            "2021-11-23 23:22:18,141 - INFO - joeynmt.training - Epoch 864: total training loss 0.13\n",
            "2021-11-23 23:22:18,141 - INFO - joeynmt.training - EPOCH 865\n",
            "2021-11-23 23:22:18,709 - INFO - joeynmt.training - Epoch 865: total training loss 0.12\n",
            "2021-11-23 23:22:18,710 - INFO - joeynmt.training - EPOCH 866\n",
            "2021-11-23 23:22:19,274 - INFO - joeynmt.training - Epoch 866: total training loss 0.13\n",
            "2021-11-23 23:22:19,274 - INFO - joeynmt.training - EPOCH 867\n",
            "2021-11-23 23:22:19,839 - INFO - joeynmt.training - Epoch 867: total training loss 0.12\n",
            "2021-11-23 23:22:19,840 - INFO - joeynmt.training - EPOCH 868\n",
            "2021-11-23 23:22:20,404 - INFO - joeynmt.training - Epoch 868: total training loss 0.13\n",
            "2021-11-23 23:22:20,405 - INFO - joeynmt.training - EPOCH 869\n",
            "2021-11-23 23:22:20,979 - INFO - joeynmt.training - Epoch 869: total training loss 0.13\n",
            "2021-11-23 23:22:20,979 - INFO - joeynmt.training - EPOCH 870\n",
            "2021-11-23 23:22:21,542 - INFO - joeynmt.training - Epoch 870: total training loss 0.13\n",
            "2021-11-23 23:22:21,542 - INFO - joeynmt.training - EPOCH 871\n",
            "2021-11-23 23:22:22,102 - INFO - joeynmt.training - Epoch 871: total training loss 0.12\n",
            "2021-11-23 23:22:22,102 - INFO - joeynmt.training - EPOCH 872\n",
            "2021-11-23 23:22:22,681 - INFO - joeynmt.training - Epoch 872: total training loss 0.12\n",
            "2021-11-23 23:22:22,681 - INFO - joeynmt.training - EPOCH 873\n",
            "2021-11-23 23:22:23,256 - INFO - joeynmt.training - Epoch 873: total training loss 0.13\n",
            "2021-11-23 23:22:23,257 - INFO - joeynmt.training - EPOCH 874\n",
            "2021-11-23 23:22:23,829 - INFO - joeynmt.training - Epoch 874: total training loss 0.12\n",
            "2021-11-23 23:22:23,829 - INFO - joeynmt.training - EPOCH 875\n",
            "2021-11-23 23:22:24,399 - INFO - joeynmt.training - Epoch 875: total training loss 0.12\n",
            "2021-11-23 23:22:24,400 - INFO - joeynmt.training - EPOCH 876\n",
            "2021-11-23 23:22:24,966 - INFO - joeynmt.training - Epoch 876: total training loss 0.12\n",
            "2021-11-23 23:22:24,966 - INFO - joeynmt.training - EPOCH 877\n",
            "2021-11-23 23:22:25,534 - INFO - joeynmt.training - Epoch 877: total training loss 0.13\n",
            "2021-11-23 23:22:25,534 - INFO - joeynmt.training - EPOCH 878\n",
            "2021-11-23 23:22:26,104 - INFO - joeynmt.training - Epoch 878: total training loss 0.13\n",
            "2021-11-23 23:22:26,104 - INFO - joeynmt.training - EPOCH 879\n",
            "2021-11-23 23:22:26,675 - INFO - joeynmt.training - Epoch 879: total training loss 0.12\n",
            "2021-11-23 23:22:26,676 - INFO - joeynmt.training - EPOCH 880\n",
            "2021-11-23 23:22:27,245 - INFO - joeynmt.training - Epoch 880: total training loss 0.13\n",
            "2021-11-23 23:22:27,245 - INFO - joeynmt.training - EPOCH 881\n",
            "2021-11-23 23:22:27,814 - INFO - joeynmt.training - Epoch 881: total training loss 0.12\n",
            "2021-11-23 23:22:27,814 - INFO - joeynmt.training - EPOCH 882\n",
            "2021-11-23 23:22:28,382 - INFO - joeynmt.training - Epoch 882: total training loss 0.12\n",
            "2021-11-23 23:22:28,383 - INFO - joeynmt.training - EPOCH 883\n",
            "2021-11-23 23:22:28,946 - INFO - joeynmt.training - Epoch 883: total training loss 0.12\n",
            "2021-11-23 23:22:28,946 - INFO - joeynmt.training - EPOCH 884\n",
            "2021-11-23 23:22:29,514 - INFO - joeynmt.training - Epoch 884: total training loss 0.12\n",
            "2021-11-23 23:22:29,515 - INFO - joeynmt.training - EPOCH 885\n",
            "2021-11-23 23:22:30,088 - INFO - joeynmt.training - Epoch 885: total training loss 0.12\n",
            "2021-11-23 23:22:30,088 - INFO - joeynmt.training - EPOCH 886\n",
            "2021-11-23 23:22:30,658 - INFO - joeynmt.training - Epoch 886: total training loss 0.13\n",
            "2021-11-23 23:22:30,659 - INFO - joeynmt.training - EPOCH 887\n",
            "2021-11-23 23:22:31,226 - INFO - joeynmt.training - Epoch 887: total training loss 0.12\n",
            "2021-11-23 23:22:31,226 - INFO - joeynmt.training - EPOCH 888\n",
            "2021-11-23 23:22:31,797 - INFO - joeynmt.training - Epoch 888: total training loss 0.13\n",
            "2021-11-23 23:22:31,797 - INFO - joeynmt.training - EPOCH 889\n",
            "2021-11-23 23:22:32,367 - INFO - joeynmt.training - Epoch 889: total training loss 0.13\n",
            "2021-11-23 23:22:32,367 - INFO - joeynmt.training - EPOCH 890\n",
            "2021-11-23 23:22:32,933 - INFO - joeynmt.training - Epoch 890: total training loss 0.13\n",
            "2021-11-23 23:22:32,933 - INFO - joeynmt.training - EPOCH 891\n",
            "2021-11-23 23:22:33,505 - INFO - joeynmt.training - Epoch 891: total training loss 0.12\n",
            "2021-11-23 23:22:33,505 - INFO - joeynmt.training - EPOCH 892\n",
            "2021-11-23 23:22:34,068 - INFO - joeynmt.training - Epoch 892: total training loss 0.13\n",
            "2021-11-23 23:22:34,069 - INFO - joeynmt.training - EPOCH 893\n",
            "2021-11-23 23:22:34,631 - INFO - joeynmt.training - Epoch 893: total training loss 0.13\n",
            "2021-11-23 23:22:34,632 - INFO - joeynmt.training - EPOCH 894\n",
            "2021-11-23 23:22:35,204 - INFO - joeynmt.training - Epoch 894: total training loss 0.12\n",
            "2021-11-23 23:22:35,204 - INFO - joeynmt.training - EPOCH 895\n",
            "2021-11-23 23:22:35,773 - INFO - joeynmt.training - Epoch 895: total training loss 0.13\n",
            "2021-11-23 23:22:35,773 - INFO - joeynmt.training - EPOCH 896\n",
            "2021-11-23 23:22:36,337 - INFO - joeynmt.training - Epoch 896: total training loss 0.13\n",
            "2021-11-23 23:22:36,337 - INFO - joeynmt.training - EPOCH 897\n",
            "2021-11-23 23:22:36,903 - INFO - joeynmt.training - Epoch 897: total training loss 0.12\n",
            "2021-11-23 23:22:36,904 - INFO - joeynmt.training - EPOCH 898\n",
            "2021-11-23 23:22:37,478 - INFO - joeynmt.training - Epoch 898: total training loss 0.13\n",
            "2021-11-23 23:22:37,479 - INFO - joeynmt.training - EPOCH 899\n",
            "2021-11-23 23:22:38,043 - INFO - joeynmt.training - Epoch 899: total training loss 0.13\n",
            "2021-11-23 23:22:38,044 - INFO - joeynmt.training - EPOCH 900\n",
            "2021-11-23 23:22:38,610 - INFO - joeynmt.training - Epoch 900, Step:     1800, Batch Loss:     0.065153, Tokens per Sec:     6662, Lr: 0.000300\n",
            "2021-11-23 23:22:38,611 - INFO - joeynmt.training - Epoch 900: total training loss 0.12\n",
            "2021-11-23 23:22:38,611 - INFO - joeynmt.training - EPOCH 901\n",
            "2021-11-23 23:22:39,175 - INFO - joeynmt.training - Epoch 901: total training loss 0.12\n",
            "2021-11-23 23:22:39,176 - INFO - joeynmt.training - EPOCH 902\n",
            "2021-11-23 23:22:39,742 - INFO - joeynmt.training - Epoch 902: total training loss 0.13\n",
            "2021-11-23 23:22:39,742 - INFO - joeynmt.training - EPOCH 903\n",
            "2021-11-23 23:22:40,321 - INFO - joeynmt.training - Epoch 903: total training loss 0.13\n",
            "2021-11-23 23:22:40,321 - INFO - joeynmt.training - EPOCH 904\n",
            "2021-11-23 23:22:40,876 - INFO - joeynmt.training - Epoch 904: total training loss 0.13\n",
            "2021-11-23 23:22:40,877 - INFO - joeynmt.training - EPOCH 905\n",
            "2021-11-23 23:22:41,450 - INFO - joeynmt.training - Epoch 905: total training loss 0.12\n",
            "2021-11-23 23:22:41,450 - INFO - joeynmt.training - EPOCH 906\n",
            "2021-11-23 23:22:42,022 - INFO - joeynmt.training - Epoch 906: total training loss 0.12\n",
            "2021-11-23 23:22:42,022 - INFO - joeynmt.training - EPOCH 907\n",
            "2021-11-23 23:22:42,590 - INFO - joeynmt.training - Epoch 907: total training loss 0.12\n",
            "2021-11-23 23:22:42,591 - INFO - joeynmt.training - EPOCH 908\n",
            "2021-11-23 23:22:43,160 - INFO - joeynmt.training - Epoch 908: total training loss 0.12\n",
            "2021-11-23 23:22:43,160 - INFO - joeynmt.training - EPOCH 909\n",
            "2021-11-23 23:22:43,722 - INFO - joeynmt.training - Epoch 909: total training loss 0.12\n",
            "2021-11-23 23:22:43,722 - INFO - joeynmt.training - EPOCH 910\n",
            "2021-11-23 23:22:44,289 - INFO - joeynmt.training - Epoch 910: total training loss 0.12\n",
            "2021-11-23 23:22:44,290 - INFO - joeynmt.training - EPOCH 911\n",
            "2021-11-23 23:22:44,865 - INFO - joeynmt.training - Epoch 911: total training loss 0.12\n",
            "2021-11-23 23:22:44,866 - INFO - joeynmt.training - EPOCH 912\n",
            "2021-11-23 23:22:45,433 - INFO - joeynmt.training - Epoch 912: total training loss 0.12\n",
            "2021-11-23 23:22:45,434 - INFO - joeynmt.training - EPOCH 913\n",
            "2021-11-23 23:22:46,000 - INFO - joeynmt.training - Epoch 913: total training loss 0.12\n",
            "2021-11-23 23:22:46,001 - INFO - joeynmt.training - EPOCH 914\n",
            "2021-11-23 23:22:46,568 - INFO - joeynmt.training - Epoch 914: total training loss 0.11\n",
            "2021-11-23 23:22:46,568 - INFO - joeynmt.training - EPOCH 915\n",
            "2021-11-23 23:22:47,127 - INFO - joeynmt.training - Epoch 915: total training loss 0.12\n",
            "2021-11-23 23:22:47,127 - INFO - joeynmt.training - EPOCH 916\n",
            "2021-11-23 23:22:47,691 - INFO - joeynmt.training - Epoch 916: total training loss 0.12\n",
            "2021-11-23 23:22:47,691 - INFO - joeynmt.training - EPOCH 917\n",
            "2021-11-23 23:22:48,256 - INFO - joeynmt.training - Epoch 917: total training loss 0.12\n",
            "2021-11-23 23:22:48,256 - INFO - joeynmt.training - EPOCH 918\n",
            "2021-11-23 23:22:48,814 - INFO - joeynmt.training - Epoch 918: total training loss 0.12\n",
            "2021-11-23 23:22:48,814 - INFO - joeynmt.training - EPOCH 919\n",
            "2021-11-23 23:22:49,373 - INFO - joeynmt.training - Epoch 919: total training loss 0.12\n",
            "2021-11-23 23:22:49,374 - INFO - joeynmt.training - EPOCH 920\n",
            "2021-11-23 23:22:49,939 - INFO - joeynmt.training - Epoch 920: total training loss 0.12\n",
            "2021-11-23 23:22:49,939 - INFO - joeynmt.training - EPOCH 921\n",
            "2021-11-23 23:22:50,510 - INFO - joeynmt.training - Epoch 921: total training loss 0.12\n",
            "2021-11-23 23:22:50,510 - INFO - joeynmt.training - EPOCH 922\n",
            "2021-11-23 23:22:51,070 - INFO - joeynmt.training - Epoch 922: total training loss 0.12\n",
            "2021-11-23 23:22:51,070 - INFO - joeynmt.training - EPOCH 923\n",
            "2021-11-23 23:22:51,633 - INFO - joeynmt.training - Epoch 923: total training loss 0.12\n",
            "2021-11-23 23:22:51,633 - INFO - joeynmt.training - EPOCH 924\n",
            "2021-11-23 23:22:52,194 - INFO - joeynmt.training - Epoch 924: total training loss 0.12\n",
            "2021-11-23 23:22:52,194 - INFO - joeynmt.training - EPOCH 925\n",
            "2021-11-23 23:22:52,761 - INFO - joeynmt.training - Epoch 925: total training loss 0.13\n",
            "2021-11-23 23:22:52,761 - INFO - joeynmt.training - EPOCH 926\n",
            "2021-11-23 23:22:53,313 - INFO - joeynmt.training - Epoch 926: total training loss 0.12\n",
            "2021-11-23 23:22:53,313 - INFO - joeynmt.training - EPOCH 927\n",
            "2021-11-23 23:22:53,881 - INFO - joeynmt.training - Epoch 927: total training loss 0.12\n",
            "2021-11-23 23:22:53,881 - INFO - joeynmt.training - EPOCH 928\n",
            "2021-11-23 23:22:54,440 - INFO - joeynmt.training - Epoch 928: total training loss 0.11\n",
            "2021-11-23 23:22:54,440 - INFO - joeynmt.training - EPOCH 929\n",
            "2021-11-23 23:22:55,017 - INFO - joeynmt.training - Epoch 929: total training loss 0.11\n",
            "2021-11-23 23:22:55,017 - INFO - joeynmt.training - EPOCH 930\n",
            "2021-11-23 23:22:55,578 - INFO - joeynmt.training - Epoch 930: total training loss 0.11\n",
            "2021-11-23 23:22:55,579 - INFO - joeynmt.training - EPOCH 931\n",
            "2021-11-23 23:22:56,143 - INFO - joeynmt.training - Epoch 931: total training loss 0.12\n",
            "2021-11-23 23:22:56,144 - INFO - joeynmt.training - EPOCH 932\n",
            "2021-11-23 23:22:56,710 - INFO - joeynmt.training - Epoch 932: total training loss 0.11\n",
            "2021-11-23 23:22:56,710 - INFO - joeynmt.training - EPOCH 933\n",
            "2021-11-23 23:22:57,269 - INFO - joeynmt.training - Epoch 933: total training loss 0.12\n",
            "2021-11-23 23:22:57,270 - INFO - joeynmt.training - EPOCH 934\n",
            "2021-11-23 23:22:57,840 - INFO - joeynmt.training - Epoch 934: total training loss 0.11\n",
            "2021-11-23 23:22:57,840 - INFO - joeynmt.training - EPOCH 935\n",
            "2021-11-23 23:22:58,404 - INFO - joeynmt.training - Epoch 935: total training loss 0.12\n",
            "2021-11-23 23:22:58,405 - INFO - joeynmt.training - EPOCH 936\n",
            "2021-11-23 23:22:58,964 - INFO - joeynmt.training - Epoch 936: total training loss 0.12\n",
            "2021-11-23 23:22:58,965 - INFO - joeynmt.training - EPOCH 937\n",
            "2021-11-23 23:22:59,541 - INFO - joeynmt.training - Epoch 937: total training loss 0.12\n",
            "2021-11-23 23:22:59,541 - INFO - joeynmt.training - EPOCH 938\n",
            "2021-11-23 23:23:00,104 - INFO - joeynmt.training - Epoch 938: total training loss 0.11\n",
            "2021-11-23 23:23:00,104 - INFO - joeynmt.training - EPOCH 939\n",
            "2021-11-23 23:23:00,669 - INFO - joeynmt.training - Epoch 939: total training loss 0.11\n",
            "2021-11-23 23:23:00,669 - INFO - joeynmt.training - EPOCH 940\n",
            "2021-11-23 23:23:01,227 - INFO - joeynmt.training - Epoch 940: total training loss 0.13\n",
            "2021-11-23 23:23:01,227 - INFO - joeynmt.training - EPOCH 941\n",
            "2021-11-23 23:23:01,794 - INFO - joeynmt.training - Epoch 941: total training loss 0.11\n",
            "2021-11-23 23:23:01,795 - INFO - joeynmt.training - EPOCH 942\n",
            "2021-11-23 23:23:02,352 - INFO - joeynmt.training - Epoch 942: total training loss 0.11\n",
            "2021-11-23 23:23:02,353 - INFO - joeynmt.training - EPOCH 943\n",
            "2021-11-23 23:23:02,921 - INFO - joeynmt.training - Epoch 943: total training loss 0.11\n",
            "2021-11-23 23:23:02,921 - INFO - joeynmt.training - EPOCH 944\n",
            "2021-11-23 23:23:03,490 - INFO - joeynmt.training - Epoch 944: total training loss 0.12\n",
            "2021-11-23 23:23:03,490 - INFO - joeynmt.training - EPOCH 945\n",
            "2021-11-23 23:23:04,063 - INFO - joeynmt.training - Epoch 945: total training loss 0.12\n",
            "2021-11-23 23:23:04,064 - INFO - joeynmt.training - EPOCH 946\n",
            "2021-11-23 23:23:04,622 - INFO - joeynmt.training - Epoch 946: total training loss 0.11\n",
            "2021-11-23 23:23:04,622 - INFO - joeynmt.training - EPOCH 947\n",
            "2021-11-23 23:23:05,189 - INFO - joeynmt.training - Epoch 947: total training loss 0.11\n",
            "2021-11-23 23:23:05,189 - INFO - joeynmt.training - EPOCH 948\n",
            "2021-11-23 23:23:05,764 - INFO - joeynmt.training - Epoch 948: total training loss 0.12\n",
            "2021-11-23 23:23:05,764 - INFO - joeynmt.training - EPOCH 949\n",
            "2021-11-23 23:23:06,333 - INFO - joeynmt.training - Epoch 949: total training loss 0.12\n",
            "2021-11-23 23:23:06,334 - INFO - joeynmt.training - EPOCH 950\n",
            "2021-11-23 23:23:06,911 - INFO - joeynmt.training - Epoch 950, Step:     1900, Batch Loss:     0.054617, Tokens per Sec:     6544, Lr: 0.000300\n",
            "2021-11-23 23:23:06,911 - INFO - joeynmt.training - Epoch 950: total training loss 0.11\n",
            "2021-11-23 23:23:06,912 - INFO - joeynmt.training - EPOCH 951\n",
            "2021-11-23 23:23:07,470 - INFO - joeynmt.training - Epoch 951: total training loss 0.12\n",
            "2021-11-23 23:23:07,471 - INFO - joeynmt.training - EPOCH 952\n",
            "2021-11-23 23:23:08,036 - INFO - joeynmt.training - Epoch 952: total training loss 0.12\n",
            "2021-11-23 23:23:08,036 - INFO - joeynmt.training - EPOCH 953\n",
            "2021-11-23 23:23:08,604 - INFO - joeynmt.training - Epoch 953: total training loss 0.11\n",
            "2021-11-23 23:23:08,604 - INFO - joeynmt.training - EPOCH 954\n",
            "2021-11-23 23:23:09,169 - INFO - joeynmt.training - Epoch 954: total training loss 0.12\n",
            "2021-11-23 23:23:09,169 - INFO - joeynmt.training - EPOCH 955\n",
            "2021-11-23 23:23:09,748 - INFO - joeynmt.training - Epoch 955: total training loss 0.11\n",
            "2021-11-23 23:23:09,748 - INFO - joeynmt.training - EPOCH 956\n",
            "2021-11-23 23:23:10,310 - INFO - joeynmt.training - Epoch 956: total training loss 0.11\n",
            "2021-11-23 23:23:10,310 - INFO - joeynmt.training - EPOCH 957\n",
            "2021-11-23 23:23:10,875 - INFO - joeynmt.training - Epoch 957: total training loss 0.11\n",
            "2021-11-23 23:23:10,876 - INFO - joeynmt.training - EPOCH 958\n",
            "2021-11-23 23:23:11,440 - INFO - joeynmt.training - Epoch 958: total training loss 0.12\n",
            "2021-11-23 23:23:11,440 - INFO - joeynmt.training - EPOCH 959\n",
            "2021-11-23 23:23:12,012 - INFO - joeynmt.training - Epoch 959: total training loss 0.10\n",
            "2021-11-23 23:23:12,012 - INFO - joeynmt.training - EPOCH 960\n",
            "2021-11-23 23:23:12,578 - INFO - joeynmt.training - Epoch 960: total training loss 0.11\n",
            "2021-11-23 23:23:12,579 - INFO - joeynmt.training - EPOCH 961\n",
            "2021-11-23 23:23:13,139 - INFO - joeynmt.training - Epoch 961: total training loss 0.11\n",
            "2021-11-23 23:23:13,140 - INFO - joeynmt.training - EPOCH 962\n",
            "2021-11-23 23:23:13,710 - INFO - joeynmt.training - Epoch 962: total training loss 0.11\n",
            "2021-11-23 23:23:13,711 - INFO - joeynmt.training - EPOCH 963\n",
            "2021-11-23 23:23:14,280 - INFO - joeynmt.training - Epoch 963: total training loss 0.11\n",
            "2021-11-23 23:23:14,280 - INFO - joeynmt.training - EPOCH 964\n",
            "2021-11-23 23:23:14,859 - INFO - joeynmt.training - Epoch 964: total training loss 0.11\n",
            "2021-11-23 23:23:14,859 - INFO - joeynmt.training - EPOCH 965\n",
            "2021-11-23 23:23:15,418 - INFO - joeynmt.training - Epoch 965: total training loss 0.11\n",
            "2021-11-23 23:23:15,418 - INFO - joeynmt.training - EPOCH 966\n",
            "2021-11-23 23:23:15,990 - INFO - joeynmt.training - Epoch 966: total training loss 0.11\n",
            "2021-11-23 23:23:15,990 - INFO - joeynmt.training - EPOCH 967\n",
            "2021-11-23 23:23:16,559 - INFO - joeynmt.training - Epoch 967: total training loss 0.11\n",
            "2021-11-23 23:23:16,559 - INFO - joeynmt.training - EPOCH 968\n",
            "2021-11-23 23:23:17,133 - INFO - joeynmt.training - Epoch 968: total training loss 0.12\n",
            "2021-11-23 23:23:17,134 - INFO - joeynmt.training - EPOCH 969\n",
            "2021-11-23 23:23:17,709 - INFO - joeynmt.training - Epoch 969: total training loss 0.11\n",
            "2021-11-23 23:23:17,709 - INFO - joeynmt.training - EPOCH 970\n",
            "2021-11-23 23:23:18,270 - INFO - joeynmt.training - Epoch 970: total training loss 0.11\n",
            "2021-11-23 23:23:18,270 - INFO - joeynmt.training - EPOCH 971\n",
            "2021-11-23 23:23:18,841 - INFO - joeynmt.training - Epoch 971: total training loss 0.11\n",
            "2021-11-23 23:23:18,841 - INFO - joeynmt.training - EPOCH 972\n",
            "2021-11-23 23:23:19,404 - INFO - joeynmt.training - Epoch 972: total training loss 0.11\n",
            "2021-11-23 23:23:19,404 - INFO - joeynmt.training - EPOCH 973\n",
            "2021-11-23 23:23:19,968 - INFO - joeynmt.training - Epoch 973: total training loss 0.11\n",
            "2021-11-23 23:23:19,969 - INFO - joeynmt.training - EPOCH 974\n",
            "2021-11-23 23:23:20,539 - INFO - joeynmt.training - Epoch 974: total training loss 0.11\n",
            "2021-11-23 23:23:20,540 - INFO - joeynmt.training - EPOCH 975\n",
            "2021-11-23 23:23:21,095 - INFO - joeynmt.training - Epoch 975: total training loss 0.11\n",
            "2021-11-23 23:23:21,096 - INFO - joeynmt.training - EPOCH 976\n",
            "2021-11-23 23:23:21,660 - INFO - joeynmt.training - Epoch 976: total training loss 0.12\n",
            "2021-11-23 23:23:21,661 - INFO - joeynmt.training - EPOCH 977\n",
            "2021-11-23 23:23:22,230 - INFO - joeynmt.training - Epoch 977: total training loss 0.11\n",
            "2021-11-23 23:23:22,231 - INFO - joeynmt.training - EPOCH 978\n",
            "2021-11-23 23:23:22,798 - INFO - joeynmt.training - Epoch 978: total training loss 0.11\n",
            "2021-11-23 23:23:22,799 - INFO - joeynmt.training - EPOCH 979\n",
            "2021-11-23 23:23:23,370 - INFO - joeynmt.training - Epoch 979: total training loss 0.11\n",
            "2021-11-23 23:23:23,371 - INFO - joeynmt.training - EPOCH 980\n",
            "2021-11-23 23:23:23,940 - INFO - joeynmt.training - Epoch 980: total training loss 0.11\n",
            "2021-11-23 23:23:23,941 - INFO - joeynmt.training - EPOCH 981\n",
            "2021-11-23 23:23:24,515 - INFO - joeynmt.training - Epoch 981: total training loss 0.11\n",
            "2021-11-23 23:23:24,515 - INFO - joeynmt.training - EPOCH 982\n",
            "2021-11-23 23:23:25,081 - INFO - joeynmt.training - Epoch 982: total training loss 0.10\n",
            "2021-11-23 23:23:25,082 - INFO - joeynmt.training - EPOCH 983\n",
            "2021-11-23 23:23:25,641 - INFO - joeynmt.training - Epoch 983: total training loss 0.11\n",
            "2021-11-23 23:23:25,642 - INFO - joeynmt.training - EPOCH 984\n",
            "2021-11-23 23:23:26,221 - INFO - joeynmt.training - Epoch 984: total training loss 0.12\n",
            "2021-11-23 23:23:26,221 - INFO - joeynmt.training - EPOCH 985\n",
            "2021-11-23 23:23:26,781 - INFO - joeynmt.training - Epoch 985: total training loss 0.11\n",
            "2021-11-23 23:23:26,781 - INFO - joeynmt.training - EPOCH 986\n",
            "2021-11-23 23:23:27,340 - INFO - joeynmt.training - Epoch 986: total training loss 0.10\n",
            "2021-11-23 23:23:27,341 - INFO - joeynmt.training - EPOCH 987\n",
            "2021-11-23 23:23:27,902 - INFO - joeynmt.training - Epoch 987: total training loss 0.10\n",
            "2021-11-23 23:23:27,902 - INFO - joeynmt.training - EPOCH 988\n",
            "2021-11-23 23:23:28,464 - INFO - joeynmt.training - Epoch 988: total training loss 0.10\n",
            "2021-11-23 23:23:28,464 - INFO - joeynmt.training - EPOCH 989\n",
            "2021-11-23 23:23:29,034 - INFO - joeynmt.training - Epoch 989: total training loss 0.11\n",
            "2021-11-23 23:23:29,034 - INFO - joeynmt.training - EPOCH 990\n",
            "2021-11-23 23:23:29,608 - INFO - joeynmt.training - Epoch 990: total training loss 0.11\n",
            "2021-11-23 23:23:29,608 - INFO - joeynmt.training - EPOCH 991\n",
            "2021-11-23 23:23:30,167 - INFO - joeynmt.training - Epoch 991: total training loss 0.10\n",
            "2021-11-23 23:23:30,168 - INFO - joeynmt.training - EPOCH 992\n",
            "2021-11-23 23:23:30,741 - INFO - joeynmt.training - Epoch 992: total training loss 0.10\n",
            "2021-11-23 23:23:30,742 - INFO - joeynmt.training - EPOCH 993\n",
            "2021-11-23 23:23:31,316 - INFO - joeynmt.training - Epoch 993: total training loss 0.10\n",
            "2021-11-23 23:23:31,316 - INFO - joeynmt.training - EPOCH 994\n",
            "2021-11-23 23:23:31,879 - INFO - joeynmt.training - Epoch 994: total training loss 0.10\n",
            "2021-11-23 23:23:31,879 - INFO - joeynmt.training - EPOCH 995\n",
            "2021-11-23 23:23:32,448 - INFO - joeynmt.training - Epoch 995: total training loss 0.10\n",
            "2021-11-23 23:23:32,448 - INFO - joeynmt.training - EPOCH 996\n",
            "2021-11-23 23:23:33,017 - INFO - joeynmt.training - Epoch 996: total training loss 0.11\n",
            "2021-11-23 23:23:33,017 - INFO - joeynmt.training - EPOCH 997\n",
            "2021-11-23 23:23:33,592 - INFO - joeynmt.training - Epoch 997: total training loss 0.11\n",
            "2021-11-23 23:23:33,592 - INFO - joeynmt.training - EPOCH 998\n",
            "2021-11-23 23:23:34,166 - INFO - joeynmt.training - Epoch 998: total training loss 0.10\n",
            "2021-11-23 23:23:34,166 - INFO - joeynmt.training - EPOCH 999\n",
            "2021-11-23 23:23:34,739 - INFO - joeynmt.training - Epoch 999: total training loss 0.10\n",
            "2021-11-23 23:23:34,740 - INFO - joeynmt.training - EPOCH 1000\n",
            "2021-11-23 23:23:35,297 - INFO - joeynmt.training - Epoch 1000, Step:     2000, Batch Loss:     0.054578, Tokens per Sec:     6777, Lr: 0.000300\n",
            "2021-11-23 23:26:26,499 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-11-23 23:26:26,914 - INFO - joeynmt.training - Example #0\n",
            "2021-11-23 23:26:26,915 - INFO - joeynmt.training - \tSource:     ▁ T h e ▁ r e s e a r c h ▁ b y ▁ t h e ▁ U n i v e r s i t y ▁ o f ▁ E d i n b u r g h ▁ o v e r ▁ a ▁ 3 0 ▁ y e a r ▁ l o o k e d ▁ a t ▁ t h e ▁ s p r e a d ▁ o f ▁ B o v i n e ▁ T u b e r c u l o s i s ▁ ( T B ) ▁ w i t h i n ▁ c o w s ▁ a n d ▁ b a d g e r ▁ p o p u l a t i o n s ▁ i n ▁ G l o u c e s t e r s h i r e .\n",
            "2021-11-23 23:26:26,915 - INFO - joeynmt.training - \tReference:  ▁ U c w a n i n g o ▁ l w e N y u v e s i ▁ y a s e - E d i n b u r g h ▁ o l u t h a t h e ▁ i m i n y a k a ▁ e n g a m a - 3 0 ▁ l u b h e k e ▁ u k u s a b a l a l a ▁ k w e - B o v i n e ▁ T u b e r c u l o s i s ▁ ( i - T B ) ▁ e z i n k o m e n i ▁ n a k u m a - b a d g e r ▁ e G l o u e s t e r s h i r e .\n",
            "2021-11-23 23:26:26,916 - INFO - joeynmt.training - \tHypothesis: ▁<unk>L<unk>e<unk>k<unk>a<unk>▁<unk>a<unk>n<unk>g<unk>e<unk>l<unk>e<unk>▁<unk>a<unk>b<unk>a<unk>▁<unk>u<unk>l<unk>i<unk>n<unk>z<unk>i<unk>l<unk>o<unk>▁<unk>0<unk>n<unk>o<unk>▁<unk>e<unk>f<unk>a<unk>n<unk>a<unk>y<unk>o<unk>▁<unk>l<unk>o<unk>k<unk>u<unk>b<unk>b<unk>e<unk>n<unk>z<unk>a<unk>.\n",
            "2021-11-23 23:26:26,916 - INFO - joeynmt.training - Example #1\n",
            "2021-11-23 23:26:26,916 - INFO - joeynmt.training - \tSource:     ▁ W h e n ▁ I ▁ b o o k e d ▁ a ▁ t h r e e - d a y ▁ s i n g i n g ▁ r e t r e a t ▁ a t ▁ P e t e r ▁ E v a n s ' s ▁ f a r m h o u s e ▁ i n ▁ r u r a l ▁ F r a n c e , ▁ I ▁ h a d ▁ b e e n ▁ a p p r e h e n s i v e ▁ a b o u t ▁ d i s p l a y i n g ▁ m y ▁ p i t i f u l ▁ a b i l i t i e s ▁ t o ▁ s o m e ▁ i n t i m i d a t i n g ▁ O x f o r d ▁ c h o r a l ▁ s c h o l a r ▁ t y p e . ▁ B u t ▁ i n s t e a d ▁ I ▁ f o u n d ▁ m y s e l f ▁ w a r b l i n g ▁ t o ▁ a ▁ s k i n n y ▁ f o r m e r ▁ p u n k ▁ g u i t a r i s t ▁ w i t h ▁ a ▁ s t r o n g ▁ \" s a r f \" ▁ L o n d o n ▁ a c c e n t ▁ a n d ▁ a n ▁ a r r a y ▁ o f ▁ t r u l y ▁ t e r r i b l e ▁ j o k e s .\n",
            "2021-11-23 23:26:26,916 - INFO - joeynmt.training - \tReference:  ▁ N g e n k a t h i ▁ n g i b e k i s e l a ▁ u h l e l o ▁ l w e z i n s u k u ▁ e z i n t a t h u ▁ l o k u v u s e l e l a ▁ i k h o n o ▁ l o m c u l o ▁ e n d l i n i ▁ y a s e p l a z i n i ▁ k a P e t e r ▁ E v a n ▁ e n d a w e n i ▁ e s e m a k h a y a ▁ e F r a n c e , ▁ n g a n g i n o k u z i b a m b a ▁ m a q o n d a n a ▁ n o k u v e z a ▁ a m a k h o n o ▁ a m i ▁ a l u s i z i ▁ k u h l o b o ▁ l w a b a c u l i ▁ b e k w a y a ▁ b a s e - O x f o r d . ▁ K o d w a ▁ e s i k h u n d l e n i ▁ s a l o k h o ▁ n g i z i t h o l e ▁ n g i c u l e l a ▁ p h a n s i ▁ k u k h a l a ▁ i s i g i n g c i ▁ s i s h o ▁ i c u l o ▁ n g o k u g c i z e l e l a ▁ i n d l e l a ▁ y o k u k h u l u m a ▁ y a s e L o n d o n ▁ k u h a m b i s a n a ▁ n a m a h l a y a ▁ a m a b i ▁ k a k h u l u .\n",
            "2021-11-23 23:26:26,917 - INFO - joeynmt.training - \tHypothesis: ▁<unk>U<unk>k<unk>u<unk>k<unk>h<unk>e<unk>,<unk>▁<unk>b<unk>a<unk>d<unk>i<unk>m<unk>a<unk>n<unk>g<unk>u<unk>k<unk>w<unk>e<unk>▁<unk>z<unk>a<unk>n<unk>g<unk>a<unk>m<unk>o<unk>▁<unk>w<unk>e<unk>f<unk>a<unk>n<unk>a<unk>▁<unk>a<unk>-<unk>-\n",
            "2021-11-23 23:26:26,917 - INFO - joeynmt.training - Example #2\n",
            "2021-11-23 23:26:26,917 - INFO - joeynmt.training - \tSource:     ▁ I n ▁ 2 0 1 2 , ▁ f o r ▁ e x a m p l e , ▁ m u l t i p l e ▁ g u e s t s ▁ w e r e ▁ i n j u r e d ▁ w h e n ▁ t h e i r ▁ t o u r ▁ b u s ▁ c r a s h e d ▁ i n ▁ S t . ▁ M a r t i n .\n",
            "2021-11-23 23:26:26,917 - INFO - joeynmt.training - \tReference:  ▁ N g o w e z i - 2 0 1 2 , ▁ i s i b o n e l o , ▁ k w a l i m a l a ▁ i n q w a b a ▁ y e z i v a k a s h i ▁ l a p h o ▁ i b h a s i ▁ l a b o ▁ l o k u v a k a s h a ▁ l i s h a y i s a ▁ e - S t . ▁ M a r t i n .\n",
            "2021-11-23 23:26:26,917 - INFO - joeynmt.training - \tHypothesis: ▁<unk>U<unk>k<unk>u<unk>p<unk>h<unk>u<unk>n<unk>o<unk>m<unk>a<unk>▁<unk>u<unk>k<unk>w<unk>e<unk>j<unk>u<unk>b<unk>a<unk>n<unk>e\n",
            "2021-11-23 23:26:26,918 - INFO - joeynmt.training - Example #3\n",
            "2021-11-23 23:26:26,918 - INFO - joeynmt.training - \tSource:     ▁ \" W e ▁ w e r e ▁ r e a l l y ▁ g o o d ▁ t h a t ▁ t o u r n a m e n t ▁ r i g h t ▁ t h e ▁ w a y ▁ t h r o u g h ▁ t o ▁ t h e ▁ f i n a l . ▁ I ▁ d o n ' t ▁ t h i n k ▁ i t ▁ m o t i v a t e s ▁ o r ▁ c a r r i e s ▁ m e ▁ t o ▁ d o ▁ a n y ▁ b e t t e r , \" ▁ G e n i a ▁ s a i d . ▁ \" I t ' s ▁ j u s t ▁ e m b r a c i n g ▁ i t ▁ a n d ▁ e n j o y i n g ▁ i t . ▁ I t ' s ▁ o n c e ▁ e v e r y ▁ f o u r ▁ y e a r s . ▁ I t ' s ▁ a ▁ d i f f e r e n t ▁ k i n d ▁ o f ▁ b u z z ▁ a n d ▁ a ▁ d i f f e r e n t ▁ k i n d ▁ o f ▁ e n e r g y ▁ w h e n ▁ y o u ▁ a r e ▁ h e r e ▁ w h e n ▁ y o u ▁ a r e ▁ p l a y i n g ▁ a t ▁ a ▁ W o r l d ▁ C u p ▁ b e c a u s e ▁ a l l ▁ t h e ▁ a t t e n t i o n ▁ i s ▁ o n ▁ r u g b y . \"\n",
            "2021-11-23 23:26:26,918 - INFO - joeynmt.training - \tReference:  ▁ \" S i d l a l e ▁ k a h l e ▁ k a k h u l u ▁ k u l o w o ▁ m q h u d e l w a n o ▁ z i b e k w a ▁ n j e ▁ k w a z e ▁ k w a f i k w a ▁ e k u g c i n e n i . ▁ A n g i b o n i ▁ u k u t h i ▁ k u y a k h u t h a z a ▁ n o m a ▁ k u n g i q h u b a ▁ k a n g c o n o , \" ▁ k u s h o ▁ u G e n i a . ▁ \" W u k u k w a m u k e l a ▁ n j e ▁ n o k u k u j a b u l e l a . ▁ K u b a ▁ n j a l o ▁ e m i n y a k e n i ▁ e m i n e . ▁ W u h l o b o ▁ o l u h l u k i l e ▁ l o m s i n d o ▁ f u t h i ▁ u h l o b o ▁ o l u h l u k i l e ▁ l w a m a n d l a ▁ u m a ▁ u l a p h a ▁ u m a ▁ u d l a l a ▁ k w i N d e b e ▁ Y o m h l a b a ▁ n g o b a ▁ k u n a k w a ▁ k a k h u l u ▁ u m d l a l o ▁ w e b h o l a ▁ l o m b h o x o . \"\n",
            "2021-11-23 23:26:26,918 - INFO - joeynmt.training - \tHypothesis: ▁<unk>U<unk>n<unk>g<unk>a<unk>k<unk>u<unk>▁<unk>l<unk>u<unk>n<unk>g<unk>i<unk>▁<unk>n<unk>g<unk>u<unk>-<unk>B<unk>a<unk>k<unk>h<unk>l<unk>e<unk>▁<unk>k<unk>a<unk>m<unk>o<unk>▁<unk>h<unk>l<unk>e<unk>n<unk>a<unk>y<unk>o<unk>▁<unk>a<unk>k<unk>u<unk>s<unk>e<unk>b<unk>e<unk>▁<unk>z<unk>a<unk>.\n",
            "2021-11-23 23:26:26,919 - INFO - joeynmt.training - Validation result (greedy) at epoch 1000, step     2000: bleu:   0.00, loss: 506040.5312, ppl:  16.6535, duration: 171.6216s\n",
            "2021-11-23 23:26:26,921 - INFO - joeynmt.training - Epoch 1000: total training loss 0.11\n",
            "2021-11-23 23:26:26,922 - INFO - joeynmt.training - Training ended after 1000 epochs.\n",
            "2021-11-23 23:26:26,922 - INFO - joeynmt.training - Best validation result (greedy) at step     2000:  16.65 ppl.\n",
            "2021-11-23 23:26:26,949 - INFO - joeynmt.prediction - Process device: cuda, n_gpu: 1, batch_size per device: 3600\n",
            "2021-11-23 23:26:26,949 - INFO - joeynmt.prediction - Loading model from models/enzu_transformer/2000.ckpt\n",
            "2021-11-23 23:26:27,185 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
            "2021-11-23 23:26:27,414 - INFO - joeynmt.model - Enc-dec model built.\n",
            "2021-11-23 23:26:27,496 - INFO - joeynmt.prediction - Decoding on dev set (data/enzu/dev.char.zu)...\n",
            "2021-11-23 23:34:01,469 - INFO - joeynmt.prediction -  dev bleu[13a]:   0.00 [Beam search decoding with beam size = 5 and alpha = 1.0]\n",
            "2021-11-23 23:34:01,470 - INFO - joeynmt.prediction - Translations saved to: models/enzu_transformer/00002000.hyps.dev\n",
            "2021-11-23 23:34:01,470 - INFO - joeynmt.prediction - Decoding on test set (data/enzu/test.char.zu)...\n",
            "2021-11-23 23:39:03,828 - INFO - joeynmt.prediction - test bleu[13a]:   0.01 [Beam search decoding with beam size = 5 and alpha = 1.0]\n",
            "2021-11-23 23:39:03,829 - INFO - joeynmt.prediction - Translations saved to: models/enzu_transformer/00002000.hyps.test\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "MBoDS09JM807"
      },
      "source": [
        "# Copy the created models from the notebook storage to google drive for persistant storage\n",
        "!cp -r joeynmt/models/${src}${tgt}_transformer/* \"$gdrive_path/models/${src}${tgt}_transformer/\""
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "n94wlrCjVc17",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5e84e22-f360-46f7-c7c6-1539f731f2be"
      },
      "source": [
        "# Output our validation accuracy\n",
        "! cat \"$gdrive_path/models/${src}${tgt}_transformer/validations.txt\""
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Steps: 1000\tLoss: 573333.18750\tPPL: 24.20686\tbleu: 0.00456\tLR: 0.00030000\t*\n",
            "Steps: 2000\tLoss: 506040.53125\tPPL: 16.65345\tbleu: 0.00461\tLR: 0.00030000\t*\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "66WhRE9lIhoD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d450ba42-f4b9-4134-ba2a-004d4c46407d"
      },
      "source": [
        "# Test our model\n",
        "! cd joeynmt; python3 -m joeynmt test \"$gdrive_path/models/${src}${tgt}_transformer/config.yaml\""
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2021-11-23 23:48:40,595 - INFO - root - Hello! This is Joey-NMT (version 1.3).\n",
            "2021-11-23 23:48:40,596 - INFO - joeynmt.data - Building vocabulary...\n",
            "2021-11-23 23:48:40,597 - INFO - joeynmt.data - Loading dev data...\n",
            "2021-11-23 23:48:40,611 - INFO - joeynmt.data - Loading test data...\n",
            "2021-11-23 23:48:40,619 - INFO - joeynmt.data - Data loaded.\n",
            "2021-11-23 23:48:40,643 - INFO - joeynmt.prediction - Process device: cuda, n_gpu: 1, batch_size per device: 3600\n",
            "2021-11-23 23:48:40,643 - INFO - joeynmt.prediction - Loading model from models/enzu_transformer/2000.ckpt\n",
            "2021-11-23 23:48:42,985 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
            "2021-11-23 23:48:43,221 - INFO - joeynmt.model - Enc-dec model built.\n",
            "2021-11-23 23:48:43,304 - INFO - joeynmt.prediction - Decoding on dev set (data/enzu/dev.char.zu)...\n",
            "2021-11-23 23:56:15,195 - INFO - joeynmt.prediction -  dev bleu[13a]:   0.00 [Beam search decoding with beam size = 5 and alpha = 1.0]\n",
            "2021-11-23 23:56:15,195 - INFO - joeynmt.prediction - Decoding on test set (data/enzu/test.char.zu)...\n",
            "2021-11-24 00:01:15,787 - INFO - joeynmt.prediction - test bleu[13a]:   0.01 [Beam search decoding with beam size = 5 and alpha = 1.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQPMx-5NhB-L"
      },
      "source": [
        ""
      ],
      "execution_count": 27,
      "outputs": []
    }
  ]
}